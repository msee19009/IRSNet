{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "RR_1_01_Leky_relu_BN_IRSNet_tau_learn_M2_N32_BN1_varified_cost.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5-5BsZsakdS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea5183c4-a007-4272-8e5f-b4f208da9348"
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow 1.x selected.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKX-tsjsHifj"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EH-7KJkoB2JL"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from scipy.io import savemat\n",
        "from scipy.io import loadmat\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KS8UdvJvHzqD",
        "outputId": "4e988764-2f74-4d5c-8b0c-17949e1db105"
      },
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOM5bs4qApNS"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import time\n",
        "tf.set_random_seed(1)\n",
        "np.random.seed(1)\n",
        "num_epochs=200\n",
        "display_step=1\n",
        "var=1\n",
        "mean=0\n",
        "beta=0.0 # This is the L2-Regularization parameter\n",
        "mini_batch_size=3*10**3  # Number of examples for training\n",
        "num_batches = 4*10**2\n",
        "m_test=10**4   # Number of examples for testing\n",
        "num_test_batch = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVfo3zX_YQGn"
      },
      "source": [
        "eta = 1\n",
        "P_b = 10 #Watt or 40 dBm\n",
        "P_i = 0 #Watt or 40 dBm\n",
        "sigma_n = np.sqrt(3.98*10**-14)\n",
        "M = 2    #Power Beam\n",
        "N = 32   #of IRS elements"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDTiIEfVA1Fb",
        "outputId": "d77a6a22-31b5-4003-d1f7-ff17098a4ab2"
      },
      "source": [
        "alpha = 2.57\n",
        "alpha_irs = 2.2;\n",
        "Lc = 10**-3;\n",
        "\n",
        "d_br = 20;\n",
        "d_rs = 15;\n",
        "d_sb = 25;\n",
        "d_is = 15;\n",
        "d_id = 30\n",
        "d_rd = 15;\n",
        "d_sd = 25;\n",
        "d_ir = 15;\n",
        "\n",
        "b_bs = Lc*d_sb**(-alpha);\n",
        "b_is = Lc*d_is**(-alpha);\n",
        "b_id = Lc*d_id**(-alpha);\n",
        "b_sd = Lc*d_sd**(-alpha_irs);\n",
        "b_rs = Lc*d_rs**(-alpha_irs);\n",
        "b_rd = Lc*d_rd**(-alpha_irs);\n",
        "b_ir = Lc*d_ir**(-alpha_irs);\n",
        "b_br = Lc*d_br**(-alpha_irs);\n",
        "\n",
        "np.random.seed(11)\n",
        "h_bs = (np.random.randn(num_batches, M,mini_batch_size)+1j*np.random.randn(num_batches, M,mini_batch_size))*np.sqrt(b_bs/2)\n",
        "h_is = (np.random.randn(num_batches, 1,mini_batch_size)+1j*np.random.randn(num_batches, 1,mini_batch_size))*np.sqrt(b_is/2)\n",
        "h_id = (np.random.randn(num_batches, 1,mini_batch_size)+1j*np.random.randn(num_batches, 1,mini_batch_size))*np.sqrt(b_id/2)\n",
        "h_sd = (np.random.randn(num_batches, 1,mini_batch_size)+1j*np.random.randn(num_batches, 1,mini_batch_size))*np.sqrt(b_sd/2)\n",
        "g_rs = (np.random.randn(num_batches, N,mini_batch_size)+1j*np.random.randn(num_batches, N,mini_batch_size))*np.sqrt(b_rs/2)\n",
        "#g_sr = np.random.randn(num_batches, N,mini_batch_size)+1j*np.random.randn(num_batches, N,mini_batch_size)\n",
        "g_rd = (np.random.randn(num_batches, N,mini_batch_size)+1j*np.random.randn(num_batches, N,mini_batch_size))*np.sqrt(b_rd/2)\n",
        "g_ir = (np.random.randn(num_batches, N,mini_batch_size)+1j*np.random.randn(num_batches, N,mini_batch_size))*np.sqrt(b_ir/2)\n",
        "G_br = (np.random.randn(num_batches, N,M*mini_batch_size)+1j*np.random.randn(num_batches, N,M*mini_batch_size))*np.sqrt(b_br/2)\n",
        " \n",
        "print(\"Shape of h_bs  is: \"+str(h_bs.shape))\n",
        "print(\"Shape of h_is  is: \"+str(h_is.shape))\n",
        "print(\"Shape of h_id  is: \"+str(h_id.shape))\n",
        "print(\"Shape of h_sd  is: \"+str(h_sd.shape))\n",
        "print(\"Shape of g_rs  is: \"+str(g_rs.shape))\n",
        "print(\"Shape of g_rd  is: \"+str(g_rd.shape))\n",
        "print(\"Shape of g_ir  is: \"+str(g_ir.shape))\n",
        "print(\"Shape of G_br  is: \"+str(G_br.shape))\n",
        " \n",
        "np.random.seed(144)\n",
        "h_bs_t = (np.random.randn( M,mini_batch_size)+1j*np.random.randn( M,mini_batch_size))*np.sqrt(b_bs/2)\n",
        "h_is_t = (np.random.randn( 1,mini_batch_size)+1j*np.random.randn( 1,mini_batch_size))*np.sqrt(b_is/2)\n",
        "h_id_t = (np.random.randn( 1,mini_batch_size)+1j*np.random.randn( 1,mini_batch_size))*np.sqrt(b_id/2)\n",
        "h_sd_t = (np.random.randn( 1,mini_batch_size)+1j*np.random.randn( 1,mini_batch_size))*np.sqrt(b_sd/2)\n",
        "g_rs_t = (np.random.randn( N,mini_batch_size)+1j*np.random.randn( N,mini_batch_size))*np.sqrt(b_rs/2)\n",
        "g_rd_t = (np.random.randn( N,mini_batch_size)+1j*np.random.randn( N,mini_batch_size))*np.sqrt(b_rd/2)\n",
        "g_ir_t = (np.random.randn( N,mini_batch_size)+1j*np.random.randn( N,mini_batch_size))*np.sqrt(b_ir/2)\n",
        "G_br_t = (np.random.randn( N,M*mini_batch_size)+1j*np.random.randn( N,M*mini_batch_size))*np.sqrt(b_br/2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of h_bs  is: (500, 4, 1000)\n",
            "Shape of h_is  is: (500, 1, 1000)\n",
            "Shape of h_id  is: (500, 1, 1000)\n",
            "Shape of h_sd  is: (500, 1, 1000)\n",
            "Shape of g_rs  is: (500, 32, 1000)\n",
            "Shape of g_rd  is: (500, 32, 1000)\n",
            "Shape of g_ir  is: (500, 32, 1000)\n",
            "Shape of G_br  is: (500, 32, 4000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOLKgtiZA62D",
        "outputId": "8e4a3e24-3637-4b3a-8e73-cd6c097060eb"
      },
      "source": [
        "dims = [2*(N*M+M+N+1),5*(N*M+M+N+1), 6*(N*M+M+N+1), 5*(N*M+M+N+1),4*(N*M+M+N+1),1*(N*M+M+N+1), 2*N+2]\n",
        "seed = np.random.randint(400)\n",
        "#seed = 12\n",
        "print(seed)\n",
        "tf.reset_default_graph()\n",
        "W1=tf.get_variable(\"W1\",[dims[1],dims[0]],dtype=\"float64\", initializer = tf.contrib.layers.xavier_initializer(seed = seed))\n",
        "b1=tf.get_variable(\"b1\",[dims[1],1],dtype=\"float64\", initializer = tf.zeros_initializer())\n",
        "W2=tf.get_variable(\"W2\",[dims[2],dims[1]],dtype=\"float64\", initializer = tf.contrib.layers.xavier_initializer(seed = seed))\n",
        "b2=tf.get_variable(\"b2\",[dims[2],1],dtype=\"float64\", initializer = tf.zeros_initializer())\n",
        "W3=tf.get_variable(\"W3\",[dims[3],dims[2]],dtype=\"float64\", initializer = tf.contrib.layers.xavier_initializer(seed = seed))\n",
        "b3=tf.get_variable(\"b3\",[dims[3],1],dtype=\"float64\", initializer = tf.zeros_initializer())\n",
        "W4=tf.get_variable(\"W4\",[dims[4],dims[3]],dtype=\"float64\", initializer = tf.contrib.layers.xavier_initializer(seed = seed))\n",
        "b4=tf.get_variable(\"b4\",[dims[4],1],dtype=\"float64\", initializer = tf.zeros_initializer())\n",
        "W5=tf.get_variable(\"W5\",[dims[5],dims[4]],dtype=\"float64\", initializer = tf.contrib.layers.xavier_initializer(seed = seed))\n",
        "b5=tf.get_variable(\"b5\",[dims[5],1],dtype=\"float64\", initializer = tf.zeros_initializer())\n",
        "W6=tf.get_variable(\"W6\",[dims[6],dims[5]],dtype=\"float64\", initializer = tf.contrib.layers.xavier_initializer(seed = seed))\n",
        "b6=tf.get_variable(\"b6\",[dims[6],1],dtype=\"float64\", initializer = tf.zeros_initializer())\n",
        "#tau = tf.get_variable(\"tau\",[1,1],dtype=\"float64\", initializer = tf.zeros_initializer())\n",
        " \n",
        "X = tf.placeholder(tf.float64, shape=(2*(N*M+M+N+1),None))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "373\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwqDWbowBNCX"
      },
      "source": [
        "Z1 = tf.add(tf.matmul(W1,X),b1)                     # Linear Layer\n",
        "mean, variance = tf.nn.moments(Z1, axes= [1] ,keepdims=True)\n",
        "Z1_BN = tf.nn.batch_normalization(Z1, mean=mean, variance= variance, scale=1.0, offset= None, variance_epsilon=10**-5) # Batch Normalization Layer\n",
        "A1 = tf.nn.dropout(tf.nn.relu(Z1_BN),rate = 0.0, seed= 95)        # Relu Layer\n",
        " \n",
        "Z2 = tf.add(tf.matmul(W2,A1),b2)                     # Linear Layer\n",
        "mean, variance = tf.nn.moments(Z2, axes= [1] ,keepdims=True)\n",
        "Z2_BN = tf.nn.batch_normalization(Z2, mean=mean, variance= variance, scale=1.0, offset= None, variance_epsilon=10**-5) # Batch Normalization Layer\n",
        "A2 = tf.nn.dropout(tf.nn.relu(Z2),rate = 0.0, seed= 195) \n",
        " \n",
        " \n",
        "Z3 = tf.add(tf.matmul(W3,A2),b3)                     # Linear Layer\n",
        "mean, variance = tf.nn.moments(Z3, axes= [1] ,keepdims=True)\n",
        "Z3_BN = tf.nn.batch_normalization(Z3, mean=mean, variance= variance, scale=1, offset= None, variance_epsilon=10**-5) # Batch Normalization Layer\n",
        "A3 = tf.nn.relu(Z3)\n",
        "\n",
        "Z4 = tf.add(tf.matmul(W4,A3),b4)                     # Linear Layer\n",
        "mean, variance = tf.nn.moments(Z4, axes= [1],keepdims=True)\n",
        "Z4_BN = tf.nn.batch_normalization(Z4, mean=mean, variance= variance, scale=1, offset= None, variance_epsilon=10**-5) # Batch Normalization Layer\n",
        "A4 = tf.nn.relu(Z4) \n",
        "\n",
        "\n",
        "Z5 = tf.add(tf.matmul(W5,A4),b5)                     # Linear Layer\n",
        "mean, variance = tf.nn.moments(Z5, axes= [1],keepdims=True)\n",
        "Z5_BN = tf.nn.batch_normalization(Z5, mean=mean, variance= variance, scale=1, offset= None, variance_epsilon=10**-5) # Batch Normalization Layer\n",
        "A5 = tf.nn.leaky_relu(Z5_BN,0.) \n",
        "theta1 = tf.nn.sigmoid(Z5)\n",
        "Z6 = tf.add(tf.matmul(W6,A5),b6)                     # Linear Layer\n",
        "# mean, variance = tf.nn.moments(Z6, axes= [1],keepdims=True)\n",
        "# Z6_BN = tf.nn.batch_normalization(Z6, mean=mean, variance= variance/variance, scale=1, offset= None, variance_epsilon=10**-5) # Batch Normalization Layer\n",
        "theta1 = tf.nn.sigmoid(Z6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjsTiX8zZo4z"
      },
      "source": [
        "## The cost function is,\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLSwW0PBcvq1"
      },
      "source": [
        "real = tf.zeros([N,tf.shape(X)[1]],dtype=tf.float64)\n",
        "#tau = tf.constant(0.1,dtype= tf.float64)\n",
        "log_base = tf.constant(2,dtype=tf.float64)\n",
        "etta = 1 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RqN-u2hIpdPX"
      },
      "source": [
        "theta_et1, theta_it1,tau,lemda = tf.split(theta1, num_or_size_splits= [N,N,1,1], axis = 0)\n",
        "theta_et = tf.exp(tf.complex(real,theta_et1*2*np.pi))\n",
        "theta_it = tf.exp(tf.complex(real,theta_it1*2*np.pi))\n",
        "# theta_et =loadmat('/content/drive/MyDrive/DL_prejects/Cost_varification_data/theta_et.mat')['theta_et']\n",
        "# print(theta_et.shape)  \n",
        "# theta_it = loadmat('/content/drive/MyDrive/DL_prejects/Cost_varification_data/theta_it.mat')['theta_it']\n",
        "# print(theta_it.shape) \n",
        "\n",
        "g_theta_et1 = tf.reshape(tf.complex(X[0:N*M,:],X[N*M:2*N*M,:]),[N,M*tf.shape(X)[1]])\n",
        "\n",
        "g_theta_et2 = g_theta_et1*tf.repeat(theta_et,M,axis=1)\n",
        "\n",
        "#g_theta_et6 = tf.reduce_sum(tf.transpose(g_theta_et2),axis = 1, keepdims=True)\n",
        "\n",
        "g_theta_et = tf.transpose(tf.reshape((tf.reduce_sum(g_theta_et2,axis = 0, keepdims= True)),[tf.shape(X)[1],M]),perm=[1,0])\n",
        "\n",
        "h_bs_c = tf.complex(X[2*N*M:2*N*M+M,:],X[2*N*M+M:2*N*M+2*M,:])\n",
        "\n",
        "\n",
        "Es = etta*tau*P_b*(tf.real(tf.norm((h_bs_c+g_theta_et), axis=0))**2)\n",
        "\n",
        "P_s = etta*(tau/(1-tau))*P_b*(tf.real(tf.norm((h_bs_c+g_theta_et), axis=0))**2)\n",
        "\n",
        "\n",
        "g_theta_it1 = tf.complex(X[2*N*M+2*M:2*N*M+2*M+N,:],X[2*N*M+2*M+N:2*N*M+2*M+2*N,:])\n",
        "g_theta_it = tf.reduce_sum(g_theta_it1*theta_it,axis = 0)\n",
        "h_sd_c = tf.complex(X[2*N*M+2*M+2*N:2*N*M+2*M+2*N+1,:],X[2*N*M+2*M+2*N+1:2*N*M+2*M+2*N+2,:])\n",
        "rd = P_s*(tf.abs((h_sd_c+g_theta_it)**2))/((sigma_n)**2)\n",
        "r = (1-tau)*tf.log(1+rd)/tf.log(log_base)\n",
        "cost = -tf.reduce_mean(r)*10\n",
        "cost1 = cost+lemda*tf.norm(theta1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aj7ZnWoGEd5R"
      },
      "source": [
        "\n",
        "# g_rs = loadmat('/content/drive/MyDrive/DL_prejects/Cost_varification_data/g_rs.mat')['g_rs']\n",
        "# G_br = loadmat('/content/drive/MyDrive/DL_prejects/Cost_varification_data/G_br.mat')['G_br']\n",
        "# h_bs = loadmat('/content/drive/MyDrive/DL_prejects/Cost_varification_data/h_bs.mat')['h_bs']\n",
        "# g_rd = loadmat('/content/drive/MyDrive/DL_prejects/Cost_varification_data/g_rd.mat')['g_rd']\n",
        "# h_sd = loadmat('/content/drive/MyDrive/DL_prejects/Cost_varification_data/h_sd.mat')['h_sd']\n",
        "\n",
        "# # j =0\n",
        "# mini_batch_size = 3;\n",
        "# g_rs1 =  np.repeat(g_rs,M,axis = 1)\n",
        "# F = np.reshape(G_br*g_rs1, [N*M,mini_batch_size])\n",
        "# F = np.concatenate([np.real(F),np.imag(F)],axis=0)\n",
        "# F1 = np.concatenate([np.real(h_bs),np.imag(h_bs)],axis=0)\n",
        "# F2 = g_rd*g_rs\n",
        "# F2 = np.concatenate([np.real(F2),np.imag(F2)],axis=0)\n",
        "# F3 = np.concatenate([np.real(h_sd),np.imag(h_sd)],axis=0)\n",
        "# F = np.concatenate([F,F1,F2,F3],axis=0)\n",
        "# F.shape\n",
        "# sess = tf.Session()\n",
        "# init = tf.global_variables_initializer()\n",
        "# sess.run(init)\n",
        "# print(sess.run(r,feed_dict={X:F}))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dlzO2CxvWrB"
      },
      "source": [
        "decay = 0.33"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYNJOh4GBmoI"
      },
      "source": [
        "lr = 0.001\n",
        "step_rate =400*10**2\n",
        "\n",
        "\n",
        "global_step = tf.Variable(0, trainable=False)\n",
        "increment_global_step = tf.assign(global_step, global_step + 1)\n",
        "learning_rate = tf.train.exponential_decay(lr, global_step, step_rate, decay, staircase=True)\n",
        "\n",
        "  \n",
        "optimizer = tf.train.AdamOptimizer(learning_rate= learning_rate )\n",
        "minimize_loss = optimizer.minimize(cost, global_step = global_step)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4G7IyIKGBnjb"
      },
      "source": [
        "init = tf.global_variables_initializer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBhx94XKVh4n"
      },
      "source": [
        "sess = tf.Session()\n",
        "sess.run(init)\n",
        "num_epochs = 500"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pbaykM2CBqGX",
        "outputId": "4934e2a1-4be2-4ae6-c410-a9c319ebd0b3"
      },
      "source": [
        "cost_train_plt = []\n",
        "cost_test_plt = []\n",
        "start_time=time.time()\n",
        "for epoch in range(num_epochs):\n",
        "  epoch1 = epoch\n",
        "  #np.random.shuffle(h_train)\n",
        "  for j in range(num_batches):\n",
        "    g_rs1 =  np.repeat(g_rs[j],M,axis = 1)\n",
        "    F = np.reshape(G_br[j]*g_rs1, [N*M,mini_batch_size])\n",
        "    F = np.concatenate([np.real(F),np.imag(F)],axis=0)\n",
        "    F1 = np.concatenate([np.real(h_bs[j]),np.imag(h_bs[j])],axis=0)\n",
        "    F2 = g_rd[j]*g_rs[j]\n",
        "    F2 = np.concatenate([np.real(F2),np.imag(F2)],axis=0)\n",
        "    F3 = np.concatenate([np.real(h_sd[j]),np.imag(h_sd[j])],axis=0)\n",
        "    F = np.concatenate([F,F1,F2,F3],axis=0)\n",
        "    sess.run(minimize_loss,feed_dict={X:F})\n",
        "    #print(sess.run(cost, feed_dict={X:h_train[j]}))\n",
        "  if (epoch) % display_step == 0:\n",
        "      print('Learning Rate',sess.run(optimizer._lr))\n",
        "      #print('Sum_Rate:',sess.run(sum_R, feed_dict={X:h_train[j]}))\n",
        "      c = sess.run(cost, feed_dict={X:F})\n",
        "      cost_train_plt.append(np.reshape(c,[1]))\n",
        "\n",
        "      g_rs1 =  np.repeat(g_rs_t,M,axis = 1)\n",
        "      F = np.reshape(G_br_t*g_rs1, [N*M,mini_batch_size])\n",
        "      F = np.concatenate([np.real(F),np.imag(F)],axis=0)\n",
        "      F1 = np.concatenate([np.real(h_bs_t),np.imag(h_bs_t)],axis=0)\n",
        "      F2 = g_rd_t*g_rs_t\n",
        "      F2 = np.concatenate([np.real(F2),np.imag(F2)],axis=0)\n",
        "      F3 = np.concatenate([np.real(h_sd_t),np.imag(h_sd_t)],axis=0)\n",
        "      F_test = np.concatenate([F,F1,F2,F3],axis=0)\n",
        "      \n",
        "      cost_test=sess.run(cost, feed_dict={X:F_test}) \n",
        "      cost_test_plt.append(np.reshape(cost_test,[1]))\n",
        "      print(\"Epoch: \" +str(epoch) + \" Training Loss= \"+ str(c) + \" Validation Loss= \"+ str(cost_test) )\n",
        "      #print(sess.run(P,feed_dict={X:h_train[j]}))\n",
        "duration = time.time()-start_time        \n",
        "print(\"The duration for training is: \"+str(duration)) \n",
        "print(\"The cost for test data is: \" + str(cost_test))\n",
        "plt.subplot(223)\n",
        "plt.plot(cost_train_plt ,label= 'training Loss')\n",
        "plt.plot(cost_test_plt, label= 'test loss')\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
        "plt.show()\n",
        "print(\"Optimization Finished!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learning Rate 0.001\n",
            "Epoch: 0 Training Loss= -32.50200613553476 Validation Loss= -32.580109561031215\n",
            "Learning Rate 0.001\n",
            "Epoch: 1 Training Loss= -32.81312730231989 Validation Loss= -32.86926993466919\n",
            "Learning Rate 0.001\n",
            "Epoch: 2 Training Loss= -33.03913159052766 Validation Loss= -33.108385628398466\n",
            "Learning Rate 0.001\n",
            "Epoch: 3 Training Loss= -33.171125777526285 Validation Loss= -33.221408733851035\n",
            "Learning Rate 0.001\n",
            "Epoch: 4 Training Loss= -33.252688651974694 Validation Loss= -33.29556553661224\n",
            "Learning Rate 0.001\n",
            "Epoch: 5 Training Loss= -33.345794554646396 Validation Loss= -33.377246196442904\n",
            "Learning Rate 0.001\n",
            "Epoch: 6 Training Loss= -33.44925173051428 Validation Loss= -33.46756628015206\n",
            "Learning Rate 0.001\n",
            "Epoch: 7 Training Loss= -33.55907558910259 Validation Loss= -33.60232871235267\n",
            "Learning Rate 0.001\n",
            "Epoch: 8 Training Loss= -33.68715250957767 Validation Loss= -33.70175808254183\n",
            "Learning Rate 0.001\n",
            "Epoch: 9 Training Loss= -33.78072404014321 Validation Loss= -33.803253920801446\n",
            "Learning Rate 0.001\n",
            "Epoch: 10 Training Loss= -33.840471464267026 Validation Loss= -33.86950249357709\n",
            "Learning Rate 0.001\n",
            "Epoch: 11 Training Loss= -33.874824351892194 Validation Loss= -33.90992305086214\n",
            "Learning Rate 0.001\n",
            "Epoch: 12 Training Loss= -33.89580332830414 Validation Loss= -33.93301680279683\n",
            "Learning Rate 0.001\n",
            "Epoch: 13 Training Loss= -33.92991818087112 Validation Loss= -33.954194257539974\n",
            "Learning Rate 0.001\n",
            "Epoch: 14 Training Loss= -33.95073078026606 Validation Loss= -33.9767743235171\n",
            "Learning Rate 0.001\n",
            "Epoch: 15 Training Loss= -33.96694999353299 Validation Loss= -33.99958738924285\n",
            "Learning Rate 0.001\n",
            "Epoch: 16 Training Loss= -33.97991566299289 Validation Loss= -34.01188547410412\n",
            "Learning Rate 0.001\n",
            "Epoch: 17 Training Loss= -33.992372201802745 Validation Loss= -34.019939166254176\n",
            "Learning Rate 0.001\n",
            "Epoch: 18 Training Loss= -34.00279464905003 Validation Loss= -34.033049910906385\n",
            "Learning Rate 0.001\n",
            "Epoch: 19 Training Loss= -34.01183291029141 Validation Loss= -34.03585525249585\n",
            "Learning Rate 0.001\n",
            "Epoch: 20 Training Loss= -34.019159060207585 Validation Loss= -34.05488585301351\n",
            "Learning Rate 0.001\n",
            "Epoch: 21 Training Loss= -34.0360295644532 Validation Loss= -34.05449032525683\n",
            "Learning Rate 0.001\n",
            "Epoch: 22 Training Loss= -34.037584444749875 Validation Loss= -34.06896307235699\n",
            "Learning Rate 0.001\n",
            "Epoch: 23 Training Loss= -34.037616939792585 Validation Loss= -34.0558288125124\n",
            "Learning Rate 0.001\n",
            "Epoch: 24 Training Loss= -34.04700669539274 Validation Loss= -34.08079367908559\n",
            "Learning Rate 0.001\n",
            "Epoch: 25 Training Loss= -34.05091594621639 Validation Loss= -34.07347197606227\n",
            "Learning Rate 0.001\n",
            "Epoch: 26 Training Loss= -34.05778039297824 Validation Loss= -34.082032226402\n",
            "Learning Rate 0.001\n",
            "Epoch: 27 Training Loss= -34.07100533767902 Validation Loss= -34.097480180305915\n",
            "Learning Rate 0.001\n",
            "Epoch: 28 Training Loss= -34.05841934844025 Validation Loss= -34.07381486062506\n",
            "Learning Rate 0.001\n",
            "Epoch: 29 Training Loss= -34.048809660641524 Validation Loss= -34.08359841972784\n",
            "Learning Rate 0.001\n",
            "Epoch: 30 Training Loss= -34.06036497268537 Validation Loss= -34.08799869285456\n",
            "Learning Rate 0.001\n",
            "Epoch: 31 Training Loss= -34.07173435234907 Validation Loss= -34.09420779089052\n",
            "Learning Rate 0.001\n",
            "Epoch: 32 Training Loss= -34.07460207774119 Validation Loss= -34.09661882030684\n",
            "Learning Rate 0.001\n",
            "Epoch: 33 Training Loss= -34.07774426465521 Validation Loss= -34.09489156680198\n",
            "Learning Rate 0.001\n",
            "Epoch: 34 Training Loss= -34.07386980438502 Validation Loss= -34.101090179813276\n",
            "Learning Rate 0.001\n",
            "Epoch: 35 Training Loss= -34.083937940503674 Validation Loss= -34.10333673889029\n",
            "Learning Rate 0.001\n",
            "Epoch: 36 Training Loss= -34.07669240043993 Validation Loss= -34.112354315014876\n",
            "Learning Rate 0.001\n",
            "Epoch: 37 Training Loss= -34.07894352968982 Validation Loss= -34.099549922187045\n",
            "Learning Rate 0.001\n",
            "Epoch: 38 Training Loss= -34.07843922732799 Validation Loss= -34.09636407475769\n",
            "Learning Rate 0.001\n",
            "Epoch: 39 Training Loss= -34.089347707740934 Validation Loss= -34.10339046318989\n",
            "Learning Rate 0.001\n",
            "Epoch: 40 Training Loss= -34.09271972232085 Validation Loss= -34.09828265392289\n",
            "Learning Rate 0.001\n",
            "Epoch: 41 Training Loss= -34.08446229093897 Validation Loss= -34.11668060419831\n",
            "Learning Rate 0.001\n",
            "Epoch: 42 Training Loss= -34.07473945825927 Validation Loss= -34.10944858738672\n",
            "Learning Rate 0.001\n",
            "Epoch: 43 Training Loss= -34.085572757349134 Validation Loss= -34.10206469776978\n",
            "Learning Rate 0.001\n",
            "Epoch: 44 Training Loss= -34.10289886890184 Validation Loss= -34.11954206938992\n",
            "Learning Rate 0.001\n",
            "Epoch: 45 Training Loss= -34.094726797771756 Validation Loss= -34.11368556198361\n",
            "Learning Rate 0.001\n",
            "Epoch: 46 Training Loss= -34.102687564243624 Validation Loss= -34.10350499151219\n",
            "Learning Rate 0.001\n",
            "Epoch: 47 Training Loss= -34.102824702055486 Validation Loss= -34.12986656150152\n",
            "Learning Rate 0.001\n",
            "Epoch: 48 Training Loss= -34.089240994300795 Validation Loss= -34.12210474878158\n",
            "Learning Rate 0.001\n",
            "Epoch: 49 Training Loss= -34.10515595049671 Validation Loss= -34.132054034418815\n",
            "Learning Rate 0.001\n",
            "Epoch: 50 Training Loss= -34.109906456543094 Validation Loss= -34.13341937504026\n",
            "Learning Rate 0.001\n",
            "Epoch: 51 Training Loss= -34.10527234070769 Validation Loss= -34.13101682465666\n",
            "Learning Rate 0.001\n",
            "Epoch: 52 Training Loss= -34.100808984214694 Validation Loss= -34.128980644851204\n",
            "Learning Rate 0.001\n",
            "Epoch: 53 Training Loss= -34.096150078529604 Validation Loss= -34.129155867003256\n",
            "Learning Rate 0.001\n",
            "Epoch: 54 Training Loss= -34.10964111958941 Validation Loss= -34.130203990939776\n",
            "Learning Rate 0.001\n",
            "Epoch: 55 Training Loss= -34.11559183505889 Validation Loss= -34.14182915284209\n",
            "Learning Rate 0.001\n",
            "Epoch: 56 Training Loss= -34.122846843871585 Validation Loss= -34.14284366596278\n",
            "Learning Rate 0.001\n",
            "Epoch: 57 Training Loss= -34.11981361149438 Validation Loss= -34.13595771734911\n",
            "Learning Rate 0.001\n",
            "Epoch: 58 Training Loss= -34.10636342135534 Validation Loss= -34.12948834007808\n",
            "Learning Rate 0.001\n",
            "Epoch: 59 Training Loss= -34.10495576596132 Validation Loss= -34.12363638946428\n",
            "Learning Rate 0.001\n",
            "Epoch: 60 Training Loss= -34.122655306087054 Validation Loss= -34.12450478485393\n",
            "Learning Rate 0.001\n",
            "Epoch: 61 Training Loss= -34.121721572257016 Validation Loss= -34.132052991677504\n",
            "Learning Rate 0.001\n",
            "Epoch: 62 Training Loss= -34.13048972511469 Validation Loss= -34.138122575616116\n",
            "Learning Rate 0.001\n",
            "Epoch: 63 Training Loss= -34.12691142525355 Validation Loss= -34.132113188536756\n",
            "Learning Rate 0.001\n",
            "Epoch: 64 Training Loss= -34.121763424780546 Validation Loss= -34.14179590816049\n",
            "Learning Rate 0.001\n",
            "Epoch: 65 Training Loss= -34.12088970497675 Validation Loss= -34.14927125731212\n",
            "Learning Rate 0.001\n",
            "Epoch: 66 Training Loss= -34.12013676226343 Validation Loss= -34.14739854981454\n",
            "Learning Rate 0.001\n",
            "Epoch: 67 Training Loss= -34.12396514409916 Validation Loss= -34.145486501599805\n",
            "Learning Rate 0.001\n",
            "Epoch: 68 Training Loss= -34.12339980182 Validation Loss= -34.14564362725682\n",
            "Learning Rate 0.001\n",
            "Epoch: 69 Training Loss= -34.1302442579455 Validation Loss= -34.14322701265987\n",
            "Learning Rate 0.001\n",
            "Epoch: 70 Training Loss= -34.13417700309199 Validation Loss= -34.147222322445636\n",
            "Learning Rate 0.001\n",
            "Epoch: 71 Training Loss= -34.11913940437982 Validation Loss= -34.14786742809707\n",
            "Learning Rate 0.001\n",
            "Epoch: 72 Training Loss= -34.128519249346006 Validation Loss= -34.142725759373235\n",
            "Learning Rate 0.001\n",
            "Epoch: 73 Training Loss= -34.133843389044394 Validation Loss= -34.13768466602666\n",
            "Learning Rate 0.001\n",
            "Epoch: 74 Training Loss= -34.121379331465974 Validation Loss= -34.13733537989257\n",
            "Learning Rate 0.001\n",
            "Epoch: 75 Training Loss= -34.13021682779141 Validation Loss= -34.135106212912795\n",
            "Learning Rate 0.001\n",
            "Epoch: 76 Training Loss= -34.12235853646432 Validation Loss= -34.15108265835094\n",
            "Learning Rate 0.001\n",
            "Epoch: 77 Training Loss= -34.1293872732668 Validation Loss= -34.15504295353508\n",
            "Learning Rate 0.001\n",
            "Epoch: 78 Training Loss= -34.1370529056511 Validation Loss= -34.151120381746004\n",
            "Learning Rate 0.00033000004\n",
            "Epoch: 79 Training Loss= -34.13489502739522 Validation Loss= -34.14395340387816\n",
            "Learning Rate 0.00033000004\n",
            "Epoch: 80 Training Loss= -34.16495344629084 Validation Loss= -34.1903544576562\n",
            "Learning Rate 0.00033000004\n",
            "Epoch: 81 Training Loss= -34.166639110240226 Validation Loss= -34.196679172207865\n",
            "Learning Rate 0.00033000004\n",
            "Epoch: 82 Training Loss= -34.17213225781727 Validation Loss= -34.189155427031125\n",
            "Learning Rate 0.00033000004\n",
            "Epoch: 83 Training Loss= -34.17922230946391 Validation Loss= -34.18637490295498\n",
            "Learning Rate 0.00033000004\n",
            "Epoch: 84 Training Loss= -34.18279690254862 Validation Loss= -34.200731217744945\n",
            "Learning Rate 0.00033000004\n",
            "Epoch: 85 Training Loss= -34.18091803587189 Validation Loss= -34.18989419673919\n",
            "Learning Rate 0.00033000004\n",
            "Epoch: 86 Training Loss= -34.185227237018154 Validation Loss= -34.191001279602\n",
            "Learning Rate 0.00033000004\n",
            "Epoch: 87 Training Loss= -34.18581679397478 Validation Loss= -34.1911915604292\n",
            "Learning Rate 0.00033000004\n",
            "Epoch: 88 Training Loss= -34.188382428649454 Validation Loss= -34.19985840864485\n",
            "Learning Rate 0.00033000004\n",
            "Epoch: 89 Training Loss= -34.184025272697106 Validation Loss= -34.19654081664697\n",
            "Learning Rate 0.00033000004\n",
            "Epoch: 90 Training Loss= -34.19018296207228 Validation Loss= -34.19885437547955\n",
            "Learning Rate 0.00033000004\n",
            "Epoch: 91 Training Loss= -34.19009657509236 Validation Loss= -34.19226730433867\n",
            "Learning Rate 0.00033000004\n",
            "Epoch: 92 Training Loss= -34.18700267451986 Validation Loss= -34.198339898645074\n",
            "Learning Rate 0.00033000004\n",
            "Epoch: 93 Training Loss= -34.19758930332594 Validation Loss= -34.19617801746653\n",
            "Learning Rate 0.00033000004\n",
            "Epoch: 94 Training Loss= -34.19103243527043 Validation Loss= -34.19629619836908\n",
            "Learning Rate 0.00033000004\n",
            "Epoch: 95 Training Loss= -34.191634431514395 Validation Loss= -34.199197156083144\n",
            "Learning Rate 0.00033000004\n",
            "Epoch: 96 Training Loss= -34.197232529936855 Validation Loss= -34.20014318312275\n",
            "Learning Rate 0.00033000004\n",
            "Epoch: 97 Training Loss= -34.197463776333656 Validation Loss= -34.19058157998576\n",
            "Learning Rate 0.00033000004\n",
            "Epoch: 98 Training Loss= -34.19883590339929 Validation Loss= -34.199903540861236\n",
            "Learning Rate 0.00033000004\n",
            "Epoch: 99 Training Loss= -34.19633032448136 Validation Loss= -34.208484192511364\n",
            "Learning Rate 0.00033000004\n",
            "Epoch: 100 Training Loss= -34.1959708185135 Validation Loss= -34.19187255535045\n",
            "Learning Rate 0.00033000004\n",
            "Epoch: 101 Training Loss= -34.20129348869451 Validation Loss= -34.202865089910034\n",
            "Learning Rate 0.00033000004\n",
            "Epoch: 102 Training Loss= -34.19896534191156 Validation Loss= -34.20212766370999\n",
            "Learning Rate 0.00033000004\n",
            "Epoch: 103 Training Loss= -34.19645124192549 Validation Loss= -34.19180149531762\n",
            "Learning Rate 0.00033000004\n",
            "Epoch: 104 Training Loss= -34.20468874367121 Validation Loss= -34.20635324533661\n",
            "Learning Rate 0.00033000004\n",
            "Epoch: 105 Training Loss= -34.20955381643176 Validation Loss= -34.20008328410518\n",
            "Learning Rate 0.00033000004\n",
            "Epoch: 106 Training Loss= -34.19809557297743 Validation Loss= -34.197932797188194\n",
            "Learning Rate 0.00033000004\n",
            "Epoch: 107 Training Loss= -34.20092603773426 Validation Loss= -34.19709076459064\n",
            "Learning Rate 0.00033000004\n",
            "Epoch: 108 Training Loss= -34.20039194123564 Validation Loss= -34.20407973239849\n",
            "Learning Rate 0.00033000004\n",
            "Epoch: 109 Training Loss= -34.20667526366025 Validation Loss= -34.207085972172806\n",
            "Learning Rate 0.00033000004\n",
            "Epoch: 110 Training Loss= -34.20478213757763 Validation Loss= -34.20296689265336\n",
            "Learning Rate 0.00033000004\n",
            "Epoch: 111 Training Loss= -34.19932348615525 Validation Loss= -34.194168309827134\n",
            "Learning Rate 0.00033000004\n",
            "Epoch: 112 Training Loss= -34.20788869940752 Validation Loss= -34.189560863750465\n",
            "Learning Rate 0.00033000004\n",
            "Epoch: 113 Training Loss= -34.205674211150864 Validation Loss= -34.19754348281706\n",
            "Learning Rate 0.00033000004\n",
            "Epoch: 114 Training Loss= -34.20896597759192 Validation Loss= -34.20187826922256\n",
            "Learning Rate 0.00033000004\n",
            "Epoch: 115 Training Loss= -34.207376977383404 Validation Loss= -34.195252342374744\n",
            "Learning Rate 0.00033000004\n",
            "Epoch: 116 Training Loss= -34.21061623103255 Validation Loss= -34.20075249695519\n",
            "Learning Rate 0.00033000004\n",
            "Epoch: 117 Training Loss= -34.20836585703217 Validation Loss= -34.20699557750888\n",
            "Learning Rate 0.00033000004\n",
            "Epoch: 118 Training Loss= -34.21569808770841 Validation Loss= -34.205563300053456\n",
            "Learning Rate 0.00033000004\n",
            "Epoch: 119 Training Loss= -34.21299442020708 Validation Loss= -34.2043971314107\n",
            "Learning Rate 0.00033000004\n",
            "Epoch: 120 Training Loss= -34.20893771945742 Validation Loss= -34.20210014678698\n",
            "Learning Rate 0.00033000004\n",
            "Epoch: 121 Training Loss= -34.21037104548268 Validation Loss= -34.2002205446111\n",
            "Learning Rate 0.00033000004\n",
            "Epoch: 122 Training Loss= -34.213458007301774 Validation Loss= -34.19878686090955\n",
            "Learning Rate 0.00033000004\n",
            "Epoch: 123 Training Loss= -34.20749846047001 Validation Loss= -34.18971023272087\n",
            "Learning Rate 0.00033000004\n",
            "Epoch: 124 Training Loss= -34.21602836701184 Validation Loss= -34.20548077476573\n",
            "Learning Rate 0.00033000004\n",
            "Epoch: 125 Training Loss= -34.21498611336877 Validation Loss= -34.194164776200914\n",
            "Learning Rate 0.00033000004\n",
            "Epoch: 126 Training Loss= -34.211665577982075 Validation Loss= -34.19354089279566\n",
            "Learning Rate 0.00033000004\n",
            "Epoch: 127 Training Loss= -34.218200682067675 Validation Loss= -34.20069062200017\n",
            "Learning Rate 0.00033000004\n",
            "Epoch: 128 Training Loss= -34.21630923844199 Validation Loss= -34.19891735564724\n",
            "Learning Rate 0.00033000004\n",
            "Epoch: 129 Training Loss= -34.220609485365806 Validation Loss= -34.1957017419687\n",
            "Learning Rate 0.00033000004\n",
            "Epoch: 130 Training Loss= -34.216797453927896 Validation Loss= -34.199547022633915\n",
            "Learning Rate 0.00033000004\n",
            "Epoch: 131 Training Loss= -34.216267696232705 Validation Loss= -34.19756362264373\n",
            "Learning Rate 0.00033000004\n",
            "Epoch: 132 Training Loss= -34.22207746357771 Validation Loss= -34.19521518764777\n",
            "Learning Rate 0.00033000004\n",
            "Epoch: 133 Training Loss= -34.21871101454344 Validation Loss= -34.19722207707329\n",
            "Learning Rate 0.00033000004\n",
            "Epoch: 134 Training Loss= -34.215898012572225 Validation Loss= -34.20791648245266\n",
            "Learning Rate 0.00033000004\n",
            "Epoch: 135 Training Loss= -34.22312018074383 Validation Loss= -34.20155530487159\n",
            "Learning Rate 0.00033000004\n",
            "Epoch: 136 Training Loss= -34.22415988957089 Validation Loss= -34.20920395024009\n",
            "Learning Rate 0.00033000004\n",
            "Epoch: 137 Training Loss= -34.22024099020731 Validation Loss= -34.197176378422036\n",
            "Learning Rate 0.00033000004\n",
            "Epoch: 138 Training Loss= -34.22085042309803 Validation Loss= -34.20469647861363\n",
            "Learning Rate 0.00033000004\n",
            "Epoch: 139 Training Loss= -34.22331202588472 Validation Loss= -34.19455546761612\n",
            "Learning Rate 0.00033000004\n",
            "Epoch: 140 Training Loss= -34.2228030531171 Validation Loss= -34.1909276083415\n",
            "Learning Rate 0.00033000004\n",
            "Epoch: 141 Training Loss= -34.22447860114898 Validation Loss= -34.18803811139182\n",
            "Learning Rate 0.00033000004\n",
            "Epoch: 142 Training Loss= -34.22698249732686 Validation Loss= -34.19302421422587\n",
            "Learning Rate 0.00033000004\n",
            "Epoch: 143 Training Loss= -34.22770414078975 Validation Loss= -34.197237773286446\n",
            "Learning Rate 0.00033000004\n",
            "Epoch: 144 Training Loss= -34.2256462693078 Validation Loss= -34.19294664362944\n",
            "Learning Rate 0.00033000004\n",
            "Epoch: 145 Training Loss= -34.219485539395016 Validation Loss= -34.193689075619346\n",
            "Learning Rate 0.00033000004\n",
            "Epoch: 146 Training Loss= -34.22450085156391 Validation Loss= -34.19154436778619\n",
            "Learning Rate 0.00033000004\n",
            "Epoch: 147 Training Loss= -34.22388004441664 Validation Loss= -34.19460404433188\n",
            "Learning Rate 0.00033000004\n",
            "Epoch: 148 Training Loss= -34.231368876083145 Validation Loss= -34.19044216231547\n",
            "Learning Rate 0.00033000004\n",
            "Epoch: 149 Training Loss= -34.225508279127624 Validation Loss= -34.203878585973115\n",
            "Learning Rate 0.00033000004\n",
            "Epoch: 150 Training Loss= -34.22654601464479 Validation Loss= -34.20122698715597\n",
            "Learning Rate 0.00033000004\n",
            "Epoch: 151 Training Loss= -34.2287779782608 Validation Loss= -34.20061623420901\n",
            "Learning Rate 0.00033000004\n",
            "Epoch: 152 Training Loss= -34.23340870984524 Validation Loss= -34.19665607173358\n",
            "Learning Rate 0.00033000004\n",
            "Epoch: 153 Training Loss= -34.22430291160975 Validation Loss= -34.19349261830674\n",
            "Learning Rate 0.00033000004\n",
            "Epoch: 154 Training Loss= -34.234148151038966 Validation Loss= -34.20036569155746\n",
            "Learning Rate 0.00033000004\n",
            "Epoch: 155 Training Loss= -34.22459165839223 Validation Loss= -34.197720073355484\n",
            "Learning Rate 0.00033000004\n",
            "Epoch: 156 Training Loss= -34.23454725288532 Validation Loss= -34.20916786168915\n",
            "Learning Rate 0.00033000004\n",
            "Epoch: 157 Training Loss= -34.233499450969965 Validation Loss= -34.20291012023621\n",
            "Learning Rate 0.00033000004\n",
            "Epoch: 158 Training Loss= -34.23298157680325 Validation Loss= -34.199232051262534\n",
            "Learning Rate 0.000108900014\n",
            "Epoch: 159 Training Loss= -34.235532217430226 Validation Loss= -34.1977616575728\n",
            "Learning Rate 0.000108900014\n",
            "Epoch: 160 Training Loss= -34.243515046193735 Validation Loss= -34.21295292329038\n",
            "Learning Rate 0.000108900014\n",
            "Epoch: 161 Training Loss= -34.24535070162636 Validation Loss= -34.2208809577711\n",
            "Learning Rate 0.000108900014\n",
            "Epoch: 162 Training Loss= -34.246360218869576 Validation Loss= -34.21285248635591\n",
            "Learning Rate 0.000108900014\n",
            "Epoch: 163 Training Loss= -34.24829350070537 Validation Loss= -34.21939434057555\n",
            "Learning Rate 0.000108900014\n",
            "Epoch: 164 Training Loss= -34.25222904244873 Validation Loss= -34.2190180424386\n",
            "Learning Rate 0.000108900014\n",
            "Epoch: 165 Training Loss= -34.24768177799573 Validation Loss= -34.21402857208144\n",
            "Learning Rate 0.000108900014\n",
            "Epoch: 166 Training Loss= -34.24913268769141 Validation Loss= -34.21260873661102\n",
            "Learning Rate 0.000108900014\n",
            "Epoch: 167 Training Loss= -34.2452312324095 Validation Loss= -34.20788086012997\n",
            "Learning Rate 0.000108900014\n",
            "Epoch: 168 Training Loss= -34.24808468611494 Validation Loss= -34.21051748212054\n",
            "Learning Rate 0.000108900014\n",
            "Epoch: 169 Training Loss= -34.24619311980089 Validation Loss= -34.21853298756218\n",
            "Learning Rate 0.000108900014\n",
            "Epoch: 170 Training Loss= -34.25381086644843 Validation Loss= -34.212982028081676\n",
            "Learning Rate 0.000108900014\n",
            "Epoch: 171 Training Loss= -34.24335332936912 Validation Loss= -34.21168967612454\n",
            "Learning Rate 0.000108900014\n",
            "Epoch: 172 Training Loss= -34.247908791874664 Validation Loss= -34.217263513352364\n",
            "Learning Rate 0.000108900014\n",
            "Epoch: 173 Training Loss= -34.25258643650574 Validation Loss= -34.213054554116894\n",
            "Learning Rate 0.000108900014\n",
            "Epoch: 174 Training Loss= -34.2499812168736 Validation Loss= -34.215231036733236\n",
            "Learning Rate 0.000108900014\n",
            "Epoch: 175 Training Loss= -34.252693282411634 Validation Loss= -34.20923822203815\n",
            "Learning Rate 0.000108900014\n",
            "Epoch: 176 Training Loss= -34.24945786157431 Validation Loss= -34.21907131193329\n",
            "Learning Rate 0.000108900014\n",
            "Epoch: 177 Training Loss= -34.24862412253526 Validation Loss= -34.21197760948297\n",
            "Learning Rate 0.000108900014\n",
            "Epoch: 178 Training Loss= -34.25301555377483 Validation Loss= -34.21278102798152\n",
            "Learning Rate 0.000108900014\n",
            "Epoch: 179 Training Loss= -34.251930102095514 Validation Loss= -34.217113159463786\n",
            "Learning Rate 0.000108900014\n",
            "Epoch: 180 Training Loss= -34.25006362656 Validation Loss= -34.21755537518202\n",
            "Learning Rate 0.000108900014\n",
            "Epoch: 181 Training Loss= -34.25330445123342 Validation Loss= -34.219316953952365\n",
            "Learning Rate 0.000108900014\n",
            "Epoch: 182 Training Loss= -34.24901488716031 Validation Loss= -34.2137856501027\n",
            "Learning Rate 0.000108900014\n",
            "Epoch: 183 Training Loss= -34.25362206191493 Validation Loss= -34.214193591439034\n",
            "Learning Rate 0.000108900014\n",
            "Epoch: 184 Training Loss= -34.25305075793572 Validation Loss= -34.21130418209238\n",
            "Learning Rate 0.000108900014\n",
            "Epoch: 185 Training Loss= -34.2563162034861 Validation Loss= -34.21895368841136\n",
            "Learning Rate 0.000108900014\n",
            "Epoch: 186 Training Loss= -34.25659623331378 Validation Loss= -34.21441954716516\n",
            "Learning Rate 0.000108900014\n",
            "Epoch: 187 Training Loss= -34.25313011851784 Validation Loss= -34.213597684881634\n",
            "Learning Rate 0.000108900014\n",
            "Epoch: 188 Training Loss= -34.253789899951144 Validation Loss= -34.21460841177747\n",
            "Learning Rate 0.000108900014\n",
            "Epoch: 189 Training Loss= -34.255426685187985 Validation Loss= -34.21268214098977\n",
            "Learning Rate 0.000108900014\n",
            "Epoch: 190 Training Loss= -34.25049989126166 Validation Loss= -34.21081506367473\n",
            "Learning Rate 0.000108900014\n",
            "Epoch: 191 Training Loss= -34.25574491917996 Validation Loss= -34.21101488197341\n",
            "Learning Rate 0.000108900014\n",
            "Epoch: 192 Training Loss= -34.25971506203997 Validation Loss= -34.214567694770885\n",
            "Learning Rate 0.000108900014\n",
            "Epoch: 193 Training Loss= -34.256982525415815 Validation Loss= -34.21221891851787\n",
            "Learning Rate 0.000108900014\n",
            "Epoch: 194 Training Loss= -34.2574217034849 Validation Loss= -34.21348085158182\n",
            "Learning Rate 0.000108900014\n",
            "Epoch: 195 Training Loss= -34.25829373974674 Validation Loss= -34.21403482125071\n",
            "Learning Rate 0.000108900014\n",
            "Epoch: 196 Training Loss= -34.25876947728684 Validation Loss= -34.21227089423276\n",
            "Learning Rate 0.000108900014\n",
            "Epoch: 197 Training Loss= -34.25855057071195 Validation Loss= -34.20918917444689\n",
            "Learning Rate 0.000108900014\n",
            "Epoch: 198 Training Loss= -34.260011039109145 Validation Loss= -34.219155472892744\n",
            "Learning Rate 0.000108900014\n",
            "Epoch: 199 Training Loss= -34.25715865490281 Validation Loss= -34.2137426703239\n",
            "Learning Rate 0.000108900014\n",
            "Epoch: 200 Training Loss= -34.2606960097877 Validation Loss= -34.204924856480034\n",
            "Learning Rate 0.000108900014\n",
            "Epoch: 201 Training Loss= -34.25763740041722 Validation Loss= -34.21292752397354\n",
            "Learning Rate 0.000108900014\n",
            "Epoch: 202 Training Loss= -34.25861274825325 Validation Loss= -34.21812102940289\n",
            "Learning Rate 0.000108900014\n",
            "Epoch: 203 Training Loss= -34.25959338488471 Validation Loss= -34.217132115043114\n",
            "Learning Rate 0.000108900014\n",
            "Epoch: 204 Training Loss= -34.26443807059023 Validation Loss= -34.21235965403327\n",
            "Learning Rate 0.000108900014\n",
            "Epoch: 205 Training Loss= -34.263452865374454 Validation Loss= -34.21203834829153\n",
            "Learning Rate 0.000108900014\n",
            "Epoch: 206 Training Loss= -34.263890644135046 Validation Loss= -34.214951710026675\n",
            "Learning Rate 0.000108900014\n",
            "Epoch: 207 Training Loss= -34.25829927277759 Validation Loss= -34.21361043309934\n",
            "Learning Rate 0.000108900014\n",
            "Epoch: 208 Training Loss= -34.26355234663766 Validation Loss= -34.21553887623038\n",
            "Learning Rate 0.000108900014\n",
            "Epoch: 209 Training Loss= -34.260723580317716 Validation Loss= -34.219082842482166\n",
            "Learning Rate 0.000108900014\n",
            "Epoch: 210 Training Loss= -34.26333524414363 Validation Loss= -34.214657201655136\n",
            "Learning Rate 0.000108900014\n",
            "Epoch: 211 Training Loss= -34.26380535885642 Validation Loss= -34.213077788067906\n",
            "Learning Rate 0.000108900014\n",
            "Epoch: 212 Training Loss= -34.26582443111227 Validation Loss= -34.21467687411617\n",
            "Learning Rate 0.000108900014\n",
            "Epoch: 213 Training Loss= -34.265136964587136 Validation Loss= -34.21347348102063\n",
            "Learning Rate 0.000108900014\n",
            "Epoch: 214 Training Loss= -34.26344486442407 Validation Loss= -34.209653732003545\n",
            "Learning Rate 0.000108900014\n",
            "Epoch: 215 Training Loss= -34.26647885719678 Validation Loss= -34.21073231522928\n",
            "Learning Rate 0.000108900014\n",
            "Epoch: 216 Training Loss= -34.26560359912766 Validation Loss= -34.21608442150449\n",
            "Learning Rate 0.000108900014\n",
            "Epoch: 217 Training Loss= -34.26104786744438 Validation Loss= -34.20650044490832\n",
            "Learning Rate 0.000108900014\n",
            "Epoch: 218 Training Loss= -34.263787720641375 Validation Loss= -34.206285495938516\n",
            "Learning Rate 0.000108900014\n",
            "Epoch: 219 Training Loss= -34.26678410451933 Validation Loss= -34.20960574292344\n",
            "Learning Rate 0.000108900014\n",
            "Epoch: 220 Training Loss= -34.26726363405627 Validation Loss= -34.207540897303005\n",
            "Learning Rate 0.000108900014\n",
            "Epoch: 221 Training Loss= -34.2642725460299 Validation Loss= -34.21086261678552\n",
            "Learning Rate 0.000108900014\n",
            "Epoch: 222 Training Loss= -34.26697501863635 Validation Loss= -34.21117415073829\n",
            "Learning Rate 0.000108900014\n",
            "Epoch: 223 Training Loss= -34.26567975466075 Validation Loss= -34.20822629408872\n",
            "Learning Rate 0.000108900014\n",
            "Epoch: 224 Training Loss= -34.26690113851824 Validation Loss= -34.21101896050111\n",
            "Learning Rate 0.000108900014\n",
            "Epoch: 225 Training Loss= -34.268497858142034 Validation Loss= -34.209421504087786\n",
            "Learning Rate 0.000108900014\n",
            "Epoch: 226 Training Loss= -34.264780348304996 Validation Loss= -34.21068163379229\n",
            "Learning Rate 0.000108900014\n",
            "Epoch: 227 Training Loss= -34.27092172614098 Validation Loss= -34.21389958732554\n",
            "Learning Rate 0.000108900014\n",
            "Epoch: 228 Training Loss= -34.26816835547641 Validation Loss= -34.21672874526742\n",
            "Learning Rate 0.000108900014\n",
            "Epoch: 229 Training Loss= -34.265098541632454 Validation Loss= -34.21797835064238\n",
            "Learning Rate 0.000108900014\n",
            "Epoch: 230 Training Loss= -34.26593803635137 Validation Loss= -34.214642054189625\n",
            "Learning Rate 0.000108900014\n",
            "Epoch: 231 Training Loss= -34.26689768793267 Validation Loss= -34.21085897693522\n",
            "Learning Rate 0.000108900014\n",
            "Epoch: 232 Training Loss= -34.273067952434346 Validation Loss= -34.21133342834305\n",
            "Learning Rate 0.000108900014\n",
            "Epoch: 233 Training Loss= -34.26972754221588 Validation Loss= -34.21345860276864\n",
            "Learning Rate 0.000108900014\n",
            "Epoch: 234 Training Loss= -34.2658194540744 Validation Loss= -34.20901720952125\n",
            "Learning Rate 0.000108900014\n",
            "Epoch: 235 Training Loss= -34.26938295909531 Validation Loss= -34.21105335744381\n",
            "Learning Rate 0.000108900014\n",
            "Epoch: 236 Training Loss= -34.26977416513466 Validation Loss= -34.213539677578574\n",
            "Learning Rate 0.000108900014\n",
            "Epoch: 237 Training Loss= -34.26882030033477 Validation Loss= -34.21214188024549\n",
            "Learning Rate 0.000108900014\n",
            "Epoch: 238 Training Loss= -34.26974635279828 Validation Loss= -34.21364797894487\n",
            "Learning Rate 3.593701e-05\n",
            "Epoch: 239 Training Loss= -34.27256942461723 Validation Loss= -34.208722595521415\n",
            "Learning Rate 3.593701e-05\n",
            "Epoch: 240 Training Loss= -34.275802828011415 Validation Loss= -34.21648731335677\n",
            "Learning Rate 3.593701e-05\n",
            "Epoch: 241 Training Loss= -34.275745134260156 Validation Loss= -34.216099353359795\n",
            "Learning Rate 3.593701e-05\n",
            "Epoch: 242 Training Loss= -34.27562811212399 Validation Loss= -34.216585813403064\n",
            "Learning Rate 3.593701e-05\n",
            "Epoch: 243 Training Loss= -34.276768931491915 Validation Loss= -34.218236512177505\n",
            "Learning Rate 3.593701e-05\n",
            "Epoch: 244 Training Loss= -34.27765237120856 Validation Loss= -34.21616659027566\n",
            "Learning Rate 3.593701e-05\n",
            "Epoch: 245 Training Loss= -34.27838309423221 Validation Loss= -34.21511411074034\n",
            "Learning Rate 3.593701e-05\n",
            "Epoch: 246 Training Loss= -34.27604025576522 Validation Loss= -34.21533750554297\n",
            "Learning Rate 3.593701e-05\n",
            "Epoch: 247 Training Loss= -34.27920187069511 Validation Loss= -34.21748113966429\n",
            "Learning Rate 3.593701e-05\n",
            "Epoch: 248 Training Loss= -34.27827197346823 Validation Loss= -34.21520660904166\n",
            "Learning Rate 3.593701e-05\n",
            "Epoch: 249 Training Loss= -34.28039503400116 Validation Loss= -34.21675735397085\n",
            "Learning Rate 3.593701e-05\n",
            "Epoch: 250 Training Loss= -34.27948731795496 Validation Loss= -34.216317765733436\n",
            "Learning Rate 3.593701e-05\n",
            "Epoch: 251 Training Loss= -34.281500649863986 Validation Loss= -34.21657658889443\n",
            "Learning Rate 3.593701e-05\n",
            "Epoch: 252 Training Loss= -34.278777485969876 Validation Loss= -34.21602894471658\n",
            "Learning Rate 3.593701e-05\n",
            "Epoch: 253 Training Loss= -34.27990321576283 Validation Loss= -34.21699768546334\n",
            "Learning Rate 3.593701e-05\n",
            "Epoch: 254 Training Loss= -34.280863577811274 Validation Loss= -34.21647113941432\n",
            "Learning Rate 3.593701e-05\n",
            "Epoch: 255 Training Loss= -34.28124676897004 Validation Loss= -34.21642819703375\n",
            "Learning Rate 3.593701e-05\n",
            "Epoch: 256 Training Loss= -34.282154450677844 Validation Loss= -34.21787105287677\n",
            "Learning Rate 3.593701e-05\n",
            "Epoch: 257 Training Loss= -34.28163678031488 Validation Loss= -34.21689336914129\n",
            "Learning Rate 3.593701e-05\n",
            "Epoch: 258 Training Loss= -34.28298238204291 Validation Loss= -34.217966090497704\n",
            "Learning Rate 3.593701e-05\n",
            "Epoch: 259 Training Loss= -34.28267235714844 Validation Loss= -34.21458402579252\n",
            "Learning Rate 3.593701e-05\n",
            "Epoch: 260 Training Loss= -34.28325871194276 Validation Loss= -34.21521021434907\n",
            "Learning Rate 3.593701e-05\n",
            "Epoch: 261 Training Loss= -34.28173385372099 Validation Loss= -34.21553212655581\n",
            "Learning Rate 3.593701e-05\n",
            "Epoch: 262 Training Loss= -34.28455509906258 Validation Loss= -34.21761652848724\n",
            "Learning Rate 3.593701e-05\n",
            "Epoch: 263 Training Loss= -34.283245465367926 Validation Loss= -34.215363805684625\n",
            "Learning Rate 3.593701e-05\n",
            "Epoch: 264 Training Loss= -34.28400907801611 Validation Loss= -34.21638142114476\n",
            "Learning Rate 3.593701e-05\n",
            "Epoch: 265 Training Loss= -34.28398256793323 Validation Loss= -34.21668865861098\n",
            "Learning Rate 3.593701e-05\n",
            "Epoch: 266 Training Loss= -34.28437610714138 Validation Loss= -34.21638853705827\n",
            "Learning Rate 3.593701e-05\n",
            "Epoch: 267 Training Loss= -34.28452936807893 Validation Loss= -34.21554832114202\n",
            "Learning Rate 3.593701e-05\n",
            "Epoch: 268 Training Loss= -34.28528922390589 Validation Loss= -34.212241390320656\n",
            "Learning Rate 3.593701e-05\n",
            "Epoch: 269 Training Loss= -34.28503015042559 Validation Loss= -34.21765983883783\n",
            "Learning Rate 3.593701e-05\n",
            "Epoch: 270 Training Loss= -34.28706065806261 Validation Loss= -34.21695727310427\n",
            "Learning Rate 3.593701e-05\n",
            "Epoch: 271 Training Loss= -34.28607320154527 Validation Loss= -34.21843466226247\n",
            "Learning Rate 3.593701e-05\n",
            "Epoch: 272 Training Loss= -34.28677376660743 Validation Loss= -34.21710829255072\n",
            "Learning Rate 3.593701e-05\n",
            "Epoch: 273 Training Loss= -34.285243376271275 Validation Loss= -34.2200794680618\n",
            "Learning Rate 3.593701e-05\n",
            "Epoch: 274 Training Loss= -34.28770957766104 Validation Loss= -34.21671249957249\n",
            "Learning Rate 3.593701e-05\n",
            "Epoch: 275 Training Loss= -34.28720389110508 Validation Loss= -34.219892372493504\n",
            "Learning Rate 3.593701e-05\n",
            "Epoch: 276 Training Loss= -34.28688288716769 Validation Loss= -34.21795095338774\n",
            "Learning Rate 3.593701e-05\n",
            "Epoch: 277 Training Loss= -34.28814718523765 Validation Loss= -34.21572772438864\n",
            "Learning Rate 3.593701e-05\n",
            "Epoch: 278 Training Loss= -34.288323871340424 Validation Loss= -34.21715141070455\n",
            "Learning Rate 3.593701e-05\n",
            "Epoch: 279 Training Loss= -34.28763603738894 Validation Loss= -34.21809865750702\n",
            "Learning Rate 3.593701e-05\n",
            "Epoch: 280 Training Loss= -34.28620966680434 Validation Loss= -34.21564149108454\n",
            "Learning Rate 3.593701e-05\n",
            "Epoch: 281 Training Loss= -34.288488635987406 Validation Loss= -34.2173949814812\n",
            "Learning Rate 3.593701e-05\n",
            "Epoch: 282 Training Loss= -34.28901012873061 Validation Loss= -34.21439502108391\n",
            "Learning Rate 3.593701e-05\n",
            "Epoch: 283 Training Loss= -34.28872710989725 Validation Loss= -34.217228833942414\n",
            "Learning Rate 3.593701e-05\n",
            "Epoch: 284 Training Loss= -34.28897132252678 Validation Loss= -34.2157588063893\n",
            "Learning Rate 3.593701e-05\n",
            "Epoch: 285 Training Loss= -34.28705487641415 Validation Loss= -34.21647960915315\n",
            "Learning Rate 3.593701e-05\n",
            "Epoch: 286 Training Loss= -34.28872909639754 Validation Loss= -34.216121261789084\n",
            "Learning Rate 3.593701e-05\n",
            "Epoch: 287 Training Loss= -34.288915718793604 Validation Loss= -34.21706232342334\n",
            "Learning Rate 3.593701e-05\n",
            "Epoch: 288 Training Loss= -34.287836557392794 Validation Loss= -34.215768995698895\n",
            "Learning Rate 3.593701e-05\n",
            "Epoch: 289 Training Loss= -34.28838017590433 Validation Loss= -34.216128258651636\n",
            "Learning Rate 3.593701e-05\n",
            "Epoch: 290 Training Loss= -34.289625302285266 Validation Loss= -34.21645822439655\n",
            "Learning Rate 3.593701e-05\n",
            "Epoch: 291 Training Loss= -34.28939708071903 Validation Loss= -34.216209803931356\n",
            "Learning Rate 3.593701e-05\n",
            "Epoch: 292 Training Loss= -34.2912500235909 Validation Loss= -34.21658599660447\n",
            "Learning Rate 3.593701e-05\n",
            "Epoch: 293 Training Loss= -34.29004067487852 Validation Loss= -34.21654330478788\n",
            "Learning Rate 3.593701e-05\n",
            "Epoch: 294 Training Loss= -34.29085470601739 Validation Loss= -34.21562020169234\n",
            "Learning Rate 3.593701e-05\n",
            "Epoch: 295 Training Loss= -34.2905791971164 Validation Loss= -34.21518985683366\n",
            "Learning Rate 3.593701e-05\n",
            "Epoch: 296 Training Loss= -34.290684506093605 Validation Loss= -34.21750218159247\n",
            "Learning Rate 3.593701e-05\n",
            "Epoch: 297 Training Loss= -34.29156485031973 Validation Loss= -34.21829171153683\n",
            "Learning Rate 3.593701e-05\n",
            "Epoch: 298 Training Loss= -34.29191037861176 Validation Loss= -34.21602783621247\n",
            "Learning Rate 3.593701e-05\n",
            "Epoch: 299 Training Loss= -34.292291238824745 Validation Loss= -34.21516708529179\n",
            "Learning Rate 3.593701e-05\n",
            "Epoch: 300 Training Loss= -34.28884522745769 Validation Loss= -34.21534990995686\n",
            "Learning Rate 3.593701e-05\n",
            "Epoch: 301 Training Loss= -34.29047061668835 Validation Loss= -34.21719045218026\n",
            "Learning Rate 3.593701e-05\n",
            "Epoch: 302 Training Loss= -34.29062485605465 Validation Loss= -34.21522021735565\n",
            "Learning Rate 3.593701e-05\n",
            "Epoch: 303 Training Loss= -34.29195585044974 Validation Loss= -34.21507634854535\n",
            "Learning Rate 3.593701e-05\n",
            "Epoch: 304 Training Loss= -34.29263906193987 Validation Loss= -34.217004743392366\n",
            "Learning Rate 3.593701e-05\n",
            "Epoch: 305 Training Loss= -34.292739814099306 Validation Loss= -34.215325480340276\n",
            "Learning Rate 3.593701e-05\n",
            "Epoch: 306 Training Loss= -34.29276633390674 Validation Loss= -34.21779973354358\n",
            "Learning Rate 3.593701e-05\n",
            "Epoch: 307 Training Loss= -34.293127275646164 Validation Loss= -34.21849390203771\n",
            "Learning Rate 3.593701e-05\n",
            "Epoch: 308 Training Loss= -34.2925233419576 Validation Loss= -34.21723884267387\n",
            "Learning Rate 3.593701e-05\n",
            "Epoch: 309 Training Loss= -34.29397218731641 Validation Loss= -34.21640994007868\n",
            "Learning Rate 3.593701e-05\n",
            "Epoch: 310 Training Loss= -34.2924651208394 Validation Loss= -34.21728063034594\n",
            "Learning Rate 3.593701e-05\n",
            "Epoch: 311 Training Loss= -34.29356219009175 Validation Loss= -34.2124747918492\n",
            "Learning Rate 3.593701e-05\n",
            "Epoch: 312 Training Loss= -34.293283885533015 Validation Loss= -34.216697025707646\n",
            "Learning Rate 3.593701e-05\n",
            "Epoch: 313 Training Loss= -34.29410118848155 Validation Loss= -34.21323724077591\n",
            "Learning Rate 3.593701e-05\n",
            "Epoch: 314 Training Loss= -34.294113662159646 Validation Loss= -34.216545077720795\n",
            "Learning Rate 3.593701e-05\n",
            "Epoch: 315 Training Loss= -34.294595038062305 Validation Loss= -34.21609392850533\n",
            "Learning Rate 3.593701e-05\n",
            "Epoch: 316 Training Loss= -34.29433727633818 Validation Loss= -34.21732456532505\n",
            "Learning Rate 3.593701e-05\n",
            "Epoch: 317 Training Loss= -34.29627163794409 Validation Loss= -34.21694724035015\n",
            "Learning Rate 3.593701e-05\n",
            "Epoch: 318 Training Loss= -34.29508268446177 Validation Loss= -34.21638707689588\n",
            "Learning Rate 1.1859212e-05\n",
            "Epoch: 319 Training Loss= -34.293755630347896 Validation Loss= -34.215618749285774\n",
            "Learning Rate 1.1859212e-05\n",
            "Epoch: 320 Training Loss= -34.289189698476655 Validation Loss= -34.21633133244409\n",
            "Learning Rate 1.1859212e-05\n",
            "Epoch: 321 Training Loss= -34.289501997988744 Validation Loss= -34.21653177818033\n",
            "Learning Rate 1.1859212e-05\n",
            "Epoch: 322 Training Loss= -34.28965193233321 Validation Loss= -34.21712003023763\n",
            "Learning Rate 1.1859212e-05\n",
            "Epoch: 323 Training Loss= -34.28978497191761 Validation Loss= -34.215796090292734\n",
            "Learning Rate 1.1859212e-05\n",
            "Epoch: 324 Training Loss= -34.289843599214734 Validation Loss= -34.216141552159065\n",
            "Learning Rate 1.1859212e-05\n",
            "Epoch: 325 Training Loss= -34.289492667622426 Validation Loss= -34.21648994325698\n",
            "Learning Rate 1.1859212e-05\n",
            "Epoch: 326 Training Loss= -34.29018915003218 Validation Loss= -34.21558765676484\n",
            "Learning Rate 1.1859212e-05\n",
            "Epoch: 327 Training Loss= -34.29025795750714 Validation Loss= -34.21634928731339\n",
            "Learning Rate 1.1859212e-05\n",
            "Epoch: 328 Training Loss= -34.29020355725569 Validation Loss= -34.21646009627911\n",
            "Learning Rate 1.1859212e-05\n",
            "Epoch: 329 Training Loss= -34.29057721569758 Validation Loss= -34.2159842373254\n",
            "Learning Rate 1.1859212e-05\n",
            "Epoch: 330 Training Loss= -34.290507146311896 Validation Loss= -34.21396760022696\n",
            "Learning Rate 1.1859212e-05\n",
            "Epoch: 331 Training Loss= -34.290667784451394 Validation Loss= -34.21502587880105\n",
            "Learning Rate 1.1859212e-05\n",
            "Epoch: 332 Training Loss= -34.29078556713406 Validation Loss= -34.21309921830411\n",
            "Learning Rate 1.1859212e-05\n",
            "Epoch: 333 Training Loss= -34.29099749941282 Validation Loss= -34.21291864854142\n",
            "Learning Rate 1.1859212e-05\n",
            "Epoch: 334 Training Loss= -34.290891857389376 Validation Loss= -34.21499063515115\n",
            "Learning Rate 1.1859212e-05\n",
            "Epoch: 335 Training Loss= -34.29120936527903 Validation Loss= -34.215494464089474\n",
            "Learning Rate 1.1859212e-05\n",
            "Epoch: 336 Training Loss= -34.291381184305614 Validation Loss= -34.21620101239756\n",
            "Learning Rate 1.1859212e-05\n",
            "Epoch: 337 Training Loss= -34.291512080373295 Validation Loss= -34.215732964630746\n",
            "Learning Rate 1.1859212e-05\n",
            "Epoch: 338 Training Loss= -34.29125353828474 Validation Loss= -34.21555180317693\n",
            "Learning Rate 1.1859212e-05\n",
            "Epoch: 339 Training Loss= -34.29150815903916 Validation Loss= -34.21506640136997\n",
            "Learning Rate 1.1859212e-05\n",
            "Epoch: 340 Training Loss= -34.29161875447166 Validation Loss= -34.21273133498092\n",
            "Learning Rate 1.1859212e-05\n",
            "Epoch: 341 Training Loss= -34.29181888708576 Validation Loss= -34.2159093699662\n",
            "Learning Rate 1.1859212e-05\n",
            "Epoch: 342 Training Loss= -34.291666508719985 Validation Loss= -34.215834553183754\n",
            "Learning Rate 1.1859212e-05\n",
            "Epoch: 343 Training Loss= -34.29208627555726 Validation Loss= -34.21321835180655\n",
            "Learning Rate 1.1859212e-05\n",
            "Epoch: 344 Training Loss= -34.29231139185046 Validation Loss= -34.212448776025525\n",
            "Learning Rate 1.1859212e-05\n",
            "Epoch: 345 Training Loss= -34.29213656429171 Validation Loss= -34.21170559609208\n",
            "Learning Rate 1.1859212e-05\n",
            "Epoch: 346 Training Loss= -34.292242621446164 Validation Loss= -34.21227761244696\n",
            "Learning Rate 1.1859212e-05\n",
            "Epoch: 347 Training Loss= -34.29231909070105 Validation Loss= -34.21669817541972\n",
            "Learning Rate 1.1859212e-05\n",
            "Epoch: 348 Training Loss= -34.29215905649449 Validation Loss= -34.216556241130306\n",
            "Learning Rate 1.1859212e-05\n",
            "Epoch: 349 Training Loss= -34.29260256442846 Validation Loss= -34.21614737667057\n",
            "Learning Rate 1.1859212e-05\n",
            "Epoch: 350 Training Loss= -34.29258589393562 Validation Loss= -34.21612026573169\n",
            "Learning Rate 1.1859212e-05\n",
            "Epoch: 351 Training Loss= -34.29286344912937 Validation Loss= -34.21640735723329\n",
            "Learning Rate 1.1859212e-05\n",
            "Epoch: 352 Training Loss= -34.29291683518522 Validation Loss= -34.21606853997139\n",
            "Learning Rate 1.1859212e-05\n",
            "Epoch: 353 Training Loss= -34.29304453716267 Validation Loss= -34.21664074938013\n",
            "Learning Rate 1.1859212e-05\n",
            "Epoch: 354 Training Loss= -34.293157209975355 Validation Loss= -34.21585227836991\n",
            "Learning Rate 1.1859212e-05\n",
            "Epoch: 355 Training Loss= -34.29341773551512 Validation Loss= -34.21669746485253\n",
            "Learning Rate 1.1859212e-05\n",
            "Epoch: 356 Training Loss= -34.29348569222591 Validation Loss= -34.21615589781557\n",
            "Learning Rate 1.1859212e-05\n",
            "Epoch: 357 Training Loss= -34.29351411293706 Validation Loss= -34.216523475098256\n",
            "Learning Rate 1.1859212e-05\n",
            "Epoch: 358 Training Loss= -34.29370888661322 Validation Loss= -34.2157123485104\n",
            "Learning Rate 1.1859212e-05\n",
            "Epoch: 359 Training Loss= -34.29381174240562 Validation Loss= -34.21586299704545\n",
            "Learning Rate 1.1859212e-05\n",
            "Epoch: 360 Training Loss= -34.29387906728264 Validation Loss= -34.21633896913909\n",
            "Learning Rate 1.1859212e-05\n",
            "Epoch: 361 Training Loss= -34.29388325882099 Validation Loss= -34.216194978520996\n",
            "Learning Rate 1.1859212e-05\n",
            "Epoch: 362 Training Loss= -34.29388308100726 Validation Loss= -34.215975401495506\n",
            "Learning Rate 1.1859212e-05\n",
            "Epoch: 363 Training Loss= -34.29384562862556 Validation Loss= -34.21588785987852\n",
            "Learning Rate 1.1859212e-05\n",
            "Epoch: 364 Training Loss= -34.29395854704182 Validation Loss= -34.21629674520827\n",
            "Learning Rate 1.1859212e-05\n",
            "Epoch: 365 Training Loss= -34.294049713464226 Validation Loss= -34.21552629544044\n",
            "Learning Rate 1.1859212e-05\n",
            "Epoch: 366 Training Loss= -34.294081754804736 Validation Loss= -34.21556142384605\n",
            "Learning Rate 1.1859212e-05\n",
            "Epoch: 367 Training Loss= -34.29427949690134 Validation Loss= -34.21630617294443\n",
            "Learning Rate 1.1859212e-05\n",
            "Epoch: 368 Training Loss= -34.294221786878424 Validation Loss= -34.21602407574666\n",
            "Learning Rate 1.1859212e-05\n",
            "Epoch: 369 Training Loss= -34.29425133164991 Validation Loss= -34.215991268438735\n",
            "Learning Rate 1.1859212e-05\n",
            "Epoch: 370 Training Loss= -34.29432540190823 Validation Loss= -34.215830383487955\n",
            "Learning Rate 1.1859212e-05\n",
            "Epoch: 371 Training Loss= -34.294637062642266 Validation Loss= -34.215908683253275\n",
            "Learning Rate 1.1859212e-05\n",
            "Epoch: 372 Training Loss= -34.29465328860873 Validation Loss= -34.21607402529456\n",
            "Learning Rate 1.1859212e-05\n",
            "Epoch: 373 Training Loss= -34.29476985442734 Validation Loss= -34.21619417341106\n",
            "Learning Rate 1.1859212e-05\n",
            "Epoch: 374 Training Loss= -34.29474152514495 Validation Loss= -34.2161444648735\n",
            "Learning Rate 1.1859212e-05\n",
            "Epoch: 375 Training Loss= -34.2946470372727 Validation Loss= -34.216143823615404\n",
            "Learning Rate 1.1859212e-05\n",
            "Epoch: 376 Training Loss= -34.29499910415515 Validation Loss= -34.21582854387336\n",
            "Learning Rate 1.1859212e-05\n",
            "Epoch: 377 Training Loss= -34.29508003836454 Validation Loss= -34.215800071876565\n",
            "Learning Rate 1.1859212e-05\n",
            "Epoch: 378 Training Loss= -34.29503343028426 Validation Loss= -34.21582206614808\n",
            "Learning Rate 1.1859212e-05\n",
            "Epoch: 379 Training Loss= -34.2952229623083 Validation Loss= -34.215804035496625\n",
            "Learning Rate 1.1859212e-05\n",
            "Epoch: 380 Training Loss= -34.29505429600481 Validation Loss= -34.21623099864518\n",
            "Learning Rate 1.1859212e-05\n",
            "Epoch: 381 Training Loss= -34.29520681805174 Validation Loss= -34.215817264612014\n",
            "Learning Rate 1.1859212e-05\n",
            "Epoch: 382 Training Loss= -34.29532324555707 Validation Loss= -34.215777023982405\n",
            "Learning Rate 1.1859212e-05\n",
            "Epoch: 383 Training Loss= -34.29543485302779 Validation Loss= -34.21573517437655\n",
            "Learning Rate 1.1859212e-05\n",
            "Epoch: 384 Training Loss= -34.29545584584084 Validation Loss= -34.21532241519508\n",
            "Learning Rate 1.1859212e-05\n",
            "Epoch: 385 Training Loss= -34.29558335814942 Validation Loss= -34.21562517872574\n",
            "Learning Rate 1.1859212e-05\n",
            "Epoch: 386 Training Loss= -34.29555522941378 Validation Loss= -34.21530408883788\n",
            "Learning Rate 1.1859212e-05\n",
            "Epoch: 387 Training Loss= -34.29570221698146 Validation Loss= -34.21564212949758\n",
            "Learning Rate 1.1859212e-05\n",
            "Epoch: 388 Training Loss= -34.29578881228722 Validation Loss= -34.215086457553845\n",
            "Learning Rate 1.1859212e-05\n",
            "Epoch: 389 Training Loss= -34.29582731037855 Validation Loss= -34.215426887430134\n",
            "Learning Rate 1.1859212e-05\n",
            "Epoch: 390 Training Loss= -34.2958538957756 Validation Loss= -34.21531566406059\n",
            "Learning Rate 1.1859212e-05\n",
            "Epoch: 391 Training Loss= -34.295848167245914 Validation Loss= -34.214956908973676\n",
            "Learning Rate 1.1859212e-05\n",
            "Epoch: 392 Training Loss= -34.296018193507855 Validation Loss= -34.21522322414471\n",
            "Learning Rate 1.1859212e-05\n",
            "Epoch: 393 Training Loss= -34.29598180692656 Validation Loss= -34.21505498218372\n",
            "Learning Rate 1.1859212e-05\n",
            "Epoch: 394 Training Loss= -34.29599751563142 Validation Loss= -34.21507176062764\n",
            "Learning Rate 1.1859212e-05\n",
            "Epoch: 395 Training Loss= -34.29617451040865 Validation Loss= -34.2149908407567\n",
            "Learning Rate 1.1859212e-05\n",
            "Epoch: 396 Training Loss= -34.29619142512051 Validation Loss= -34.21532483490753\n",
            "Learning Rate 1.1859212e-05\n",
            "Epoch: 397 Training Loss= -34.2961997744557 Validation Loss= -34.21524571877168\n",
            "Learning Rate 1.1859212e-05\n",
            "Epoch: 398 Training Loss= -34.29645771531958 Validation Loss= -34.21537095096398\n",
            "Learning Rate 3.9135407e-06\n",
            "Epoch: 399 Training Loss= -34.296380202027905 Validation Loss= -34.215180675822495\n",
            "Learning Rate 3.9135407e-06\n",
            "Epoch: 400 Training Loss= -34.29222261854879 Validation Loss= -34.2155144365373\n",
            "Learning Rate 3.9135407e-06\n",
            "Epoch: 401 Training Loss= -34.292274193328744 Validation Loss= -34.215634358871505\n",
            "Learning Rate 3.9135407e-06\n",
            "Epoch: 402 Training Loss= -34.29229062711822 Validation Loss= -34.215795813498126\n",
            "Learning Rate 3.9135407e-06\n",
            "Epoch: 403 Training Loss= -34.2923759149073 Validation Loss= -34.21592232889222\n",
            "Learning Rate 3.9135407e-06\n",
            "Epoch: 404 Training Loss= -34.29238395140295 Validation Loss= -34.21591218440993\n",
            "Learning Rate 3.9135407e-06\n",
            "Epoch: 405 Training Loss= -34.29230301627749 Validation Loss= -34.21587201920886\n",
            "Learning Rate 3.9135407e-06\n",
            "Epoch: 406 Training Loss= -34.292461566780894 Validation Loss= -34.21594103819408\n",
            "Learning Rate 3.9135407e-06\n",
            "Epoch: 407 Training Loss= -34.29240429025759 Validation Loss= -34.21591062880709\n",
            "Learning Rate 3.9135407e-06\n",
            "Epoch: 408 Training Loss= -34.29243283675396 Validation Loss= -34.215921765610965\n",
            "Learning Rate 3.9135407e-06\n",
            "Epoch: 409 Training Loss= -34.292501012071725 Validation Loss= -34.21590542201163\n",
            "Learning Rate 3.9135407e-06\n",
            "Epoch: 410 Training Loss= -34.29257557255728 Validation Loss= -34.21588993038278\n",
            "Learning Rate 3.9135407e-06\n",
            "Epoch: 411 Training Loss= -34.292559010601664 Validation Loss= -34.21585684740965\n",
            "Learning Rate 3.9135407e-06\n",
            "Epoch: 412 Training Loss= -34.292605240834476 Validation Loss= -34.21590032449468\n",
            "Learning Rate 3.9135407e-06\n",
            "Epoch: 413 Training Loss= -34.2926849630014 Validation Loss= -34.21590057069233\n",
            "Learning Rate 3.9135407e-06\n",
            "Epoch: 414 Training Loss= -34.29272330504948 Validation Loss= -34.215908058315975\n",
            "Learning Rate 3.9135407e-06\n",
            "Epoch: 415 Training Loss= -34.29274582429337 Validation Loss= -34.215951058667635\n",
            "Learning Rate 3.9135407e-06\n",
            "Epoch: 416 Training Loss= -34.292839000409316 Validation Loss= -34.21591846071258\n",
            "Learning Rate 3.9135407e-06\n",
            "Epoch: 417 Training Loss= -34.292711207407265 Validation Loss= -34.215846464786736\n",
            "Learning Rate 3.9135407e-06\n",
            "Epoch: 418 Training Loss= -34.292897921001526 Validation Loss= -34.21585932599661\n",
            "Learning Rate 3.9135407e-06\n",
            "Epoch: 419 Training Loss= -34.29288695481094 Validation Loss= -34.21583811486708\n",
            "Learning Rate 3.9135407e-06\n",
            "Epoch: 420 Training Loss= -34.29294524793388 Validation Loss= -34.21583986271402\n",
            "Learning Rate 3.9135407e-06\n",
            "Epoch: 421 Training Loss= -34.292997224663594 Validation Loss= -34.21579928603983\n",
            "Learning Rate 3.9135407e-06\n",
            "Epoch: 422 Training Loss= -34.29304517593793 Validation Loss= -34.21576387763272\n",
            "Learning Rate 3.9135407e-06\n",
            "Epoch: 423 Training Loss= -34.293102986967725 Validation Loss= -34.215781863524555\n",
            "Learning Rate 3.9135407e-06\n",
            "Epoch: 424 Training Loss= -34.29315915314732 Validation Loss= -34.21575324291422\n",
            "Learning Rate 3.9135407e-06\n",
            "Epoch: 425 Training Loss= -34.293207144111946 Validation Loss= -34.215728866608536\n",
            "Learning Rate 3.9135407e-06\n",
            "Epoch: 426 Training Loss= -34.29325314565687 Validation Loss= -34.21578059252195\n",
            "Learning Rate 3.9135407e-06\n",
            "Epoch: 427 Training Loss= -34.293266013708 Validation Loss= -34.215800228270396\n",
            "Learning Rate 3.9135407e-06\n",
            "Epoch: 428 Training Loss= -34.29330592704545 Validation Loss= -34.21582465432539\n",
            "Learning Rate 3.9135407e-06\n",
            "Epoch: 429 Training Loss= -34.29335467643415 Validation Loss= -34.215833216504684\n",
            "Learning Rate 3.9135407e-06\n",
            "Epoch: 430 Training Loss= -34.293367895979195 Validation Loss= -34.215853405508746\n",
            "Learning Rate 3.9135407e-06\n",
            "Epoch: 431 Training Loss= -34.29343846930586 Validation Loss= -34.21590068113521\n",
            "Learning Rate 3.9135407e-06\n",
            "Epoch: 432 Training Loss= -34.29344311192415 Validation Loss= -34.21584041736981\n",
            "Learning Rate 3.9135407e-06\n",
            "Epoch: 433 Training Loss= -34.29351244767159 Validation Loss= -34.21586550295182\n",
            "Learning Rate 3.9135407e-06\n",
            "Epoch: 434 Training Loss= -34.29351137104361 Validation Loss= -34.21593998093583\n",
            "Learning Rate 3.9135407e-06\n",
            "Epoch: 435 Training Loss= -34.29354677140122 Validation Loss= -34.215885318473944\n",
            "Learning Rate 3.9135407e-06\n",
            "Epoch: 436 Training Loss= -34.293568113075146 Validation Loss= -34.21586432435404\n",
            "Learning Rate 3.9135407e-06\n",
            "Epoch: 437 Training Loss= -34.29364501982003 Validation Loss= -34.21591609861265\n",
            "Learning Rate 3.9135407e-06\n",
            "Epoch: 438 Training Loss= -34.29372645691946 Validation Loss= -34.215874214092594\n",
            "Learning Rate 3.9135407e-06\n",
            "Epoch: 439 Training Loss= -34.29374568656085 Validation Loss= -34.215913992225275\n",
            "Learning Rate 3.9135407e-06\n",
            "Epoch: 440 Training Loss= -34.29380651633528 Validation Loss= -34.215922428127314\n",
            "Learning Rate 3.9135407e-06\n",
            "Epoch: 441 Training Loss= -34.29388811692606 Validation Loss= -34.21592811756402\n",
            "Learning Rate 3.9135407e-06\n",
            "Epoch: 442 Training Loss= -34.29388642190699 Validation Loss= -34.21595224368669\n",
            "Learning Rate 3.9135407e-06\n",
            "Epoch: 443 Training Loss= -34.29391734332594 Validation Loss= -34.215901949385945\n",
            "Learning Rate 3.9135407e-06\n",
            "Epoch: 444 Training Loss= -34.293961976659794 Validation Loss= -34.21597727845484\n",
            "Learning Rate 3.9135407e-06\n",
            "Epoch: 445 Training Loss= -34.29397093915511 Validation Loss= -34.215863699697444\n",
            "Learning Rate 3.9135407e-06\n",
            "Epoch: 446 Training Loss= -34.29400892328862 Validation Loss= -34.21594151536807\n",
            "Learning Rate 3.9135407e-06\n",
            "Epoch: 447 Training Loss= -34.29412817384486 Validation Loss= -34.21598913289661\n",
            "Learning Rate 3.9135407e-06\n",
            "Epoch: 448 Training Loss= -34.29414253708067 Validation Loss= -34.2159691608492\n",
            "Learning Rate 3.9135407e-06\n",
            "Epoch: 449 Training Loss= -34.29412961723982 Validation Loss= -34.21600248846819\n",
            "Learning Rate 3.9135407e-06\n",
            "Epoch: 450 Training Loss= -34.29423855881844 Validation Loss= -34.21595810266225\n",
            "Learning Rate 3.9135407e-06\n",
            "Epoch: 451 Training Loss= -34.294238572784266 Validation Loss= -34.21593925956174\n",
            "Learning Rate 3.9135407e-06\n",
            "Epoch: 452 Training Loss= -34.29431109435433 Validation Loss= -34.216009308780485\n",
            "Learning Rate 3.9135407e-06\n",
            "Epoch: 453 Training Loss= -34.29429120390624 Validation Loss= -34.215900870306214\n",
            "Learning Rate 3.9135407e-06\n",
            "Epoch: 454 Training Loss= -34.294374834792265 Validation Loss= -34.21600920048196\n",
            "Learning Rate 3.9135407e-06\n",
            "Epoch: 455 Training Loss= -34.29444143331577 Validation Loss= -34.21597946174055\n",
            "Learning Rate 3.9135407e-06\n",
            "Epoch: 456 Training Loss= -34.294483438123784 Validation Loss= -34.21600049956052\n",
            "Learning Rate 3.9135407e-06\n",
            "Epoch: 457 Training Loss= -34.294489086228985 Validation Loss= -34.21594063923336\n",
            "Learning Rate 3.9135407e-06\n",
            "Epoch: 458 Training Loss= -34.29456501348023 Validation Loss= -34.21604743623742\n",
            "Learning Rate 3.9135407e-06\n",
            "Epoch: 459 Training Loss= -34.29456124239337 Validation Loss= -34.21602618707163\n",
            "Learning Rate 3.9135407e-06\n",
            "Epoch: 460 Training Loss= -34.29458087856575 Validation Loss= -34.21604591646669\n",
            "Learning Rate 3.9135407e-06\n",
            "Epoch: 461 Training Loss= -34.29464644986193 Validation Loss= -34.21603262294646\n",
            "Learning Rate 3.9135407e-06\n",
            "Epoch: 462 Training Loss= -34.29470419492263 Validation Loss= -34.216021142226026\n",
            "Learning Rate 3.9135407e-06\n",
            "Epoch: 463 Training Loss= -34.294713194467455 Validation Loss= -34.216050476080724\n",
            "Learning Rate 3.9135407e-06\n",
            "Epoch: 464 Training Loss= -34.294747964180786 Validation Loss= -34.21605309291581\n",
            "Learning Rate 3.9135407e-06\n",
            "Epoch: 465 Training Loss= -34.29483871372404 Validation Loss= -34.21619451941702\n",
            "Learning Rate 3.9135407e-06\n",
            "Epoch: 466 Training Loss= -34.29482335108776 Validation Loss= -34.21615304302736\n",
            "Learning Rate 3.9135407e-06\n",
            "Epoch: 467 Training Loss= -34.29485971632319 Validation Loss= -34.21609905386872\n",
            "Learning Rate 3.9135407e-06\n",
            "Epoch: 468 Training Loss= -34.29486117472833 Validation Loss= -34.21605758359275\n",
            "Learning Rate 3.9135407e-06\n",
            "Epoch: 469 Training Loss= -34.2949215475732 Validation Loss= -34.2161593338959\n",
            "Learning Rate 3.9135407e-06\n",
            "Epoch: 470 Training Loss= -34.294961484982835 Validation Loss= -34.21607227697277\n",
            "Learning Rate 3.9135407e-06\n",
            "Epoch: 471 Training Loss= -34.295026980530714 Validation Loss= -34.216169348086886\n",
            "Learning Rate 3.9135407e-06\n",
            "Epoch: 472 Training Loss= -34.295087780967044 Validation Loss= -34.21607939639538\n",
            "Learning Rate 3.9135407e-06\n",
            "Epoch: 473 Training Loss= -34.29506887079056 Validation Loss= -34.216171031209164\n",
            "Learning Rate 3.9135407e-06\n",
            "Epoch: 474 Training Loss= -34.29513547727045 Validation Loss= -34.21618810809899\n",
            "Learning Rate 3.9135407e-06\n",
            "Epoch: 475 Training Loss= -34.29517274606609 Validation Loss= -34.21617316432216\n",
            "Learning Rate 3.9135407e-06\n",
            "Epoch: 476 Training Loss= -34.29518644432549 Validation Loss= -34.216218118086225\n",
            "Learning Rate 3.9135407e-06\n",
            "Epoch: 477 Training Loss= -34.2952738242771 Validation Loss= -34.21627609067832\n",
            "Learning Rate 3.9135407e-06\n",
            "Epoch: 478 Training Loss= -34.295290253708814 Validation Loss= -34.21619538658693\n",
            "Learning Rate 1.2914684e-06\n",
            "Epoch: 479 Training Loss= -34.295363818543294 Validation Loss= -34.21622251342184\n",
            "Learning Rate 1.2914684e-06\n",
            "Epoch: 480 Training Loss= -34.29445863586561 Validation Loss= -34.21713888431712\n",
            "Learning Rate 1.2914684e-06\n",
            "Epoch: 481 Training Loss= -34.294398245288306 Validation Loss= -34.217172551357436\n",
            "Learning Rate 1.2914684e-06\n",
            "Epoch: 482 Training Loss= -34.29440469578551 Validation Loss= -34.21723589643187\n",
            "Learning Rate 1.2914684e-06\n",
            "Epoch: 483 Training Loss= -34.294431290294355 Validation Loss= -34.21718534344278\n",
            "Learning Rate 1.2914684e-06\n",
            "Epoch: 484 Training Loss= -34.29449196429007 Validation Loss= -34.217147419046476\n",
            "Learning Rate 1.2914684e-06\n",
            "Epoch: 485 Training Loss= -34.29455893549812 Validation Loss= -34.217101832889504\n",
            "Learning Rate 1.2914684e-06\n",
            "Epoch: 486 Training Loss= -34.29461698236039 Validation Loss= -34.217124307430154\n",
            "Learning Rate 1.2914684e-06\n",
            "Epoch: 487 Training Loss= -34.294652640544015 Validation Loss= -34.217128749829854\n",
            "Learning Rate 1.2914684e-06\n",
            "Epoch: 488 Training Loss= -34.29470563104744 Validation Loss= -34.217120728739154\n",
            "Learning Rate 1.2914684e-06\n",
            "Epoch: 489 Training Loss= -34.29476498758766 Validation Loss= -34.21712602879652\n",
            "Learning Rate 1.2914684e-06\n",
            "Epoch: 490 Training Loss= -34.2947964097743 Validation Loss= -34.21710042569558\n",
            "Learning Rate 1.2914684e-06\n",
            "Epoch: 491 Training Loss= -34.29481632237998 Validation Loss= -34.21712369408681\n",
            "Learning Rate 1.2914684e-06\n",
            "Epoch: 492 Training Loss= -34.294863114260764 Validation Loss= -34.21712143646414\n",
            "Learning Rate 1.2914684e-06\n",
            "Epoch: 493 Training Loss= -34.294871276784946 Validation Loss= -34.21712892577448\n",
            "Learning Rate 1.2914684e-06\n",
            "Epoch: 494 Training Loss= -34.29490443766115 Validation Loss= -34.21710407277585\n",
            "Learning Rate 1.2914684e-06\n",
            "Epoch: 495 Training Loss= -34.294908179872166 Validation Loss= -34.217120152838206\n",
            "Learning Rate 1.2914684e-06\n",
            "Epoch: 496 Training Loss= -34.294944965066804 Validation Loss= -34.21711392061924\n",
            "Learning Rate 1.2914684e-06\n",
            "Epoch: 497 Training Loss= -34.29494707957226 Validation Loss= -34.21710038891015\n",
            "Learning Rate 1.2914684e-06\n",
            "Epoch: 498 Training Loss= -34.29497914411391 Validation Loss= -34.217150623109085\n",
            "Learning Rate 1.2914684e-06\n",
            "Epoch: 499 Training Loss= -34.295002440819616 Validation Loss= -34.21709494303785\n",
            "The duration for training is: 4090.391240119934\n",
            "The cost for test data is: -34.21709494303785\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimization Finished!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycukftHnr4pZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "e12d1266-1e5a-4176-9179-feba1e964342"
      },
      "source": [
        "a = -np.array(cost_train_plt)\n",
        "b = -np.array(cost_test_plt)\n",
        "plt.subplot(223)\n",
        "plt.plot(a ,label= 'Training Average Rate')\n",
        "plt.plot(b, label= 'Testing Average Rate')\n",
        "plt.xlabel('Epocs')\n",
        "plt.ylabel('Avg Sum Rate')\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad = 0.)\n",
        "plt.show()\n",
        "# graph_folder = '/content/drive/MyDrive/DL_prejects/Graph_data'\n",
        "# savemat(graph_folder+'/M8_tau_learned_train_rate.mat',{'Train_rate': a})\n",
        "# savemat(graph_folder+'/M8_tau_learned_Valid_rate.mat',{'Valid_rate': b})"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDqiGdrCoWEB"
      },
      "source": [
        "## Save Trained Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFpf9xkszdYT"
      },
      "source": [
        "trained_param_folder = '/content/drive/MyDrive/Trainedwieghts'\n",
        "Save_train_param = True \n",
        "if Save_train_param== True:\n",
        "  savemat(trained_param_folder+'/W1.mat',{'W1': sess.run(W1)})\n",
        "  savemat(trained_param_folder+'/W2.mat',{'W2': sess.run(W2)})\n",
        "  savemat(trained_param_folder+'/W3.mat',{'W3': sess.run(W3)})\n",
        "  savemat(trained_param_folder+'/W4.mat',{'W4': sess.run(W4)})\n",
        "  savemat(trained_param_folder+'/W5.mat',{'W5': sess.run(W5)})\n",
        "  savemat(trained_param_folder+'/W6.mat',{'W6': sess.run(W6)})\n",
        "\n",
        "  savemat(trained_param_folder+'/b1.mat',{'b1': sess.run(b1)})\n",
        "  savemat(trained_param_folder+'/b2.mat',{'b2': sess.run(b2)})\n",
        "  savemat(trained_param_folder+'/b3.mat',{'b3': sess.run(b3)})\n",
        "  savemat(trained_param_folder+'/b4.mat',{'b4': sess.run(b4)})\n",
        "  savemat(trained_param_folder+'/b5.mat',{'b5': sess.run(b5)})\n",
        "  savemat(trained_param_folder+'/b6.mat',{'b6': sess.run(b6)})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "j997S9ZwwY1F",
        "outputId": "5bdffc45-81e0-48d8-a63f-5d38c6535290"
      },
      "source": [
        "str(M)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'4'"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHpnsoRSz_ME"
      },
      "source": [
        "## Load Trained Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usUG4lgsnNeS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6ac3189-a257-420f-86ec-35fc37049383"
      },
      "source": [
        "M = 2\n",
        "N=32\n",
        "mini_batch_size = 10**3\n",
        "np.random.seed(112)\n",
        "\n",
        "h_bs_test = (np.random.randn( M,mini_batch_size)+1j*np.random.randn( M,mini_batch_size))*np.sqrt(b_bs/2)\n",
        "h_is_test = (np.random.randn( 1,mini_batch_size)+1j*np.random.randn( 1,mini_batch_size))*np.sqrt(b_is/2)\n",
        "h_id_test = (np.random.randn( 1,mini_batch_size)+1j*np.random.randn( 1,mini_batch_size))*np.sqrt(b_id/2)\n",
        "h_sd_test = (np.random.randn( 1,mini_batch_size)+1j*np.random.randn( 1,mini_batch_size))*np.sqrt(b_sd/2)\n",
        "g_rs_test = (np.random.randn( N,mini_batch_size)+1j*np.random.randn( N,mini_batch_size))*np.sqrt(b_rs/2)\n",
        "g_sr_test = (np.random.randn( N,mini_batch_size)+1j*np.random.randn( N,mini_batch_size))*np.sqrt(b_rd/2)\n",
        "g_rd_test = (np.random.randn( N,mini_batch_size)+1j*np.random.randn( N,mini_batch_size))*np.sqrt(b_rd/2)\n",
        "g_ir_test = (np.random.randn( N,mini_batch_size)+1j*np.random.randn( N,mini_batch_size))*np.sqrt(b_ir/2)\n",
        "G_br_test = (np.random.randn( N,M*mini_batch_size)+1j*np.random.randn( N,M*mini_batch_size))*np.sqrt(b_br/2)\n",
        "\n",
        "tic = time.time()\n",
        "g_rs1 =  np.repeat(g_rs_test,M,axis = 1)\n",
        "F = np.reshape(G_br_test*g_rs1, [N*M,mini_batch_size])\n",
        "F = np.concatenate([np.real(F),np.imag(F)],axis=0)\n",
        "F1 = np.concatenate([np.real(h_bs_test),np.imag(h_bs_test)],axis=0)\n",
        "F2 = g_rd_test*g_rs_test\n",
        "F2 = np.concatenate([np.real(F2),np.imag(F2)],axis=0)\n",
        "F3 = np.concatenate([np.real(h_sd_test),np.imag(h_sd_test)],axis=0)\n",
        "F = np.concatenate([F,F1,F2,F3],axis=0)\n",
        "\n",
        "Cost = sess.run(cost, feed_dict={X:F})\n",
        "toc = time.time() \n",
        "print(toc-tic)\n",
        "print(Cost)\n",
        "Cost = sess.run(tau, feed_dict={X:F})\n",
        "np.mean(Cost)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.011296749114990234\n",
            "-33.82863816307288\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3087082418860108"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lKZtg7Yz5M3z",
        "outputId": "626a9173-d090-474f-dc62-86f20183b088"
      },
      "source": [
        "mini_batch_size = 10**0\n",
        "\n",
        "t = np.zeros((1000,1))\n",
        "for i in range(1000):\n",
        "    mini_batch_size = 10**0\n",
        "    h_bs_test = (np.random.randn( M,mini_batch_size)+1j*np.random.randn( M,mini_batch_size))*np.sqrt(b_bs/2)\n",
        "    h_is_test = (np.random.randn( 1,mini_batch_size)+1j*np.random.randn( 1,mini_batch_size))*np.sqrt(b_is/2)\n",
        "    h_id_test = (np.random.randn( 1,mini_batch_size)+1j*np.random.randn( 1,mini_batch_size))*np.sqrt(b_id/2)\n",
        "    h_sd_test = (np.random.randn( 1,mini_batch_size)+1j*np.random.randn( 1,mini_batch_size))*np.sqrt(b_sd/2)\n",
        "    g_rs_test = (np.random.randn( N,mini_batch_size)+1j*np.random.randn( N,mini_batch_size))*np.sqrt(b_rs/2)\n",
        "    g_sr_test = (np.random.randn( N,mini_batch_size)+1j*np.random.randn( N,mini_batch_size))*np.sqrt(b_rd/2)\n",
        "    g_rd_test = (np.random.randn( N,mini_batch_size)+1j*np.random.randn( N,mini_batch_size))*np.sqrt(b_rd/2)\n",
        "    g_ir_test = (np.random.randn( N,mini_batch_size)+1j*np.random.randn( N,mini_batch_size))*np.sqrt(b_ir/2)\n",
        "    G_br_test = (np.random.randn( N,M*mini_batch_size)+1j*np.random.randn( N,M*mini_batch_size))*np.sqrt(b_br/2)\n",
        "    \n",
        "    tic = time.time()\n",
        "    g_rs1 =  np.repeat(g_rs_test,M,axis = 1)\n",
        "    F = np.reshape(G_br_test*g_rs1, [N*M,mini_batch_size])\n",
        "    F = np.concatenate([np.real(F),np.imag(F)],axis=0)\n",
        "    F1 = np.concatenate([np.real(h_bs_test),np.imag(h_bs_test)],axis=0)\n",
        "    F2 = g_rd_test*g_rs_test\n",
        "    F2 = np.concatenate([np.real(F2),np.imag(F2)],axis=0)\n",
        "    F3 = np.concatenate([np.real(h_sd_test),np.imag(h_sd_test)],axis=0)\n",
        "    F = np.concatenate([F,F1,F2,F3],axis=0)\n",
        "\n",
        "    Cost = sess.run(cost, feed_dict={X:F})\n",
        "    toc = time.time() \n",
        "    t[i] = toc-tic\n",
        "print(np.mean(t))\n",
        "print(Cost)\n",
        "Cost = sess.run(tau, feed_dict={X:F})\n",
        "np.mean(Cost)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0020459666252136233\n",
            "-0.757640044401787\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5700643924910194"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    }
  ]
}