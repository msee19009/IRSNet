{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "RR_1_Leky_relu_BN_IRSNet_tau_learn_M4_N16_BN1_varified_cost.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5-5BsZsakdS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c762af4-1867-4e26-ccdf-6c95bde2c890"
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow 1.x selected.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKX-tsjsHifj"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EH-7KJkoB2JL"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from scipy.io import savemat\n",
        "from scipy.io import loadmat\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KS8UdvJvHzqD",
        "outputId": "0bf02ac7-da58-45dd-bd10-77517322e099"
      },
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOM5bs4qApNS"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import time\n",
        "tf.set_random_seed(1)\n",
        "np.random.seed(1)\n",
        "num_epochs=200\n",
        "display_step=1\n",
        "var=1\n",
        "mean=0\n",
        "beta=0.0 # This is the L2-Regularization parameter\n",
        "mini_batch_size=3*10**3  # Number of examples for training\n",
        "num_batches = 3*10**2\n",
        "m_test=10**4   # Number of examples for testing\n",
        "num_test_batch = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVfo3zX_YQGn"
      },
      "source": [
        "eta = 1\n",
        "P_b = 10 #Watt or 40 dBm\n",
        "P_i = 0 #Watt or 40 dBm\n",
        "sigma_n = np.sqrt(3.98*10**-14)\n",
        "M = 4    #Power Beam\n",
        "N = 16   #of IRS elements"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cer-9tQmCYnp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDTiIEfVA1Fb",
        "outputId": "43199d3c-1c84-4403-ec8a-62307001b39b"
      },
      "source": [
        "alpha = 2.57\n",
        "alpha_irs = 2.2;\n",
        "Lc = 10**-3;\n",
        "\n",
        "d_br = 20;\n",
        "d_rs = 15;\n",
        "d_sb = 25;\n",
        "d_is = 15;\n",
        "d_id = 30\n",
        "d_rd = 15;\n",
        "d_sd = 25;\n",
        "d_ir = 15;\n",
        "\n",
        "b_bs = Lc*d_sb**(-alpha);\n",
        "b_is = Lc*d_is**(-alpha);\n",
        "b_id = Lc*d_id**(-alpha);\n",
        "b_sd = Lc*d_sd**(-alpha_irs);\n",
        "b_rs = Lc*d_rs**(-alpha_irs);\n",
        "b_rd = Lc*d_rd**(-alpha_irs);\n",
        "b_ir = Lc*d_ir**(-alpha_irs);\n",
        "b_br = Lc*d_br**(-alpha_irs);\n",
        "\n",
        "np.random.seed(11)\n",
        "h_bs = (np.random.randn(num_batches, M,mini_batch_size)+1j*np.random.randn(num_batches, M,mini_batch_size))*np.sqrt(b_bs/2)\n",
        "h_is = (np.random.randn(num_batches, 1,mini_batch_size)+1j*np.random.randn(num_batches, 1,mini_batch_size))*np.sqrt(b_is/2)\n",
        "h_id = (np.random.randn(num_batches, 1,mini_batch_size)+1j*np.random.randn(num_batches, 1,mini_batch_size))*np.sqrt(b_id/2)\n",
        "h_sd = (np.random.randn(num_batches, 1,mini_batch_size)+1j*np.random.randn(num_batches, 1,mini_batch_size))*np.sqrt(b_sd/2)\n",
        "g_rs = (np.random.randn(num_batches, N,mini_batch_size)+1j*np.random.randn(num_batches, N,mini_batch_size))*np.sqrt(b_rs/2)\n",
        "#g_sr = np.random.randn(num_batches, N,mini_batch_size)+1j*np.random.randn(num_batches, N,mini_batch_size)\n",
        "g_rd = (np.random.randn(num_batches, N,mini_batch_size)+1j*np.random.randn(num_batches, N,mini_batch_size))*np.sqrt(b_rd/2)\n",
        "g_ir = (np.random.randn(num_batches, N,mini_batch_size)+1j*np.random.randn(num_batches, N,mini_batch_size))*np.sqrt(b_ir/2)\n",
        "G_br = (np.random.randn(num_batches, N,M*mini_batch_size)+1j*np.random.randn(num_batches, N,M*mini_batch_size))*np.sqrt(b_br/2)\n",
        " \n",
        "print(\"Shape of h_bs  is: \"+str(h_bs.shape))\n",
        "print(\"Shape of h_is  is: \"+str(h_is.shape))\n",
        "print(\"Shape of h_id  is: \"+str(h_id.shape))\n",
        "print(\"Shape of h_sd  is: \"+str(h_sd.shape))\n",
        "print(\"Shape of g_rs  is: \"+str(g_rs.shape))\n",
        "print(\"Shape of g_rd  is: \"+str(g_rd.shape))\n",
        "print(\"Shape of g_ir  is: \"+str(g_ir.shape))\n",
        "print(\"Shape of G_br  is: \"+str(G_br.shape))\n",
        " \n",
        "np.random.seed(144)\n",
        "h_bs_t = (np.random.randn( M,mini_batch_size)+1j*np.random.randn( M,mini_batch_size))*np.sqrt(b_bs/2)\n",
        "h_is_t = (np.random.randn( 1,mini_batch_size)+1j*np.random.randn( 1,mini_batch_size))*np.sqrt(b_is/2)\n",
        "h_id_t = (np.random.randn( 1,mini_batch_size)+1j*np.random.randn( 1,mini_batch_size))*np.sqrt(b_id/2)\n",
        "h_sd_t = (np.random.randn( 1,mini_batch_size)+1j*np.random.randn( 1,mini_batch_size))*np.sqrt(b_sd/2)\n",
        "g_rs_t = (np.random.randn( N,mini_batch_size)+1j*np.random.randn( N,mini_batch_size))*np.sqrt(b_rs/2)\n",
        "g_rd_t = (np.random.randn( N,mini_batch_size)+1j*np.random.randn( N,mini_batch_size))*np.sqrt(b_rd/2)\n",
        "g_ir_t = (np.random.randn( N,mini_batch_size)+1j*np.random.randn( N,mini_batch_size))*np.sqrt(b_ir/2)\n",
        "G_br_t = (np.random.randn( N,M*mini_batch_size)+1j*np.random.randn( N,M*mini_batch_size))*np.sqrt(b_br/2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of h_bs  is: (300, 4, 1000)\n",
            "Shape of h_is  is: (300, 1, 1000)\n",
            "Shape of h_id  is: (300, 1, 1000)\n",
            "Shape of h_sd  is: (300, 1, 1000)\n",
            "Shape of g_rs  is: (300, 16, 1000)\n",
            "Shape of g_rd  is: (300, 16, 1000)\n",
            "Shape of g_ir  is: (300, 16, 1000)\n",
            "Shape of G_br  is: (300, 16, 4000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOLKgtiZA62D",
        "outputId": "5dad80f8-e233-45a1-cb66-b3c1727542fc"
      },
      "source": [
        "dims = [2*(N*M+M+N+1),5*(N*M+M+N+1), 6*(N*M+M+N+1), 5*(N*M+M+N+1),4*(N*M+M+N+1),1*(N*M+M+N+1), 2*N+2]\n",
        "seed = np.random.randint(400)\n",
        "#seed = 12\n",
        "print(seed)\n",
        "tf.reset_default_graph()\n",
        "W1=tf.get_variable(\"W1\",[dims[1],dims[0]],dtype=\"float64\", initializer = tf.contrib.layers.xavier_initializer(seed = seed))\n",
        "b1=tf.get_variable(\"b1\",[dims[1],1],dtype=\"float64\", initializer = tf.zeros_initializer())\n",
        "W2=tf.get_variable(\"W2\",[dims[2],dims[1]],dtype=\"float64\", initializer = tf.contrib.layers.xavier_initializer(seed = seed))\n",
        "b2=tf.get_variable(\"b2\",[dims[2],1],dtype=\"float64\", initializer = tf.zeros_initializer())\n",
        "W3=tf.get_variable(\"W3\",[dims[3],dims[2]],dtype=\"float64\", initializer = tf.contrib.layers.xavier_initializer(seed = seed))\n",
        "b3=tf.get_variable(\"b3\",[dims[3],1],dtype=\"float64\", initializer = tf.zeros_initializer())\n",
        "W4=tf.get_variable(\"W4\",[dims[4],dims[3]],dtype=\"float64\", initializer = tf.contrib.layers.xavier_initializer(seed = seed))\n",
        "b4=tf.get_variable(\"b4\",[dims[4],1],dtype=\"float64\", initializer = tf.zeros_initializer())\n",
        "W5=tf.get_variable(\"W5\",[dims[5],dims[4]],dtype=\"float64\", initializer = tf.contrib.layers.xavier_initializer(seed = seed))\n",
        "b5=tf.get_variable(\"b5\",[dims[5],1],dtype=\"float64\", initializer = tf.zeros_initializer())\n",
        "W6=tf.get_variable(\"W6\",[dims[6],dims[5]],dtype=\"float64\", initializer = tf.contrib.layers.xavier_initializer(seed = seed))\n",
        "b6=tf.get_variable(\"b6\",[dims[6],1],dtype=\"float64\", initializer = tf.zeros_initializer())\n",
        "#tau = tf.get_variable(\"tau\",[1,1],dtype=\"float64\", initializer = tf.zeros_initializer())\n",
        " \n",
        "X = tf.placeholder(tf.float64, shape=(2*(N*M+M+N+1),None))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "392\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwqDWbowBNCX"
      },
      "source": [
        "Z1 = tf.add(tf.matmul(W1,X),b1)                     # Linear Layer\n",
        "mean, variance = tf.nn.moments(Z1, axes= [1] ,keepdims=True)\n",
        "Z1_BN = tf.nn.batch_normalization(Z1, mean=mean, variance= variance, scale=1.0, offset= None, variance_epsilon=10**-5) # Batch Normalization Layer\n",
        "A1 = tf.nn.dropout(tf.nn.relu(Z1_BN),rate = 0.0, seed= 95)        # Relu Layer\n",
        " \n",
        "Z2 = tf.add(tf.matmul(W2,A1),b2)                     # Linear Layer\n",
        "mean, variance = tf.nn.moments(Z2, axes= [1] ,keepdims=True)\n",
        "Z2_BN = tf.nn.batch_normalization(Z2, mean=mean, variance= variance, scale=1.0, offset= None, variance_epsilon=10**-5) # Batch Normalization Layer\n",
        "A2 = tf.nn.dropout(tf.nn.relu(Z2),rate = 0.0, seed= 195) \n",
        " \n",
        " \n",
        "Z3 = tf.add(tf.matmul(W3,A2),b3)                     # Linear Layer\n",
        "mean, variance = tf.nn.moments(Z3, axes= [1] ,keepdims=True)\n",
        "Z3_BN = tf.nn.batch_normalization(Z3, mean=mean, variance= variance, scale=1, offset= None, variance_epsilon=10**-5) # Batch Normalization Layer\n",
        "A3 = tf.nn.relu(Z3)\n",
        "\n",
        "Z4 = tf.add(tf.matmul(W4,A3),b4)                     # Linear Layer\n",
        "mean, variance = tf.nn.moments(Z4, axes= [1],keepdims=True)\n",
        "Z4_BN = tf.nn.batch_normalization(Z4, mean=mean, variance= variance, scale=1, offset= None, variance_epsilon=10**-5) # Batch Normalization Layer\n",
        "A4 = tf.nn.relu(Z4) \n",
        "\n",
        "\n",
        "Z5 = tf.add(tf.matmul(W5,A4),b5)                     # Linear Layer\n",
        "mean, variance = tf.nn.moments(Z5, axes= [1],keepdims=True)\n",
        "Z5_BN = tf.nn.batch_normalization(Z5, mean=mean, variance= variance, scale=1, offset= None, variance_epsilon=10**-5) # Batch Normalization Layer\n",
        "A5 = tf.nn.leaky_relu(Z5_BN,0.) \n",
        "theta1 = tf.nn.sigmoid(Z5)\n",
        "Z6 = tf.add(tf.matmul(W6,A5),b6)                     # Linear Layer\n",
        "# mean, variance = tf.nn.moments(Z6, axes= [1],keepdims=True)\n",
        "# Z6_BN = tf.nn.batch_normalization(Z6, mean=mean, variance= variance/variance, scale=1, offset= None, variance_epsilon=10**-5) # Batch Normalization Layer\n",
        "theta1 = tf.nn.sigmoid(Z6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjsTiX8zZo4z"
      },
      "source": [
        "## The cost function is,\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLSwW0PBcvq1"
      },
      "source": [
        "real = tf.zeros([N,tf.shape(X)[1]],dtype=tf.float64)\n",
        "#tau = tf.constant(0.1,dtype= tf.float64)\n",
        "log_base = tf.constant(2,dtype=tf.float64)\n",
        "etta = 1 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RqN-u2hIpdPX"
      },
      "source": [
        "theta_et1, theta_it1,tau,lemda = tf.split(theta1, num_or_size_splits= [N,N,1,1], axis = 0)\n",
        "theta_et = tf.exp(tf.complex(real,theta_et1*2*np.pi))\n",
        "theta_it = tf.exp(tf.complex(real,theta_it1*2*np.pi))\n",
        "# theta_et =loadmat('/content/drive/MyDrive/DL_prejects/Cost_varification_data/theta_et.mat')['theta_et']\n",
        "# print(theta_et.shape)  \n",
        "# theta_it = loadmat('/content/drive/MyDrive/DL_prejects/Cost_varification_data/theta_it.mat')['theta_it']\n",
        "# print(theta_it.shape) \n",
        "\n",
        "g_theta_et1 = tf.reshape(tf.complex(X[0:N*M,:],X[N*M:2*N*M,:]),[N,M*tf.shape(X)[1]])\n",
        "\n",
        "g_theta_et2 = g_theta_et1*tf.repeat(theta_et,M,axis=1)\n",
        "\n",
        "#g_theta_et6 = tf.reduce_sum(tf.transpose(g_theta_et2),axis = 1, keepdims=True)\n",
        "\n",
        "g_theta_et = tf.transpose(tf.reshape((tf.reduce_sum(g_theta_et2,axis = 0, keepdims= True)),[tf.shape(X)[1],M]),perm=[1,0])\n",
        "\n",
        "h_bs_c = tf.complex(X[2*N*M:2*N*M+M,:],X[2*N*M+M:2*N*M+2*M,:])\n",
        "\n",
        "\n",
        "Es = etta*tau*P_b*(tf.real(tf.norm((h_bs_c+g_theta_et), axis=0))**2)\n",
        "\n",
        "P_s = etta*(tau/(1-tau))*P_b*(tf.real(tf.norm((h_bs_c+g_theta_et), axis=0))**2)\n",
        "\n",
        "\n",
        "g_theta_it1 = tf.complex(X[2*N*M+2*M:2*N*M+2*M+N,:],X[2*N*M+2*M+N:2*N*M+2*M+2*N,:])\n",
        "g_theta_it = tf.reduce_sum(g_theta_it1*theta_it,axis = 0)\n",
        "h_sd_c = tf.complex(X[2*N*M+2*M+2*N:2*N*M+2*M+2*N+1,:],X[2*N*M+2*M+2*N+1:2*N*M+2*M+2*N+2,:])\n",
        "rd = P_s*(tf.abs((h_sd_c+g_theta_it)**2))/((sigma_n)**2)\n",
        "r = (1-tau)*tf.log(1+rd)/tf.log(log_base)\n",
        "cost = -tf.reduce_mean(r)*10\n",
        "cost1 = cost+lemda*tf.norm(theta1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aj7ZnWoGEd5R"
      },
      "source": [
        "\n",
        "# g_rs = loadmat('/content/drive/MyDrive/DL_prejects/Cost_varification_data/g_rs.mat')['g_rs']\n",
        "# G_br = loadmat('/content/drive/MyDrive/DL_prejects/Cost_varification_data/G_br.mat')['G_br']\n",
        "# h_bs = loadmat('/content/drive/MyDrive/DL_prejects/Cost_varification_data/h_bs.mat')['h_bs']\n",
        "# g_rd = loadmat('/content/drive/MyDrive/DL_prejects/Cost_varification_data/g_rd.mat')['g_rd']\n",
        "# h_sd = loadmat('/content/drive/MyDrive/DL_prejects/Cost_varification_data/h_sd.mat')['h_sd']\n",
        "\n",
        "# # j =0\n",
        "# mini_batch_size = 3;\n",
        "# g_rs1 =  np.repeat(g_rs,M,axis = 1)\n",
        "# F = np.reshape(G_br*g_rs1, [N*M,mini_batch_size])\n",
        "# F = np.concatenate([np.real(F),np.imag(F)],axis=0)\n",
        "# F1 = np.concatenate([np.real(h_bs),np.imag(h_bs)],axis=0)\n",
        "# F2 = g_rd*g_rs\n",
        "# F2 = np.concatenate([np.real(F2),np.imag(F2)],axis=0)\n",
        "# F3 = np.concatenate([np.real(h_sd),np.imag(h_sd)],axis=0)\n",
        "# F = np.concatenate([F,F1,F2,F3],axis=0)\n",
        "# F.shape\n",
        "# sess = tf.Session()\n",
        "# init = tf.global_variables_initializer()\n",
        "# sess.run(init)\n",
        "# print(sess.run(r,feed_dict={X:F}))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dlzO2CxvWrB"
      },
      "source": [
        "decay = 0.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYNJOh4GBmoI"
      },
      "source": [
        "lr = 0.001\n",
        "step_rate =500*10**2\n",
        "\n",
        "\n",
        "global_step = tf.Variable(0, trainable=False)\n",
        "increment_global_step = tf.assign(global_step, global_step + 1)\n",
        "learning_rate = tf.train.exponential_decay(lr, global_step, step_rate, decay, staircase=True)\n",
        "\n",
        "  \n",
        "optimizer = tf.train.AdamOptimizer(learning_rate= learning_rate )\n",
        "minimize_loss = optimizer.minimize(cost, global_step = global_step)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4G7IyIKGBnjb"
      },
      "source": [
        "init = tf.global_variables_initializer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBhx94XKVh4n"
      },
      "source": [
        "sess = tf.Session()\n",
        "sess.run(init)\n",
        "num_epochs = 500"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pbaykM2CBqGX",
        "outputId": "73b95a52-8610-4947-864a-933f0ad770d9"
      },
      "source": [
        "cost_train_plt = []\n",
        "cost_test_plt = []\n",
        "start_time=time.time()\n",
        "for epoch in range(num_epochs):\n",
        "  epoch1 = epoch\n",
        "  #np.random.shuffle(h_train)\n",
        "  for j in range(num_batches):\n",
        "    g_rs1 =  np.repeat(g_rs[j],M,axis = 1)\n",
        "    F = np.reshape(G_br[j]*g_rs1, [N*M,mini_batch_size])\n",
        "    F = np.concatenate([np.real(F),np.imag(F)],axis=0)\n",
        "    F1 = np.concatenate([np.real(h_bs[j]),np.imag(h_bs[j])],axis=0)\n",
        "    F2 = g_rd[j]*g_rs[j]\n",
        "    F2 = np.concatenate([np.real(F2),np.imag(F2)],axis=0)\n",
        "    F3 = np.concatenate([np.real(h_sd[j]),np.imag(h_sd[j])],axis=0)\n",
        "    F = np.concatenate([F,F1,F2,F3],axis=0)\n",
        "    sess.run(minimize_loss,feed_dict={X:F})\n",
        "    #print(sess.run(cost, feed_dict={X:h_train[j]}))\n",
        "  if (epoch) % display_step == 0:\n",
        "      print('Learning Rate',sess.run(optimizer._lr))\n",
        "      #print('Sum_Rate:',sess.run(sum_R, feed_dict={X:h_train[j]}))\n",
        "      c = sess.run(cost, feed_dict={X:F})\n",
        "      cost_train_plt.append(np.reshape(c,[1]))\n",
        "\n",
        "      g_rs1 =  np.repeat(g_rs_t,M,axis = 1)\n",
        "      F = np.reshape(G_br_t*g_rs1, [N*M,mini_batch_size])\n",
        "      F = np.concatenate([np.real(F),np.imag(F)],axis=0)\n",
        "      F1 = np.concatenate([np.real(h_bs_t),np.imag(h_bs_t)],axis=0)\n",
        "      F2 = g_rd_t*g_rs_t\n",
        "      F2 = np.concatenate([np.real(F2),np.imag(F2)],axis=0)\n",
        "      F3 = np.concatenate([np.real(h_sd_t),np.imag(h_sd_t)],axis=0)\n",
        "      F_test = np.concatenate([F,F1,F2,F3],axis=0)\n",
        "      \n",
        "      cost_test=sess.run(cost, feed_dict={X:F_test}) \n",
        "      cost_test_plt.append(np.reshape(cost_test,[1]))\n",
        "      print(\"Epoch: \" +str(epoch) + \" Training Loss= \"+ str(c) + \" Validation Loss= \"+ str(cost_test) )\n",
        "      #print(sess.run(P,feed_dict={X:h_train[j]}))\n",
        "duration = time.time()-start_time        \n",
        "print(\"The duration for training is: \"+str(duration)) \n",
        "print(\"The cost for test data is: \" + str(cost_test))\n",
        "plt.subplot(223)\n",
        "plt.plot(cost_train_plt ,label= 'training Loss')\n",
        "plt.plot(cost_test_plt, label= 'test loss')\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
        "plt.show()\n",
        "print(\"Optimization Finished!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learning Rate 0.0005\n",
            "Epoch: 0 Training Loss= -39.94982973506294 Validation Loss= -40.94040278809714\n",
            "Learning Rate 0.0005\n",
            "Epoch: 1 Training Loss= -39.95081243526768 Validation Loss= -40.934177509430675\n",
            "Learning Rate 0.0005\n",
            "Epoch: 2 Training Loss= -39.94922391953726 Validation Loss= -40.9416486547908\n",
            "Learning Rate 0.0005\n",
            "Epoch: 3 Training Loss= -39.95289353079941 Validation Loss= -40.94390499390339\n",
            "Learning Rate 0.0005\n",
            "Epoch: 4 Training Loss= -39.951518132555705 Validation Loss= -40.93743376814445\n",
            "Learning Rate 0.0005\n",
            "Epoch: 5 Training Loss= -39.95311229358228 Validation Loss= -40.939053819058046\n",
            "Learning Rate 0.0005\n",
            "Epoch: 6 Training Loss= -39.94940420107713 Validation Loss= -40.93839856670212\n",
            "Learning Rate 0.0005\n",
            "Epoch: 7 Training Loss= -39.94660734526092 Validation Loss= -40.940440032163714\n",
            "Learning Rate 0.0005\n",
            "Epoch: 8 Training Loss= -39.944732473594065 Validation Loss= -40.938982593672094\n",
            "Learning Rate 0.0005\n",
            "Epoch: 9 Training Loss= -39.94859354847483 Validation Loss= -40.9399556153499\n",
            "Learning Rate 0.0005\n",
            "Epoch: 10 Training Loss= -39.95019557658603 Validation Loss= -40.93776279637914\n",
            "Learning Rate 0.0005\n",
            "Epoch: 11 Training Loss= -39.944981428538675 Validation Loss= -40.94015115589234\n",
            "Learning Rate 0.0005\n",
            "Epoch: 12 Training Loss= -39.95134824589426 Validation Loss= -40.93745648073111\n",
            "Learning Rate 0.0005\n",
            "Epoch: 13 Training Loss= -39.945786920762814 Validation Loss= -40.93685524285753\n",
            "Learning Rate 0.0005\n",
            "Epoch: 14 Training Loss= -39.95305130966426 Validation Loss= -40.94486591550031\n",
            "Learning Rate 0.0005\n",
            "Epoch: 15 Training Loss= -39.9526759714506 Validation Loss= -40.94456238473231\n",
            "Learning Rate 0.0005\n",
            "Epoch: 16 Training Loss= -39.95970006312063 Validation Loss= -40.94529552803225\n",
            "Learning Rate 0.0005\n",
            "Epoch: 17 Training Loss= -39.95250455339898 Validation Loss= -40.94426294293989\n",
            "Learning Rate 0.0005\n",
            "Epoch: 18 Training Loss= -39.958319854042095 Validation Loss= -40.946326505341474\n",
            "Learning Rate 0.0005\n",
            "Epoch: 19 Training Loss= -39.955187731729346 Validation Loss= -40.941323462689844\n",
            "Learning Rate 0.0005\n",
            "Epoch: 20 Training Loss= -39.953803847857685 Validation Loss= -40.94481779261137\n",
            "Learning Rate 0.0005\n",
            "Epoch: 21 Training Loss= -39.954548812859436 Validation Loss= -40.94324508458351\n",
            "Learning Rate 0.0005\n",
            "Epoch: 22 Training Loss= -39.95580679073682 Validation Loss= -40.94258727741585\n",
            "Learning Rate 0.0005\n",
            "Epoch: 23 Training Loss= -39.96100008472331 Validation Loss= -40.947459397552706\n",
            "Learning Rate 0.0005\n",
            "Epoch: 24 Training Loss= -39.95576654085417 Validation Loss= -40.94150550615368\n",
            "Learning Rate 0.0005\n",
            "Epoch: 25 Training Loss= -39.96044830774984 Validation Loss= -40.94182481024619\n",
            "Learning Rate 0.0005\n",
            "Epoch: 26 Training Loss= -39.95311567346909 Validation Loss= -40.94119810989604\n",
            "Learning Rate 0.0005\n",
            "Epoch: 27 Training Loss= -39.95590478973063 Validation Loss= -40.94388655938933\n",
            "Learning Rate 0.0005\n",
            "Epoch: 28 Training Loss= -39.9529814816873 Validation Loss= -40.94060328902466\n",
            "Learning Rate 0.0005\n",
            "Epoch: 29 Training Loss= -39.95570385708369 Validation Loss= -40.93838003593825\n",
            "Learning Rate 0.0005\n",
            "Epoch: 30 Training Loss= -39.95394167050984 Validation Loss= -40.93668433222669\n",
            "Learning Rate 0.0005\n",
            "Epoch: 31 Training Loss= -39.95402871830426 Validation Loss= -40.9391648460944\n",
            "Learning Rate 0.0005\n",
            "Epoch: 32 Training Loss= -39.95524454767302 Validation Loss= -40.93704611875086\n",
            "Learning Rate 0.0005\n",
            "Epoch: 33 Training Loss= -39.950084288437495 Validation Loss= -40.93857154867783\n",
            "Learning Rate 0.0005\n",
            "Epoch: 34 Training Loss= -39.95782816504385 Validation Loss= -40.94132972039102\n",
            "Learning Rate 0.0005\n",
            "Epoch: 35 Training Loss= -39.953311089654655 Validation Loss= -40.940882332807405\n",
            "Learning Rate 0.0005\n",
            "Epoch: 36 Training Loss= -39.949881493138065 Validation Loss= -40.938095486094454\n",
            "Learning Rate 0.0005\n",
            "Epoch: 37 Training Loss= -39.950733162685886 Validation Loss= -40.94354188012475\n",
            "Learning Rate 0.0005\n",
            "Epoch: 38 Training Loss= -39.954066969981604 Validation Loss= -40.9443664790045\n",
            "Learning Rate 0.0005\n",
            "Epoch: 39 Training Loss= -39.95295780995667 Validation Loss= -40.93963800819211\n",
            "Learning Rate 0.0005\n",
            "Epoch: 40 Training Loss= -39.95410572418218 Validation Loss= -40.94396777696417\n",
            "Learning Rate 0.0005\n",
            "Epoch: 41 Training Loss= -39.95672568644029 Validation Loss= -40.93941436409101\n",
            "Learning Rate 0.0005\n",
            "Epoch: 42 Training Loss= -39.95325774989767 Validation Loss= -40.9425461050218\n",
            "Learning Rate 0.0005\n",
            "Epoch: 43 Training Loss= -39.95398487768223 Validation Loss= -40.940380562377754\n",
            "Learning Rate 0.0005\n",
            "Epoch: 44 Training Loss= -39.954839483956384 Validation Loss= -40.93800990127035\n",
            "Learning Rate 0.0005\n",
            "Epoch: 45 Training Loss= -39.95566049748919 Validation Loss= -40.94176615663504\n",
            "Learning Rate 0.0005\n",
            "Epoch: 46 Training Loss= -39.95613899109412 Validation Loss= -40.94157388956969\n",
            "Learning Rate 0.0005\n",
            "Epoch: 47 Training Loss= -39.951686448125784 Validation Loss= -40.938300017261405\n",
            "Learning Rate 0.0005\n",
            "Epoch: 48 Training Loss= -39.95909084952652 Validation Loss= -40.93750300502954\n",
            "Learning Rate 0.0005\n",
            "Epoch: 49 Training Loss= -39.957774693476516 Validation Loss= -40.938673119422184\n",
            "Learning Rate 0.0005\n",
            "Epoch: 50 Training Loss= -39.959667434482306 Validation Loss= -40.93893544038813\n",
            "Learning Rate 0.0005\n",
            "Epoch: 51 Training Loss= -39.95953851612952 Validation Loss= -40.94480167006972\n",
            "Learning Rate 0.0005\n",
            "Epoch: 52 Training Loss= -39.96169933287327 Validation Loss= -40.94447440578535\n",
            "Learning Rate 0.0005\n",
            "Epoch: 53 Training Loss= -39.96038360479125 Validation Loss= -40.944758631960184\n",
            "Learning Rate 0.0005\n",
            "Epoch: 54 Training Loss= -39.96060685489433 Validation Loss= -40.942126955697766\n",
            "Learning Rate 0.0005\n",
            "Epoch: 55 Training Loss= -39.95994595707907 Validation Loss= -40.94087048029707\n",
            "Learning Rate 0.0005\n",
            "Epoch: 56 Training Loss= -39.96352207029218 Validation Loss= -40.94281245678873\n",
            "Learning Rate 0.0005\n",
            "Epoch: 57 Training Loss= -39.96007513708457 Validation Loss= -40.93753558395096\n",
            "Learning Rate 0.0005\n",
            "Epoch: 58 Training Loss= -39.96091801941948 Validation Loss= -40.9378981775386\n",
            "Learning Rate 0.0005\n",
            "Epoch: 59 Training Loss= -39.95637975977423 Validation Loss= -40.93853495828309\n",
            "Learning Rate 0.0005\n",
            "Epoch: 60 Training Loss= -39.96637771742128 Validation Loss= -40.94085252674563\n",
            "Learning Rate 0.0005\n",
            "Epoch: 61 Training Loss= -39.96255360670744 Validation Loss= -40.94175063400432\n",
            "Learning Rate 0.0005\n",
            "Epoch: 62 Training Loss= -39.96259922555786 Validation Loss= -40.942405002915194\n",
            "Learning Rate 0.0005\n",
            "Epoch: 63 Training Loss= -39.95592209236348 Validation Loss= -40.94311618985717\n",
            "Learning Rate 0.0005\n",
            "Epoch: 64 Training Loss= -39.958408759672615 Validation Loss= -40.94728476265011\n",
            "Learning Rate 0.0005\n",
            "Epoch: 65 Training Loss= -39.95520835023035 Validation Loss= -40.94088556053299\n",
            "Learning Rate 0.0005\n",
            "Epoch: 66 Training Loss= -39.961808705201236 Validation Loss= -40.9440835527877\n",
            "Learning Rate 0.0005\n",
            "Epoch: 67 Training Loss= -39.95557792465834 Validation Loss= -40.9400261346768\n",
            "Learning Rate 0.0005\n",
            "Epoch: 68 Training Loss= -39.95638695732609 Validation Loss= -40.9382803207174\n",
            "Learning Rate 0.0005\n",
            "Epoch: 69 Training Loss= -39.95854136878209 Validation Loss= -40.94589257278073\n",
            "Learning Rate 0.0005\n",
            "Epoch: 70 Training Loss= -39.9598678749841 Validation Loss= -40.942420460776916\n",
            "Learning Rate 0.0005\n",
            "Epoch: 71 Training Loss= -39.9608287496739 Validation Loss= -40.94217209847677\n",
            "Learning Rate 0.0005\n",
            "Epoch: 72 Training Loss= -39.954349757627014 Validation Loss= -40.939822192988906\n",
            "Learning Rate 0.0005\n",
            "Epoch: 73 Training Loss= -39.96242785141364 Validation Loss= -40.945571919499095\n",
            "Learning Rate 0.0005\n",
            "Epoch: 74 Training Loss= -39.96057740704146 Validation Loss= -40.94223990064978\n",
            "Learning Rate 0.0005\n",
            "Epoch: 75 Training Loss= -39.96633017236904 Validation Loss= -40.94480607311748\n",
            "Learning Rate 0.0005\n",
            "Epoch: 76 Training Loss= -39.95872813419964 Validation Loss= -40.94333397963558\n",
            "Learning Rate 0.0005\n",
            "Epoch: 77 Training Loss= -39.960927559291996 Validation Loss= -40.936210318659846\n",
            "Learning Rate 0.0005\n",
            "Epoch: 78 Training Loss= -39.95915381377673 Validation Loss= -40.939807828992556\n",
            "Learning Rate 0.0005\n",
            "Epoch: 79 Training Loss= -39.958932880390215 Validation Loss= -40.94331214764377\n",
            "Learning Rate 0.0005\n",
            "Epoch: 80 Training Loss= -39.9661511551875 Validation Loss= -40.94487482682996\n",
            "Learning Rate 0.0005\n",
            "Epoch: 81 Training Loss= -39.96121851572695 Validation Loss= -40.94458622418579\n",
            "Learning Rate 0.0005\n",
            "Epoch: 82 Training Loss= -39.96710179179689 Validation Loss= -40.94517877544098\n",
            "Learning Rate 0.0005\n",
            "Epoch: 83 Training Loss= -39.962163290223835 Validation Loss= -40.93978873869482\n",
            "Learning Rate 0.0005\n",
            "Epoch: 84 Training Loss= -39.966247483439375 Validation Loss= -40.942509999902654\n",
            "Learning Rate 0.0005\n",
            "Epoch: 85 Training Loss= -39.96419051350231 Validation Loss= -40.9420728281523\n",
            "Learning Rate 0.0005\n",
            "Epoch: 86 Training Loss= -39.96381778170428 Validation Loss= -40.94842004301598\n",
            "Learning Rate 0.0005\n",
            "Epoch: 87 Training Loss= -39.96276620533984 Validation Loss= -40.946863959200606\n",
            "Learning Rate 0.0005\n",
            "Epoch: 88 Training Loss= -39.96150766983382 Validation Loss= -40.94461354643997\n",
            "Learning Rate 0.0005\n",
            "Epoch: 89 Training Loss= -39.96458410056806 Validation Loss= -40.94233375189627\n",
            "Learning Rate 0.0005\n",
            "Epoch: 90 Training Loss= -39.95468136947311 Validation Loss= -40.941448876012274\n",
            "Learning Rate 0.0005\n",
            "Epoch: 91 Training Loss= -39.96357320988031 Validation Loss= -40.94409108161666\n",
            "Learning Rate 0.0005\n",
            "Epoch: 92 Training Loss= -39.96338977220369 Validation Loss= -40.948158998734556\n",
            "Learning Rate 0.0005\n",
            "Epoch: 93 Training Loss= -39.961478251630346 Validation Loss= -40.93965312529278\n",
            "Learning Rate 0.0005\n",
            "Epoch: 94 Training Loss= -39.96185509827853 Validation Loss= -40.94606084118091\n",
            "Learning Rate 0.0005\n",
            "Epoch: 95 Training Loss= -39.96545613539434 Validation Loss= -40.94424062220784\n",
            "Learning Rate 0.0005\n",
            "Epoch: 96 Training Loss= -39.96551065707869 Validation Loss= -40.94364803471681\n",
            "Learning Rate 0.0005\n",
            "Epoch: 97 Training Loss= -39.96596245184766 Validation Loss= -40.946284558131715\n",
            "Learning Rate 0.0005\n",
            "Epoch: 98 Training Loss= -39.96095033983133 Validation Loss= -40.94460256151987\n",
            "Learning Rate 0.0005\n",
            "Epoch: 99 Training Loss= -39.964891260201426 Validation Loss= -40.94325093857875\n",
            "Learning Rate 0.0005\n",
            "Epoch: 100 Training Loss= -39.966338323238695 Validation Loss= -40.94844685859235\n",
            "Learning Rate 0.0005\n",
            "Epoch: 101 Training Loss= -39.966310235523814 Validation Loss= -40.9458828867352\n",
            "Learning Rate 0.0005\n",
            "Epoch: 102 Training Loss= -39.96413804879297 Validation Loss= -40.94499506349485\n",
            "Learning Rate 0.0005\n",
            "Epoch: 103 Training Loss= -39.96716381179137 Validation Loss= -40.945954658917884\n",
            "Learning Rate 0.0005\n",
            "Epoch: 104 Training Loss= -39.965545641510026 Validation Loss= -40.9455268255951\n",
            "Learning Rate 0.0005\n",
            "Epoch: 105 Training Loss= -39.96714283178385 Validation Loss= -40.93830719465395\n",
            "Learning Rate 0.0005\n",
            "Epoch: 106 Training Loss= -39.96384115131046 Validation Loss= -40.94409156404473\n",
            "Learning Rate 0.0005\n",
            "Epoch: 107 Training Loss= -39.96741698752727 Validation Loss= -40.94855128883067\n",
            "Learning Rate 0.0005\n",
            "Epoch: 108 Training Loss= -39.96526532181732 Validation Loss= -40.94566461875602\n",
            "Learning Rate 0.0005\n",
            "Epoch: 109 Training Loss= -39.965744259299086 Validation Loss= -40.94662404109924\n",
            "Learning Rate 0.0005\n",
            "Epoch: 110 Training Loss= -39.960303195171065 Validation Loss= -40.94360463835697\n",
            "Learning Rate 0.0005\n",
            "Epoch: 111 Training Loss= -39.95897808613295 Validation Loss= -40.941377070352296\n",
            "Learning Rate 0.0005\n",
            "Epoch: 112 Training Loss= -39.9635765842568 Validation Loss= -40.94130807980505\n",
            "Learning Rate 0.0005\n",
            "Epoch: 113 Training Loss= -39.96321617845743 Validation Loss= -40.945071743925354\n",
            "Learning Rate 0.0005\n",
            "Epoch: 114 Training Loss= -39.96533185885011 Validation Loss= -40.94606948770063\n",
            "Learning Rate 0.0005\n",
            "Epoch: 115 Training Loss= -39.95648501512885 Validation Loss= -40.94290441881238\n",
            "Learning Rate 0.0005\n",
            "Epoch: 116 Training Loss= -39.96729646100234 Validation Loss= -40.9497989315777\n",
            "Learning Rate 0.0005\n",
            "Epoch: 117 Training Loss= -39.96569436050081 Validation Loss= -40.946735171307495\n",
            "Learning Rate 0.0005\n",
            "Epoch: 118 Training Loss= -39.96438715538088 Validation Loss= -40.9501248170649\n",
            "Learning Rate 0.0005\n",
            "Epoch: 119 Training Loss= -39.960649632445275 Validation Loss= -40.94394902565247\n",
            "Learning Rate 0.0005\n",
            "Epoch: 120 Training Loss= -39.9620189020179 Validation Loss= -40.94451586216213\n",
            "Learning Rate 0.0005\n",
            "Epoch: 121 Training Loss= -39.96399261529898 Validation Loss= -40.94223387516141\n",
            "Learning Rate 0.0005\n",
            "Epoch: 122 Training Loss= -39.9619405686789 Validation Loss= -40.94221314957149\n",
            "Learning Rate 0.0005\n",
            "Epoch: 123 Training Loss= -39.95824915014709 Validation Loss= -40.93935657493057\n",
            "Learning Rate 0.00025\n",
            "Epoch: 124 Training Loss= -39.97330620657993 Validation Loss= -40.9498642798431\n",
            "Learning Rate 0.00025\n",
            "Epoch: 125 Training Loss= -39.98039720467278 Validation Loss= -40.95277981915067\n",
            "Learning Rate 0.00025\n",
            "Epoch: 126 Training Loss= -39.977021155275885 Validation Loss= -40.953111689854666\n",
            "Learning Rate 0.00025\n",
            "Epoch: 127 Training Loss= -39.97884235826577 Validation Loss= -40.95646249036423\n",
            "Learning Rate 0.00025\n",
            "Epoch: 128 Training Loss= -39.980629681702034 Validation Loss= -40.95182199683813\n",
            "Learning Rate 0.00025\n",
            "Epoch: 129 Training Loss= -39.98263870273576 Validation Loss= -40.95330133988132\n",
            "Learning Rate 0.00025\n",
            "Epoch: 130 Training Loss= -39.98390868061509 Validation Loss= -40.95606763943478\n",
            "Learning Rate 0.00025\n",
            "Epoch: 131 Training Loss= -39.98249498911696 Validation Loss= -40.9561864160319\n",
            "Learning Rate 0.00025\n",
            "Epoch: 132 Training Loss= -39.98190589155493 Validation Loss= -40.9549434255173\n",
            "Learning Rate 0.00025\n",
            "Epoch: 133 Training Loss= -39.98198617022578 Validation Loss= -40.95350726737798\n",
            "Learning Rate 0.00025\n",
            "Epoch: 134 Training Loss= -39.9812099802411 Validation Loss= -40.95394191610996\n",
            "Learning Rate 0.00025\n",
            "Epoch: 135 Training Loss= -39.98325015561668 Validation Loss= -40.95366072713532\n",
            "Learning Rate 0.00025\n",
            "Epoch: 136 Training Loss= -39.984045891891064 Validation Loss= -40.95212487323942\n",
            "Learning Rate 0.00025\n",
            "Epoch: 137 Training Loss= -39.980810365903075 Validation Loss= -40.95299037686358\n",
            "Learning Rate 0.00025\n",
            "Epoch: 138 Training Loss= -39.984870112244884 Validation Loss= -40.95434375166824\n",
            "Learning Rate 0.00025\n",
            "Epoch: 139 Training Loss= -39.98371959777116 Validation Loss= -40.95369537289646\n",
            "Learning Rate 0.00025\n",
            "Epoch: 140 Training Loss= -39.98515767078353 Validation Loss= -40.953670377457726\n",
            "Learning Rate 0.00025\n",
            "Epoch: 141 Training Loss= -39.985534751560564 Validation Loss= -40.95422116176367\n",
            "Learning Rate 0.00025\n",
            "Epoch: 142 Training Loss= -39.986908693415636 Validation Loss= -40.952710368258344\n",
            "Learning Rate 0.00025\n",
            "Epoch: 143 Training Loss= -39.98315286932442 Validation Loss= -40.95242927121055\n",
            "Learning Rate 0.00025\n",
            "Epoch: 144 Training Loss= -39.98333723126386 Validation Loss= -40.95394055686738\n",
            "Learning Rate 0.00025\n",
            "Epoch: 145 Training Loss= -39.98319519646116 Validation Loss= -40.95511607557178\n",
            "Learning Rate 0.00025\n",
            "Epoch: 146 Training Loss= -39.986249909030974 Validation Loss= -40.951912546130686\n",
            "Learning Rate 0.00025\n",
            "Epoch: 147 Training Loss= -39.98390421997038 Validation Loss= -40.9530049173398\n",
            "Learning Rate 0.00025\n",
            "Epoch: 148 Training Loss= -39.98501934524833 Validation Loss= -40.952639421666824\n",
            "Learning Rate 0.00025\n",
            "Epoch: 149 Training Loss= -39.986648195411576 Validation Loss= -40.95373089529655\n",
            "Learning Rate 0.00025\n",
            "Epoch: 150 Training Loss= -39.98218848386286 Validation Loss= -40.9531896668921\n",
            "Learning Rate 0.00025\n",
            "Epoch: 151 Training Loss= -39.98999776194935 Validation Loss= -40.952034501966786\n",
            "Learning Rate 0.00025\n",
            "Epoch: 152 Training Loss= -39.98815285686392 Validation Loss= -40.95315761850084\n",
            "Learning Rate 0.00025\n",
            "Epoch: 153 Training Loss= -39.986123571319766 Validation Loss= -40.95224015745535\n",
            "Learning Rate 0.00025\n",
            "Epoch: 154 Training Loss= -39.98535030572559 Validation Loss= -40.95282562633205\n",
            "Learning Rate 0.00025\n",
            "Epoch: 155 Training Loss= -39.98467816949304 Validation Loss= -40.9508509336479\n",
            "Learning Rate 0.00025\n",
            "Epoch: 156 Training Loss= -39.986496279944745 Validation Loss= -40.95354070718936\n",
            "Learning Rate 0.00025\n",
            "Epoch: 157 Training Loss= -39.98455435324975 Validation Loss= -40.95311703159386\n",
            "Learning Rate 0.00025\n",
            "Epoch: 158 Training Loss= -39.986549242216384 Validation Loss= -40.954252942355026\n",
            "Learning Rate 0.00025\n",
            "Epoch: 159 Training Loss= -39.987070223326654 Validation Loss= -40.955633497520424\n",
            "Learning Rate 0.00025\n",
            "Epoch: 160 Training Loss= -39.987093959632986 Validation Loss= -40.95343651733127\n",
            "Learning Rate 0.00025\n",
            "Epoch: 161 Training Loss= -39.98719498272243 Validation Loss= -40.95444718238646\n",
            "Learning Rate 0.00025\n",
            "Epoch: 162 Training Loss= -39.986883209101556 Validation Loss= -40.95403512868997\n",
            "Learning Rate 0.00025\n",
            "Epoch: 163 Training Loss= -39.986935347930036 Validation Loss= -40.954540164184834\n",
            "Learning Rate 0.00025\n",
            "Epoch: 164 Training Loss= -39.98632439310876 Validation Loss= -40.95404223924114\n",
            "Learning Rate 0.00025\n",
            "Epoch: 165 Training Loss= -39.99268050640284 Validation Loss= -40.952661934935776\n",
            "Learning Rate 0.00025\n",
            "Epoch: 166 Training Loss= -39.98911452458822 Validation Loss= -40.95252883446405\n",
            "Learning Rate 0.00025\n",
            "Epoch: 167 Training Loss= -39.98765430525935 Validation Loss= -40.94992722591588\n",
            "Learning Rate 0.00025\n",
            "Epoch: 168 Training Loss= -39.98778228758995 Validation Loss= -40.954738999691536\n",
            "Learning Rate 0.00025\n",
            "Epoch: 169 Training Loss= -39.987675244016515 Validation Loss= -40.955263833951946\n",
            "Learning Rate 0.00025\n",
            "Epoch: 170 Training Loss= -39.99248463745935 Validation Loss= -40.95236807807288\n",
            "Learning Rate 0.00025\n",
            "Epoch: 171 Training Loss= -39.98842843747194 Validation Loss= -40.954537680754456\n",
            "Learning Rate 0.00025\n",
            "Epoch: 172 Training Loss= -39.987762887389174 Validation Loss= -40.955201505962286\n",
            "Learning Rate 0.00025\n",
            "Epoch: 173 Training Loss= -39.98743473947621 Validation Loss= -40.95634511100429\n",
            "Learning Rate 0.00025\n",
            "Epoch: 174 Training Loss= -39.99006218457015 Validation Loss= -40.95504177520216\n",
            "Learning Rate 0.00025\n",
            "Epoch: 175 Training Loss= -39.98748004112251 Validation Loss= -40.95385219153228\n",
            "Learning Rate 0.00025\n",
            "Epoch: 176 Training Loss= -39.98804129845222 Validation Loss= -40.954767780422344\n",
            "Learning Rate 0.00025\n",
            "Epoch: 177 Training Loss= -39.98718380447751 Validation Loss= -40.95438005121862\n",
            "Learning Rate 0.00025\n",
            "Epoch: 178 Training Loss= -39.991802047874025 Validation Loss= -40.95524858441925\n",
            "Learning Rate 0.00025\n",
            "Epoch: 179 Training Loss= -39.989302951345245 Validation Loss= -40.953341791683506\n",
            "Learning Rate 0.00025\n",
            "Epoch: 180 Training Loss= -39.98745541173585 Validation Loss= -40.95487802772824\n",
            "Learning Rate 0.00025\n",
            "Epoch: 181 Training Loss= -39.98951558389773 Validation Loss= -40.95107810090134\n",
            "Learning Rate 0.00025\n",
            "Epoch: 182 Training Loss= -39.9870138024437 Validation Loss= -40.95396169247647\n",
            "Learning Rate 0.00025\n",
            "Epoch: 183 Training Loss= -39.98691164677024 Validation Loss= -40.95163074718545\n",
            "Learning Rate 0.00025\n",
            "Epoch: 184 Training Loss= -39.99267566938759 Validation Loss= -40.95504992161886\n",
            "Learning Rate 0.00025\n",
            "Epoch: 185 Training Loss= -39.989824451839375 Validation Loss= -40.952688709149854\n",
            "Learning Rate 0.00025\n",
            "Epoch: 186 Training Loss= -39.98860243708452 Validation Loss= -40.95598680591601\n",
            "Learning Rate 0.00025\n",
            "Epoch: 187 Training Loss= -39.987623478511 Validation Loss= -40.95568981699486\n",
            "Learning Rate 0.00025\n",
            "Epoch: 188 Training Loss= -39.989505488754425 Validation Loss= -40.95624225063376\n",
            "Learning Rate 0.00025\n",
            "Epoch: 189 Training Loss= -39.99216959765774 Validation Loss= -40.95642579799551\n",
            "Learning Rate 0.00025\n",
            "Epoch: 190 Training Loss= -39.99144369715389 Validation Loss= -40.956659530633885\n",
            "Learning Rate 0.00025\n",
            "Epoch: 191 Training Loss= -39.99218846865643 Validation Loss= -40.955074452379534\n",
            "Learning Rate 0.00025\n",
            "Epoch: 192 Training Loss= -39.98947323244146 Validation Loss= -40.954665179316265\n",
            "Learning Rate 0.00025\n",
            "Epoch: 193 Training Loss= -39.99222043017301 Validation Loss= -40.9588837888908\n",
            "Learning Rate 0.00025\n",
            "Epoch: 194 Training Loss= -39.988434858024185 Validation Loss= -40.95687880506758\n",
            "Learning Rate 0.00025\n",
            "Epoch: 195 Training Loss= -39.991534101711544 Validation Loss= -40.956911399293006\n",
            "Learning Rate 0.00025\n",
            "Epoch: 196 Training Loss= -39.98961291876161 Validation Loss= -40.95738193571674\n",
            "Learning Rate 0.00025\n",
            "Epoch: 197 Training Loss= -39.989631647250775 Validation Loss= -40.95662701059239\n",
            "Learning Rate 0.00025\n",
            "Epoch: 198 Training Loss= -39.991008604049114 Validation Loss= -40.955156834136844\n",
            "Learning Rate 0.00025\n",
            "Epoch: 199 Training Loss= -39.991950312103604 Validation Loss= -40.9574072298329\n",
            "Learning Rate 0.00025\n",
            "Epoch: 200 Training Loss= -39.98939920378098 Validation Loss= -40.956837596836976\n",
            "Learning Rate 0.00025\n",
            "Epoch: 201 Training Loss= -39.99003822110146 Validation Loss= -40.95455232571105\n",
            "Learning Rate 0.00025\n",
            "Epoch: 202 Training Loss= -39.990765227578315 Validation Loss= -40.95629718137033\n",
            "Learning Rate 0.00025\n",
            "Epoch: 203 Training Loss= -39.989449196889886 Validation Loss= -40.95507670603621\n",
            "Learning Rate 0.00025\n",
            "Epoch: 204 Training Loss= -39.99068916731899 Validation Loss= -40.95808913326714\n",
            "Learning Rate 0.00025\n",
            "Epoch: 205 Training Loss= -39.992650971930324 Validation Loss= -40.95819233791509\n",
            "Learning Rate 0.00025\n",
            "Epoch: 206 Training Loss= -39.99154418198198 Validation Loss= -40.95601200654159\n",
            "Learning Rate 0.00025\n",
            "Epoch: 207 Training Loss= -39.98890074970055 Validation Loss= -40.956467504647165\n",
            "Learning Rate 0.00025\n",
            "Epoch: 208 Training Loss= -39.98600013891516 Validation Loss= -40.95426498302898\n",
            "Learning Rate 0.00025\n",
            "Epoch: 209 Training Loss= -39.989201718562846 Validation Loss= -40.95668071623691\n",
            "Learning Rate 0.00025\n",
            "Epoch: 210 Training Loss= -39.99047895516546 Validation Loss= -40.955199503817965\n",
            "Learning Rate 0.00025\n",
            "Epoch: 211 Training Loss= -39.98932126698759 Validation Loss= -40.95517027054594\n",
            "Learning Rate 0.00025\n",
            "Epoch: 212 Training Loss= -39.99236648183698 Validation Loss= -40.95796650806325\n",
            "Learning Rate 0.00025\n",
            "Epoch: 213 Training Loss= -39.99069499544425 Validation Loss= -40.95697025710145\n",
            "Learning Rate 0.00025\n",
            "Epoch: 214 Training Loss= -39.991901051680735 Validation Loss= -40.95497739804895\n",
            "Learning Rate 0.00025\n",
            "Epoch: 215 Training Loss= -39.98989445922862 Validation Loss= -40.95367510090809\n",
            "Learning Rate 0.00025\n",
            "Epoch: 216 Training Loss= -39.9918142137157 Validation Loss= -40.950077546557814\n",
            "Learning Rate 0.00025\n",
            "Epoch: 217 Training Loss= -39.993189296651494 Validation Loss= -40.95539564603606\n",
            "Learning Rate 0.00025\n",
            "Epoch: 218 Training Loss= -39.99097719883033 Validation Loss= -40.952873663024086\n",
            "Learning Rate 0.00025\n",
            "Epoch: 219 Training Loss= -39.9911498790382 Validation Loss= -40.95452545042533\n",
            "Learning Rate 0.00025\n",
            "Epoch: 220 Training Loss= -39.98993257198298 Validation Loss= -40.954139091798005\n",
            "Learning Rate 0.00025\n",
            "Epoch: 221 Training Loss= -39.99084550580973 Validation Loss= -40.955151749537194\n",
            "Learning Rate 0.00025\n",
            "Epoch: 222 Training Loss= -39.99274147548611 Validation Loss= -40.95307276640575\n",
            "Learning Rate 0.00025\n",
            "Epoch: 223 Training Loss= -39.99177473191345 Validation Loss= -40.95617020982441\n",
            "Learning Rate 0.00025\n",
            "Epoch: 224 Training Loss= -39.992029342003754 Validation Loss= -40.95613182942916\n",
            "Learning Rate 0.00025\n",
            "Epoch: 225 Training Loss= -39.99324995529167 Validation Loss= -40.954047019027406\n",
            "Learning Rate 0.00025\n",
            "Epoch: 226 Training Loss= -39.98849777395587 Validation Loss= -40.95489882649714\n",
            "Learning Rate 0.00025\n",
            "Epoch: 227 Training Loss= -39.99482963993237 Validation Loss= -40.95499272652137\n",
            "Learning Rate 0.00025\n",
            "Epoch: 228 Training Loss= -39.99396351480244 Validation Loss= -40.95558895010525\n",
            "Learning Rate 0.00025\n",
            "Epoch: 229 Training Loss= -39.99355144883366 Validation Loss= -40.95606383418084\n",
            "Learning Rate 0.00025\n",
            "Epoch: 230 Training Loss= -39.99207353496759 Validation Loss= -40.95767369214194\n",
            "Learning Rate 0.00025\n",
            "Epoch: 231 Training Loss= -39.992111964682636 Validation Loss= -40.95829803697462\n",
            "Learning Rate 0.00025\n",
            "Epoch: 232 Training Loss= -39.99438855040037 Validation Loss= -40.95797272394384\n",
            "Learning Rate 0.00025\n",
            "Epoch: 233 Training Loss= -39.99512734025674 Validation Loss= -40.95513138719077\n",
            "Learning Rate 0.00025\n",
            "Epoch: 234 Training Loss= -39.988548226097905 Validation Loss= -40.95393894028312\n",
            "Learning Rate 0.00025\n",
            "Epoch: 235 Training Loss= -39.992186000283596 Validation Loss= -40.954247519505\n",
            "Learning Rate 0.00025\n",
            "Epoch: 236 Training Loss= -39.9940479303335 Validation Loss= -40.95657856276651\n",
            "Learning Rate 0.00025\n",
            "Epoch: 237 Training Loss= -39.99117438177191 Validation Loss= -40.95558134621659\n",
            "Learning Rate 0.00025\n",
            "Epoch: 238 Training Loss= -39.99307923370782 Validation Loss= -40.95588722372465\n",
            "Learning Rate 0.00025\n",
            "Epoch: 239 Training Loss= -39.99239898009533 Validation Loss= -40.95601306433235\n",
            "Learning Rate 0.00025\n",
            "Epoch: 240 Training Loss= -39.99516344161803 Validation Loss= -40.954266176955834\n",
            "Learning Rate 0.00025\n",
            "Epoch: 241 Training Loss= -39.993402470342126 Validation Loss= -40.95611119457979\n",
            "Learning Rate 0.00025\n",
            "Epoch: 242 Training Loss= -39.990019078876955 Validation Loss= -40.95376743557992\n",
            "Learning Rate 0.00025\n",
            "Epoch: 243 Training Loss= -39.99507154792251 Validation Loss= -40.95726624209636\n",
            "Learning Rate 0.00025\n",
            "Epoch: 244 Training Loss= -39.99210048452274 Validation Loss= -40.95810108269759\n",
            "Learning Rate 0.00025\n",
            "Epoch: 245 Training Loss= -39.99163890204662 Validation Loss= -40.957900090373364\n",
            "Learning Rate 0.00025\n",
            "Epoch: 246 Training Loss= -39.99297692113693 Validation Loss= -40.95666763403362\n",
            "Learning Rate 0.00025\n",
            "Epoch: 247 Training Loss= -39.99396667132585 Validation Loss= -40.956641596519326\n",
            "Learning Rate 0.00025\n",
            "Epoch: 248 Training Loss= -39.98983918171874 Validation Loss= -40.960915270895484\n",
            "Learning Rate 0.00025\n",
            "Epoch: 249 Training Loss= -39.99171426232506 Validation Loss= -40.95677554101787\n",
            "Learning Rate 0.00025\n",
            "Epoch: 250 Training Loss= -39.99063910028422 Validation Loss= -40.95599185938169\n",
            "Learning Rate 0.00025\n",
            "Epoch: 251 Training Loss= -39.99217403015716 Validation Loss= -40.960509610265355\n",
            "Learning Rate 0.00025\n",
            "Epoch: 252 Training Loss= -39.99394231982342 Validation Loss= -40.95739652866558\n",
            "Learning Rate 0.00025\n",
            "Epoch: 253 Training Loss= -39.99409094360723 Validation Loss= -40.95916484725216\n",
            "Learning Rate 0.00025\n",
            "Epoch: 254 Training Loss= -39.9937602284649 Validation Loss= -40.95667025984662\n",
            "Learning Rate 0.00025\n",
            "Epoch: 255 Training Loss= -39.9971565790621 Validation Loss= -40.959974009392816\n",
            "Learning Rate 0.00025\n",
            "Epoch: 256 Training Loss= -39.99093719170199 Validation Loss= -40.955093750296086\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-3ad8b34df0fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mF3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_sd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_sd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mF1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mF2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mF3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminimize_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;31m#print(sess.run(cost, feed_dict={X:h_train[j]}))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdisplay_step\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1363\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycukftHnr4pZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "outputId": "90ffe84d-a96d-476c-fe38-f96c9038f274"
      },
      "source": [
        "a = -np.array(cost_train_plt)\n",
        "b = -np.array(cost_test_plt)\n",
        "plt.subplot(223)\n",
        "plt.plot(a ,label= 'Training Average Rate')\n",
        "plt.plot(b, label= 'Testing Average Rate')\n",
        "plt.xlabel('Epocs')\n",
        "plt.ylabel('Avg Sum Rate')\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad = 0.)\n",
        "plt.show()\n",
        "# graph_folder = '/content/drive/MyDrive/DL_prejects/Graph_data'\n",
        "# savemat(graph_folder+'/M8_tau_learned_train_rate.mat',{'Train_rate': a})\n",
        "# savemat(graph_folder+'/M8_tau_learned_Valid_rate.mat',{'Valid_rate': b})"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWEAAACQCAYAAADZRtrfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXxU5fU/8M+ZPZNMhoSEbCQEwhICSWS1ILiwuYZvxQ2E0p+KKNaKUlFRWytq60KVL7UuuBT4yiZK1aJibQGx2lpBQIIksiWEkJWEmUky+5zfH3eCATPJJGSYDD3v1yuv3HvnLudOkpNnnnvveYiZIYQQIjxU4Q5ACCH+m0kSFkKIMJIkLIQQYSRJWAghwkiSsBBChJEkYSGECCNNuAMIRkJCAmdmZoY7DHGe2rlzZy0zJ7bxei+NRvM6gKGQhovoGB+AQo/HM2fEiBHVra0QEUk4MzMTO3bsCHcY4jxFRKVtva7RaF5PTk4enJiYWK9SqeTGehE0n89HNTU1OZWVla8DmNraOvJfXYj2DU1MTLRKAhYdpVKpODEx0QLlU1SrIqIlLAQzw+NjMANWhxuxBi00KsKxejviorWobXBBRYBaRdCpVUiI0aOmwYmiShsSY/QYkBQDrbrTbQ6VJGDRWf7fnYC/fJKERVCYGXa3F+X1dlRZnacSnt3thcmggcPtQ7XNgbpGN+wuD3wMqAjw+Bg2hwcerw9qlQqNTg/cXh/sbi+aXF7oNCpUWhzQqAnROg2a/NuebHKh0upAjygdPD5GbYPztHg0KoLHF3xezOxpxNb7LwURdfVbE3KVlZXqSy+9dBAA1NbWalUqFcfHx3sAYPfu3fsNBkPAN2L79u3GN998s+eKFSvK2jrGsGHDsnft2lXUVTHfeuut6Zs2bYqrqKj4Vq1Wd9Vuu1xxcbEuPz9/aGZmpsPtdlNeXl7junXrSvV6fcD3dNOmTSa9Xu+bPHlyY1fEIEn4v1xhuQWbCyvh9HhRZXWiyurAcYsdKiJY7e5TydTq8MDbgaTXkl6jgk6jAjMQrVdDp1HBoFEjSqeG0+1DstkAj893KtGqiDAkzYwJ2Umw2N1Q+fNmlE6NxBg9YgwaVFod8HgZybEGuH0+JETroVEridnS5Eb5STt6GLXI790DlVYHCIjIBAwAycnJ3qKiou8AYMGCBakxMTHexYsXVzW/7na7odVqW9324osvbrr44oub2jtGVyZgr9eLzZs390hJSXF99NFHpoKCAtvZ7M/n84GZEapknp6e7iwqKvrO4/Fg3LhxA9988824efPm1QVaf8uWLaaYmBivJGHRLmaG18coOdEEu8sLh8cLAnCi0YX6RhcOVjfg9X8eAaAkyqRYA5JjDRiWHgevjxGj10CrURJXlFYNtUqFRJMePaK0SDEb4PExGpweqFWEWIMWvWL1iDPqoCJAo1bB62UYdCqoiaDpfFeAaMV1112XqdfrfYWFhcbRo0c3zJw5s+6+++7LcDqdKoPB4FuxYsWR/Px856ZNm0x/+MMfkrZu3XpwwYIFqWVlZbrS0lL98ePHdXfeeWfVo48+Wg0ARqNxWFNT065NmzaZFi9enBofH+8uLi6Oys3NbXrvvfeOqFQqrF+/3vzQQw/1NhqNvlGjRjWUlpbqt27devDM2D788EPTgAED7Ndff339mjVr4gsKCmx33XVXWnp6umvRokU1wOn/TH79618n/eUvf4l3uVx09dVXn3zhhReOFxcX6y6//PKBw4YNa9i7d2/0Rx99dODxxx9P3rNnT7TD4VAVFBTUv/DCC8cBBIzLarWqbrvttoyioqIoj8dDjzzyyPFZs2adDPSeajQaDB8+vLG8vFwLAGvWrDE//fTTKW63WxUXF+dZv3794aamJtWqVasSVSoVv/322z2XLl16NC8vz3HLLbf0KS8v1wHA888/f3TKlClBJ+iQJ2EiUgPYAaCcma8hotUARgJwA/gPgDuY2R3qOCIBM8Ph9sHl9eFIbSNqbU44/C3UhBgdahtciNKqERulwbbiGkTr1DAbdSipbUT5STsanR7UN7lQ2+CCjxlalQour6/NY+b1NuP12SORaNJHbEvxXFr4zp707yttxq7c58BkU9Nz1+e32V3QmoqKCt0333xTpNFoUFdXp/r666+LtFot3nvvPdMDDzzQ+5NPPjl05jYHDx40fPnll8UnT55UDx48eOjChQtrzvzovX///qjdu3cfzszMdI8YMSL7008/jRk/fnzj/Pnz+2zbtq0oOzvbVVBQ0DdQXGvWrIm/8cYb62bMmHHyiSeeSHM6nTRz5sy6e++9N6M5Cb///vtxn3zyyfcbN26MPXjwoOHbb7/dz8yYNGlS/48//jimX79+rqNHj+rfeOONIxMnTiwBgOeff748KSnJ6/F4MHbs2EFfffVVVG5uriNQXA8//HDKZZddZt2wYUNJbW2teuTIkYOnTp1qjY2NbfWPoqmpiXbu3Bm9bNmyMgCYPHlyw/Tp04tUKhWef/75hMWLFye/9tprx2bPnl3T8tNIQUFB3wULFlRdfvnlDQcOHNBdfvnlAw4fPrwv2J/juWgJzwewH0Csf341gFn+6TUA5gB4+RzE0aUanB6ln9Tlhc3pQVGFDWoVwceMpFg9/n24DqUnGqFRq3CiwQm3l9HDqFxAqrI44PH50OD0wO7yomeMHha7GzaHG25vxz7yqwhI7REFg1aNKK0aiSY94qP1SDUbkGjSI61HFFQqQny0DkadGjaHBzmpsUiI1iPZbIBWTZJ8I9S0adPqNRrlT7iurk5900039S0pKTEQEbvd7lZ/qFOmTDkZFRXFUVFRnvj4ePexY8c0WVlZpzWCcnNzG5uXDRkypOnQoUM6k8nkTU9Pd2ZnZ7sAYPr06XWvv/76j+6tdjgctGXLFvPLL79cFhcX57vgggsaN27cGDtjxgzLiRMnNCUlJdqKigqN2Wz29u/f3/3ss88mbd++PTYnJycHAJqamlRFRUWGfv36uVJSUlwTJ0481aJcuXJl/IoVKxI8Hg/V1NRo9+zZY/B6vQgU17Zt22I/+eSTHsuWLUsGAKfTSQcPHtQNHz7c0TLmsrIyfXZ2dk55ebnu0ksvtVx44YV2ADhy5Ijupz/9ae+amhqty+VSpaenn35hwu+LL76IPXDgQFTzfENDg9pisajMZnPbLSC/kCZhIuoN4GoATwFYAADM/FGL1/8DoHcoYwiWx+uD1eFBUaUV+yts0KkJTo8PXh/D7fVhW3ENKiwODE4x4WhdE76vaghqv/HROtQ1Ki3YnjE6mAxa9OlphFajfEw36tSwOT0wR2lhjtIi1qCF0+NFao8oDOgVA5vDA6fHh4x4IxiMfx86gZN2N64dloa0HlHwMaDTyEf9c6UzLdZQiYmJOfVH/uCDD6Zdcskltk8//fRQcXGxbsKECYNa26Zlq1etVsPj8fwoWQezTiAbN26Mtdls6qFDhw4BALvdrjIYDL4ZM2ZYpk6dWv/WW2/FVVZWaqdNm1YHKJ/+7r333oqFCxfWttxPcXGxzmg0njq/oqIi3Ysvvpi0c+fO/YmJid7rrrsu0+FwtPmLz8x45513Dubn57eaPJs19wlXVFRoxowZk7169WrzzJkzLXfffXfG/PnzK2fOnGlp7qYJdJxvvvlmv9Fo7NRFk1C3hJcCeACA6cwXiEgL4GdQWsrnXJXVgY/2VuBIbSMsdjd2HT2Jo3WBr1+k9YhCitmAbcU1SIo14BeXZaHW5kJ2inJqSbEGmAwa1De5UVbXhGuHpcHrY6THG+HzMVxeHwzas7+wkJ0c2/5K4r+O1WpV9+7d2wUAr776akJX7z8vL89RVlamLy4u1g0aNMi1fv36+NbWW7t2bfzSpUtL77jjjjp/XKrMzMxcm82mmjVrVt3tt9+eWV9fr/nss8+KAeDKK6+0/va3v02dO3dundls9h05ckSr0+l+lMzq6+vVUVFRvvj4eG9ZWZlm27Zt5ksuucTWVlyXXXaZ9Q9/+EPSihUrjqpUKnzxxRdRF110kT3QOaakpHgWL1587LnnnkuZOXOmxWazqTMyMtwAsGLFip7N65lMJq/Vaj31xzxu3Djr73//+15PPPFEFQB8+eWXUWPHjg14nDOFLAkT0TUAqpl5JxFd2soqLwHYzsyfB9h+LoC5AJCRkXHW8Via3Pi6pA7v7DyGsvom7K+wovliv8mgwYBeMRiYFIMrhqZgbFZPlNQ2wurw4MK+8TBo1TBoVSAiMHOHP76rVASDqvvepiMi34MPPlg5Z86cvs8880zq5MmTA1586qyYmBh+/vnnS6+44ooBRqPRl5+f/6MLTzabTbV9+3bzypUrTz2BGBsb6xs5cmTDunXrzLfffnt9Y2OjKikpydWnTx83AEybNs26b98+w6hRo7IBwGg0+lavXn1Eo9GclojHjBljHzp0aFNWVtbQlJQU14gRIxrai+vpp58+Pnfu3Izs7Owcn89H6enpztYuJLY0a9ask0899VTq5s2bYx555JHjM2bMyDKbzZ5x48bZjh49qgeA66677uT111+f9fHHH/dYunTp0eXLl5fNmTMnY+DAgTler5cuvPBC29ixY48G+95SqIY3IqLfQ2npegAYoPQJb2TmWUT0GIBhAKYxc7v9JiNHjuTOPrZssbvxzs5jWPrp97A5PYjWqTG8TxxG9InD1bkp0GvU6BWr75JWqohMRLSTmUcGen3Pnj0l+fn5tYFe/2/R3M/p8/kwe/bsjAEDBjgee+yxVushSFyn27NnT0J+fn5ma6+FrCXMzIsALAIAf0v4fn8CngPgcgATg0nAZ6OkthEzX/8K5SftGD8gAbN+0gcj+8ShZ4w+lIcV4ry0dOnShLVr1ya43W4aMmRI04IFC7rFP6buGlewQtYSPu0gPyTha4jIA6AUQPMN3BuZeXFb23emJVxYbsH1r3wJvUaN/51+AS4ZmCh3AYhWSUtYhNpZt4SJKApABjMXdyYAZt4GYJt/OuS3xdU1unDX6m+g16ix8a6xyEqMCfUhhRCiU9q9t4mICgDsBrDZP38BEX0Q6sDOxvLth1FW34RXZo2QBCyE6NaCucH0twBGAzgJAMy8G0DAp2XCra7RhVX/KkFBXirGZPVsd30hhAinYJKwm5ktZyzrtmX9VnxxBHa3F3dP6B/uUIQQol3BJOF9RHQzADURDSCiPwL4MsRxdQozY+Ouclw8IBEDk370fIgQEamyslKdnZ2dk52dnZOQkJDfq1evvOZ5h8PR7tXmTZs2mT799NPo5vlnn3028cUXX+yyj4kVFRUajUYz/Nlnnw04RFR3cd1112WmpaXlZmdn5wwaNCjn/fffbzdRPPTQQ8mhjCmYJPxLAEMAOKHUerAgTE+5tedQTQOO1dtx5dCQvmdCnFPNpSyLioq+mz17ds2dd95Z1TzfVi3hZlu2bDF9/vnnpy6OPPDAAzV33333ia6Kb9WqVXH5+fmNGzZsaPUpuo5yu0Nbz+vJJ588VlRU9N2SJUvK7rnnnj7trb9s2bKUUMYTTBK+mpkfYeZR/q9HEWCspHArLLcCAIZlxIU5EiFC6/PPPzeOGjVq0JAhQwaPGzduQGlpqRYAnnzyyV5ZWVlDBg4cmHPNNdf0Ky4u1q1atSrxlVdeScrOzs7ZvHlzzIIFC1J/85vfJAHA6NGjB82bNy8tNzd3cGZm5tDNmzfHAMrTb1dddVW/rKysIZMnT87Ky8vL3r59e6vV4zZs2BC/ZMmSsqqqKu2hQ4e0J06cUKempuZ6vV4AyqPLycnJeU6nk/bt26cfP378gCFDhgweMWLEoF27dhkApYV68803Z+Tl5WXPmzev99atW40XXHBB9uDBg3OGDRuWvWfPHn17cW3cuDH2ggsuyM7JyRl85ZVX9rNYLG3mt4kTJzZUV1efKsQ8adKkrCFDhgzu37//kCVLliQAwF133ZXmdDpV2dnZOVOnTu0LAC+99FJ8bm7u4Ozs7Jybb765j8fjOaufZTC3iy0CsCGIZWFXWG6BXqNCVmJ0+ysL0Rnv/SId1d91aSlL9Mppwk//FHRhIGbGPffck/Hhhx8eTE1N9bz22mtx999/f9qGDRtKli1bllxaWro3KiqKa2tr1QkJCd4zSy/+7W9/O60Aicfjob179+5fv369efHixalXXHHF988991xijx49vIcOHdr39ddfG8aMGTOktVgOHjyoramp0V522WVNU6dOrV+1alX8448/XjV48OCm5oLu69evN19yySUWvV7Pc+bM6bN8+fLS3Nxc55YtW6LnzZuX8e9///t7ILjSnIHiqqio0Pzud79L2b59+/exsbG+Rx55JPmJJ55IWrJkSUWg9/Hdd981T5o06dQj3qtXry5JSkryNjQ00LBhw3JmzZpV/9JLL5WvWLGiV3NR/W+++cbwzjvvxO/YsaNIr9fzrFmzMl555ZWeZ/PJImASJqIrAVwFII2IlrV4KRbKo8jdzuHaRvRLjJEC4l3F4wTUOsDrAprqAFMy0FAFuJsArxswpQCOk8q02w6wF3A1AV4noItR1nNYAbUWiIoHPHZlPVIDWgPgagSq9wPsA2LTlNdVGmW5Sg14PUBUHGCvB0gFECn7Yh+gMQA+jxIjkRKDNgrK8B0JgN6kxF6xBzD2BDR6IOd/wv2Odgmn06k6cOBA1IQJEwYCysgTiYmJbgAYNGiQ/dprr+07derUkzNnzgyqhsQNN9xQDwBjx45tXLhwoQ4Avvzyy5j58+dXA8CoUaMcAwcObLW61apVq+KnTp1aDwA/+9nP6m677bbMxx9/vOqGG26oX7t2bVxBQYHt7bffjr/rrrtqLBaLateuXTE33HBDVvP2LpfrVJ92MKU5A8W1bdu26EOHDhlGjx6dDQBut5ua60uc6dFHH+39+OOPp1VVVWm3bNlyakSRZ555JunDDz/sAQCVlZXaffv2GZKTk0+rkbF582ZTYWGhMT8/fzAAOBwOVa9evc4qH7bVEj4OpRj7VAA7Wyy3AbjvbA4aKkfrmtBf7gsOntsBfPoboPQLJcn6PEpydPuTpatRSXogJbE2J+RIFJ/VNUm4Ay3WUGFm9O/f37579+4fDUm0devWAx9//LHp/fffNy9ZsiSluLi43eLizf3KGo0GXq+3Q4+Vvvvuu/E1NTXajRs3xgNAdXW1du/evfrmgu5VVVXqwsJCY0FBgdVqtapMJpOnuVV5ps6U5mzGzBg3bpz1r3/965H2Yn7yySeP3XLLLfVPPfVUrzlz5mTu27dv/6ZNm0yfffaZaceOHUUmk8k3evToQXa7/UetOWamG2644cSf/vSn8vbfneAETMLMvAfAHiJaEwkjX/h8jLK6JkzI7hXuUH6MWWmtAYDHpSS7usNAT3+DQK0HrMcAyzGl1WdKBhwWwFIOWMoAc29AawSO/gvolQP0yACq9iktwugEoOo7oHKvst/oBKDpBNBYCxjMgDkNiO0N2I4rrUK9CYjLVF4/vhtwNwJ9LwESBgAqrdIC1UYpxwOAhmrAlATozYDTCpjTlX2AAVulcjyVRmmpqnXK9+hEwOl/Kt3YU2khO6zKftX+LjhXA6CJAhIHKe9P3WHlOM3vj84IaKOVlrfJf12EfUq8pFLW8XqUaV208t1ep7SQHSeV98/tAHr2BzwOICbpXP20Q06v1/vq6uo0f//736MnTZrU6HQ6ae/evfphw4Y5Dh06pCsoKLBNmTKlIT09Pd5isajPLL0YjDFjxjSsW7curqCgwLZz507D999/H3XmOt9++62+sbFRXV1d/W3zsvvuuy915cqV8UuWLKnIy8trvOOOOzImTpxo0Wg0iI+P9/Xu3dv15ptvxt166631Pp8PX331VdSYMWN+VPYxUGnOQHFdeumljb/61a8yCgsL9UOHDnVarVZVSUmJNi8vL2At4UWLFlW/9dZbCe+++26s3W5Xmc1mr8lk8u3atcuwZ8+eU32aGo2GnU4n6fV6vuKKK6zTpk3r//DDD1elpaV5qqqq1BaLRT1w4MBOt06C6RPO9FdEy4FSDQ0AwMz9OnvQUKhtcMLp8SE97ke/K13D41IShy4GaKoF6kuB+hKgsUZJdkf/BcSmKn/40T2Bsq+VpND8kVutA+L7Kdu4z6gCGGwLM9B6GoOSyGN6AY3VSkIzJiiJ/8hhJQaNAUjOVZL9wS1AQn+g73hg+M+B7Ku64h06OzEB7m760fI27oIynT+Jti0qlQrr1q07dM8992TYbDa11+ulefPmVeXm5jpvvvnmvjabTc3MNGfOnOqEhATvmaUXgznGwoULa2688cbMrKysIVlZWY7+/fs74uLivC3XWblyZfxVV11V33LZ9OnT62fMmNFvyZIlFTfeeGP9rbfe2m/Tpk2nyh2sXbv28O23397nmWeeSfF4PHTttdfWtZaEA5XmDBRXamqq59VXXy2ZPn16v+Yujscee6y8rSSsUqnw4IMPHl+yZEnyli1bDixfvjyxX79+Q/r16+doWRJz5syZNYMHD84ZOnRo0wcffHDk0UcfLZ84ceJAn88HrVbLy5YtO3o2SbjdAj5E9E8AjwF4AUABgFsAqJj5N509aEcFU8Bn7zELCl78J16bPRKTczrxx1jyTyWxHvuPksDqjwA1xUriNacDFd8CzjOfWWlBa1T6QJslDFRatD6fkhh75SgtVF2M0pJMyf+h79NxEojrq7QUo3spiVJrVLYxmJX1HCeBlAv8cR4B0oYr/ahNdUBiNqBq8cnJYVFamRqd0spsJgWMWiUFfH7M4/HA5XKR0Wjkffv26adMmTLw0KFDhcHcEvffGFd7zraATxQz/4OIiJlLAfyWiHYCOGdJOBjVNmXYqERTEGUqm+qAw9uA/X9Vkuyxr5VugNZkjgdOHgUGFwBJOf6LUp4fLiwNulL5GJ805IdWsatJaQ2HwoBJp8+bWrkn2mD+YVoSr+gEm82mGj9+/CC3203MjBdeeKG0OyS67hrX2QgmCTuJSAXgABHdDaAcQLe7+lVtUz519GotCbsdwM4/A0UfAiUtBvIwpSj9mX0uUpJt1mVKi9Rer7Q8rceVZcFqTojaEHWJCHGOxMXF+QoLC/eHO44zdde4zkYwSXg+ACOAewA8AWACgNmhDKozavxJOOHMgu0lXwBfLAUO/A0AKRdwLl4IpA4HBkxWugPOZEpSvhLbvCArhBBnrd0kzMxf+ycbANxCRGoA0wF8FcrAOqra5kCcUXv6yMOHPwPWTgd8XmDM3cCEXystX3XISxqL84vP5/ORSqWK6I+9Ijx8Ph8BCDiKUMCnGogologWEdGLRDSFFHcDOAjgxhDEelZqbM7T+4Mtx4D1s5Q7FubvBi5/SunHlQQsOq6wpqbG7P9jEiJoPp+PampqzAAKA63TVkb6PwD1AP4FYA6AhwEQgGv9NYW7lWqbE71M/jvoGmqA/7tWaQHPfEdJxEJ0ksfjmVNZWfl6ZWXlUARXb0WIZj4AhR6PZ06gFdpKwv2YORcAiOh1ABVQhjhydG2MXaPa6sSFff33V//9t8r9uDevB+K7bf15ESFGjBhRjW5atEpEvrb+q596So6ZvQCOddcEzMyoafB3RzTVAXs3AMNnA1kTwh2aEEK0qa2WcD4RWf3TBCDKP08AmJljA296blntHrg8PiUJ716j1DkYeWu4wxJCiHa1VTuiQ8+ah1NNg/9BjRgd8M8/A+kXKg9PCCFEN3deXGSotir3CGd6S4ETB4Fhs8IckRBCBOf8SML+BzWS7QeUBb1HhzEaIYQI3nmRhJufluthK1aqifWUkZaFEJEh6CcXiCi25frMXBeSiDqh2uaAQauCznJEqdErD2QIISJEu9mKiO4A8DgAB4DmxzYZQLepJ9z8tBxZypUC6EIIESGC6Y64H8BQZs5k5r7+r6ATMBGpiWgXEW3yz99NRAeJiIkoob3tg3HqaTnLMUnCQoiIEkwSPgSg1UH+gjQfQMvSc18AmASg9Cz2eZpqmxOpRlZGsohN66rdCiFEyAU75P2XRPQVgFNDhTDzPe1tSES9AVwN4CkAC/zb7fK/1pl4W1Vjc6IgzT+wqjm9y/YrhBChFkwSfhXAFgB70UY5tgCWAngAgKmD2wXN6fHCYnejj9o/MoZZWsJCiMgRTBLWMvOCju6YiK4BUM3MO4no0k5sPxfAXADIyMgIuJ6lSSlxkQT/EGDSJyyEiCDB9Al/TERziSiFiOKbv4LY7iIAU4moBMA6ABOI6K1gA2Pm5cw8kplHJiYGHmHX5vQAAOI81QAIMEnZSiFE5AimJTzD/31Ri2Xt3qLGzIuat/G3hO9n5i5/ntjmUJJwrKtKGfJdo+vqQwghRMgEM7xRlxbkJaJ7oPQTJwP4log+YuaABY/b0+BPwkZXHRDTiaHuhRAijIJ5WKPVQT2ZeVWwB2HmbQC2+aeXAVgW7LbtsTmUPmG9sxYw9+qq3QohxDkRTHfEqBbTBgATAXwDIOgkHErNfcJaey2QlhvmaIQQomOC6Y74Zct5IuoB5UJbt2BzeEDwQd1Uo/QJCyFEBOlMFbVGAN1m4LYGhwdmNIJ8bukTFkJEnGD6hP+KHwr3qAEMBvB2KIPqiAanG2lamzITHfhWNiGE6I6C6RNe0mLaA6CUmY+FKJ4Os7u9SNfalGf5pCUshIgwwfQJfwYARNQTwMVQSlp2nyTs8iFZbZUkLISISAH7hIloExEN9U+nACgEcCuA/yOie89RfO1yuL1IUvkHhY6R7gghRGRp68JcX2Yu9E/fAuBTZi4AcCGUZNwt2N1eJNJJQK0DDD3CHY4QQnRIW0nY3WJ6IoCPAICZbeh4NbWQsbu8SIRF6YrowvKYQghxLrTVJ1xGRL+E0v87HMBmACCiKADacxBbUOxuL+JRL/cICyEiUlst4dsADAHw/wDcxMwn/ct/AuDPIY4raA63Fz18J4FoScJCiMgTsCXMzNUA7mxl+VYAW0MZVEfY3V6YvXXSEhZCRKTOPDHXrTidLsR4LXJ7mhAiIkV8EtZ5rFDBB0R3ycDNQghxTkV8Eta6/Y8s62PDG4gQQnRCMLUjWqv9awGwg5nf7/qQgufx+mDwNSkzBknCQojIE0xL2NRHQBAAAAe9SURBVADgAgAH/F95AHoDuI2IloYwtnY1Or2IJX8SlpawECICBVPAJw/ARczsBQAiehnA5wDGAdgbwtjaZbG7YUJzEjaFMxQhhOiUYFrCcQBiWsxHA4j3J2VnSKIKktXhhgl2ZUa6I4QQESiYlvCzAHYT0TYABKWS2u+IKBrA30MYW7ssdjdMp7ojzOEMRQghOiWYUpZvENFHAEb7Fz3MzMf90wtDFlkQrHY3YppbwtIdIYSIQMGOrLEGwAfM3Bj6kIJnsbsRQ3aw2gDS6MIdjhBCdFgwfcJLAIwH8B0RvUNE1xORIcRxBcXqcMMIJ6AzhjsUIYTolHaTMDN/xsx3AegH4FUANwKoDnVg7bHY3fjLruOIVrkBbVS4wxFCiE4J5sJcc/nKAgA3QSlruTKUQQVDqyYYdWpcmB4FckgSFkJEpnZbwkT0NoD9ACYAeBFAFjP/MtSBtceo02DDHWPQO4YAjSRhIURkCqYl/AaAGS0e1hhHRDOY+RehDa19KhUBbrt0RwghIlYwfcKfAMgjomeJqATAEwCKgj0AEamJaBcRbfLP9yWir4joIBGtJ6Kzu63BbQe03eI6oRBCdFhboy0PJKLHiKgIwB8BlAEgZr6Mmf/YgWPMh9Kd0ewZAC8wc38A9VBG8Og8j126I4QQEautlnARlH7ga5h5nD/xejuycyLqDeBqAK/758m/z3f8q6wE8NOOBn0a6Y4QQkSwtpLwNAAVALYS0WtENBHKY8sdsRTAA/hhdOaeAE4ys8c/fwxAWgf3eTq3Q5KwECJiBUzCzPweM08HkA1lTLl7AfQiopeJaEp7OyaiawBUM/POzgRGRHOJaAcR7aipqQm8okdawkKIyBXMhblGZl7DzAVQ6gjvAvBgEPu+CMBU/8W8dVC6If4XQA8iar4rozeA8gDHXc7MI5l5ZGJiYuCjuKVPWAgRuTo0vBEz1/uT48Qg1l3EzL2ZORPAdABbmHkmlFb19f7Vfg7g7EbnkLsjhBARLBxjzD0IYAERHYTSR/xGp/fkdQPslZawECJiBfXY8tli5m0AtvmnD+OHsphnx2FVvktBdyFEhIrs0Zbt9cr3qLjwxiGEEJ0kSVgIIcJIkrAQQoSRJGEhhAgjScJCCBFGkZuEmYGyr5Rpg4y0LISITJGbhG2VwKEtwMhbAZU63NEIIUSnnJP7hEMiNgWYuw2IywxzIEII0XmRm4QBIL5vuCMQQoizErndEUIIcR6QJCyEEGFEzBzuGNpFRDUASgO8nACg9hyG09UiPX4g8s+hDzO3US9ViNCJiCTcFiLawcwjwx1HZ0V6/MD5cQ5ChIt0RwghRBhJEhZCiDA6H5Lw8nAHcJYiPX7g/DgHIcIi4vuEhRAikp0PLWEhhIhYEZuEiegKIiomooNE9FC44wmEiN4komoiKmyxLJ6IPiWiA/7vcf7lRETL/Of0LREND1/kp2JNJ6KtRPQdEe0jovn+5RFzDkJ0ZxGZhIlIDeBPAK4EkANgBhHlhDeqgFYAuOKMZQ8B+AczDwDwD/88oJzPAP/XXAAvn6MY2+IB8CtmzgHwEwC/8L/XkXQOQnRbEZmEoQwUepCZDzOzC8A6AP8T5phaxczbAdSdsfh/AKz0T68E8NMWy1ex4t8AehBRyrmJtHXMXMHM3/inbQD2A0hDBJ2DEN1ZpCbhNABlLeaP+ZdFiiRmrvBPVwJI8k936/MiokwAwwB8hQg9ByG6m0hNwucNVm5P6fa3qBBRDIB3AdzLzNaWr0XKOQjRHUVqEi4HkN5ivrd/WaSoav6I7v9e7V/eLc+LiLRQEvBqZt7oXxxR5yBEdxWpSfhrAAOIqC8R6QBMB/BBmGPqiA8A/Nw//XMA77dYPtt/h8FPAFhafOQPCyIiAG8A2M/Mz7d4KWLOQYjuLGIf1iCiqwAsBaAG8CYzPxXmkFpFRGsBXAql0lgVgMcAvAfgbQAZUKrD3cjMdf6E9yKUuymaANzCzDvCEXczIhoH4HMAewH4/IsfhtIvHBHnIER3FrFJWAghzgeR2h0hhBDnBUnCQggRRpKEhRAijCQJCyFEGEkSFkKIMNKEOwARGBF5odwa1mwdMz8drniEEF1PblHrxoiogZljwh2HECJ0pDsiAhFRCRE9S0R7ieg/RNTfvzyTiLb46/j+g4gy/MuTiOgvRLTH/zWWiKKJ6EP/fCER3RTesxLiv5Mk4e4tioh2t/hqmSgtzJwL5em0pf5lfwSwkpnzAKwGsMy/fBmAz5g5H8BwAPugPNF2nJnzmXkogM3n4oSEEKeT7ohuLFB3BBGVAJjAzIf9xXUqmbknEdUCSGFmt395BTMnEFENgN7M7Gyxj4EA/gZgPYBNzPz5OTkpIcRppCUcuTjAdHAbM38PpVW8F8CTRPSbrgpMCBE8ScKR66YW3//ln/4SSkU5AJgJpfAOoAw/NA9QhoYiIjMRpQJoYua3ADwHJSELIc4x6Y7oxlq5RW0zMz/k745YD2U8NyeAGcx8kIj6APgzlIptNVAqmB0loiQAywH0A+CFkpBjoSRfHwA3gHlS7UyIc0+ScATyJ+GRzFwb7liEEGdHuiOEECKMpCUshBBhJC1hIYQII0nCQggRRpKEhRAijCQJCyFEGEkSFkKIMJIkLIQQYfT/AaVE7670kuBPAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDqiGdrCoWEB"
      },
      "source": [
        "## Save Trained Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFpf9xkszdYT"
      },
      "source": [
        "trained_param_folder = '/content/drive/MyDrive/Trainedweights'\n",
        "Save_train_param = True \n",
        "if Save_train_param== True:\n",
        "  savemat(trained_param_folder+'/W1.mat',{'W1': sess.run(W1)})\n",
        "  savemat(trained_param_folder+'/W2.mat',{'W2': sess.run(W2)})\n",
        "  savemat(trained_param_folder+'/W3.mat',{'W3': sess.run(W3)})\n",
        "  savemat(trained_param_folder+'/W4.mat',{'W4': sess.run(W4)})\n",
        "  savemat(trained_param_folder+'/W5.mat',{'W5': sess.run(W5)})\n",
        "  # savemat(trained_param_folder+'/W6.mat',{'W6': sess.run(W6)})\n",
        "\n",
        "  savemat(trained_param_folder+'/b1.mat',{'b1': sess.run(b1)})\n",
        "  savemat(trained_param_folder+'/b2.mat',{'b2': sess.run(b2)})\n",
        "  savemat(trained_param_folder+'/b3.mat',{'b3': sess.run(b3)})\n",
        "  savemat(trained_param_folder+'/b4.mat',{'b4': sess.run(b4)})\n",
        "  savemat(trained_param_folder+'/b5.mat',{'b5': sess.run(b5)})\n",
        "  # savemat(trained_param_folder+'/b6.mat',{'b6': sess.run(b6)})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "j997S9ZwwY1F",
        "outputId": "5bdffc45-81e0-48d8-a63f-5d38c6535290"
      },
      "source": [
        "str(M)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'4'"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHpnsoRSz_ME"
      },
      "source": [
        "## Load Trained Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usUG4lgsnNeS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c576cb1-d456-4293-85a9-06b4bf216ec9"
      },
      "source": [
        "mini_batch_size = 10**3\n",
        "np.random.seed(112)\n",
        "\n",
        "h_bs_test = (np.random.randn( M,mini_batch_size)+1j*np.random.randn( M,mini_batch_size))*np.sqrt(b_bs/2)\n",
        "h_is_test = (np.random.randn( 1,mini_batch_size)+1j*np.random.randn( 1,mini_batch_size))*np.sqrt(b_is/2)\n",
        "h_id_test = (np.random.randn( 1,mini_batch_size)+1j*np.random.randn( 1,mini_batch_size))*np.sqrt(b_id/2)\n",
        "h_sd_test = (np.random.randn( 1,mini_batch_size)+1j*np.random.randn( 1,mini_batch_size))*np.sqrt(b_sd/2)\n",
        "g_rs_test = (np.random.randn( N,mini_batch_size)+1j*np.random.randn( N,mini_batch_size))*np.sqrt(b_rs/2)\n",
        "g_sr_test = (np.random.randn( N,mini_batch_size)+1j*np.random.randn( N,mini_batch_size))*np.sqrt(b_rd/2)\n",
        "g_rd_test = (np.random.randn( N,mini_batch_size)+1j*np.random.randn( N,mini_batch_size))*np.sqrt(b_rd/2)\n",
        "g_ir_test = (np.random.randn( N,mini_batch_size)+1j*np.random.randn( N,mini_batch_size))*np.sqrt(b_ir/2)\n",
        "G_br_test = (np.random.randn( N,M*mini_batch_size)+1j*np.random.randn( N,M*mini_batch_size))*np.sqrt(b_br/2)\n",
        "\n",
        "tic = time.time()\n",
        "g_rs1 =  np.repeat(g_rs_test,M,axis = 1)\n",
        "F = np.reshape(G_br_test*g_rs1, [N*M,mini_batch_size])\n",
        "F = np.concatenate([np.real(F),np.imag(F)],axis=0)\n",
        "F1 = np.concatenate([np.real(h_bs_test),np.imag(h_bs_test)],axis=0)\n",
        "F2 = g_rd_test*g_rs_test\n",
        "F2 = np.concatenate([np.real(F2),np.imag(F2)],axis=0)\n",
        "F3 = np.concatenate([np.real(h_sd_test),np.imag(h_sd_test)],axis=0)\n",
        "F = np.concatenate([F,F1,F2,F3],axis=0)\n",
        "\n",
        "Cost = sess.run(cost, feed_dict={X:F})\n",
        "toc = time.time() \n",
        "print(toc-tic)\n",
        "print(Cost)\n",
        "Cost = sess.run(tau, feed_dict={X:F})\n",
        "np.mean(Cost)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.007581472396850586\n",
            "-40.986636607056\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.2694898745044253"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lKZtg7Yz5M3z",
        "outputId": "626a9173-d090-474f-dc62-86f20183b088"
      },
      "source": [
        "mini_batch_size = 10**0\n",
        "\n",
        "t = np.zeros((1000,1))\n",
        "for i in range(1000):\n",
        "    mini_batch_size = 10**0\n",
        "    h_bs_test = (np.random.randn( M,mini_batch_size)+1j*np.random.randn( M,mini_batch_size))*np.sqrt(b_bs/2)\n",
        "    h_is_test = (np.random.randn( 1,mini_batch_size)+1j*np.random.randn( 1,mini_batch_size))*np.sqrt(b_is/2)\n",
        "    h_id_test = (np.random.randn( 1,mini_batch_size)+1j*np.random.randn( 1,mini_batch_size))*np.sqrt(b_id/2)\n",
        "    h_sd_test = (np.random.randn( 1,mini_batch_size)+1j*np.random.randn( 1,mini_batch_size))*np.sqrt(b_sd/2)\n",
        "    g_rs_test = (np.random.randn( N,mini_batch_size)+1j*np.random.randn( N,mini_batch_size))*np.sqrt(b_rs/2)\n",
        "    g_sr_test = (np.random.randn( N,mini_batch_size)+1j*np.random.randn( N,mini_batch_size))*np.sqrt(b_rd/2)\n",
        "    g_rd_test = (np.random.randn( N,mini_batch_size)+1j*np.random.randn( N,mini_batch_size))*np.sqrt(b_rd/2)\n",
        "    g_ir_test = (np.random.randn( N,mini_batch_size)+1j*np.random.randn( N,mini_batch_size))*np.sqrt(b_ir/2)\n",
        "    G_br_test = (np.random.randn( N,M*mini_batch_size)+1j*np.random.randn( N,M*mini_batch_size))*np.sqrt(b_br/2)\n",
        "    \n",
        "    tic = time.time()\n",
        "    g_rs1 =  np.repeat(g_rs_test,M,axis = 1)\n",
        "    F = np.reshape(G_br_test*g_rs1, [N*M,mini_batch_size])\n",
        "    F = np.concatenate([np.real(F),np.imag(F)],axis=0)\n",
        "    F1 = np.concatenate([np.real(h_bs_test),np.imag(h_bs_test)],axis=0)\n",
        "    F2 = g_rd_test*g_rs_test\n",
        "    F2 = np.concatenate([np.real(F2),np.imag(F2)],axis=0)\n",
        "    F3 = np.concatenate([np.real(h_sd_test),np.imag(h_sd_test)],axis=0)\n",
        "    F = np.concatenate([F,F1,F2,F3],axis=0)\n",
        "\n",
        "    Cost = sess.run(cost, feed_dict={X:F})\n",
        "    toc = time.time() \n",
        "    t[i] = toc-tic\n",
        "print(np.mean(t))\n",
        "print(Cost)\n",
        "Cost = sess.run(tau, feed_dict={X:F})\n",
        "np.mean(Cost)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0020459666252136233\n",
            "-0.757640044401787\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5700643924910194"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    }
  ]
}