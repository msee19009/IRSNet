{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l5-5BsZsakdS",
    "outputId": "aeace05d-7eb2-4b2a-b873-5bc976f824d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow 1.x selected.\n"
     ]
    }
   ],
   "source": [
    "%tensorflow_version 1.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eKX-tsjsHifj"
   },
   "source": [
    "# New Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "EH-7KJkoB2JL"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import savemat\n",
    "from scipy.io import loadmat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KS8UdvJvHzqD",
    "outputId": "4e988764-2f74-4d5c-8b0c-17949e1db105"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive \n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "FOM5bs4qApNS"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "tf.set_random_seed(1)\n",
    "np.random.seed(1)\n",
    "num_epochs=200\n",
    "display_step=1\n",
    "var=1\n",
    "mean=0\n",
    "beta=0.0 # This is the L2-Regularization parameter\n",
    "mini_batch_size=3*10**3  # Number of examples for training\n",
    "num_batches = 4*10**2\n",
    "m_test=10**4   # Number of examples for testing\n",
    "num_test_batch = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "DVfo3zX_YQGn"
   },
   "outputs": [],
   "source": [
    "eta = 1\n",
    "P_b = 10 #Watt or 40 dBm\n",
    "P_i = 0 #Watt or 40 dBm\n",
    "sigma_n = np.sqrt(3.98*10**-14)\n",
    "M = 2    #Power Beam\n",
    "N = 16   #of IRS elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aDTiIEfVA1Fb",
    "outputId": "3ca9c8fa-a391-4e53-9af2-695383796a35"
   },
   "outputs": [],
   "source": [
    "alpha = 2.57\n",
    "alpha_irs = 2.2;\n",
    "Lc = 10**-3;\n",
    "\n",
    "d_br = 20;\n",
    "d_rs = 15;\n",
    "d_sb = 25;\n",
    "d_is = 15;\n",
    "d_id = 30\n",
    "d_rd = 15;\n",
    "d_sd = 25;\n",
    "d_ir = 15;\n",
    "\n",
    "b_bs = Lc*d_sb**(-alpha);\n",
    "b_is = Lc*d_is**(-alpha);\n",
    "b_id = Lc*d_id**(-alpha);\n",
    "b_sd = Lc*d_sd**(-alpha_irs);\n",
    "b_rs = Lc*d_rs**(-alpha_irs);\n",
    "b_rd = Lc*d_rd**(-alpha_irs);\n",
    "b_ir = Lc*d_ir**(-alpha_irs);\n",
    "b_br = Lc*d_br**(-alpha_irs);\n",
    "\n",
    "np.random.seed(11)\n",
    "# h_bs = (np.random.randn(num_batches, M,mini_batch_size)+1j*np.random.randn(num_batches, M,mini_batch_size))*np.sqrt(b_bs/2)\n",
    "# h_is = (np.random.randn(num_batches, 1,mini_batch_size)+1j*np.random.randn(num_batches, 1,mini_batch_size))*np.sqrt(b_is/2)\n",
    "# h_id = (np.random.randn(num_batches, 1,mini_batch_size)+1j*np.random.randn(num_batches, 1,mini_batch_size))*np.sqrt(b_id/2)\n",
    "# h_sd = (np.random.randn(num_batches, 1,mini_batch_size)+1j*np.random.randn(num_batches, 1,mini_batch_size))*np.sqrt(b_sd/2)\n",
    "# g_rs = (np.random.randn(num_batches, N,mini_batch_size)+1j*np.random.randn(num_batches, N,mini_batch_size))*np.sqrt(b_rs/2)\n",
    "# #g_sr = np.random.randn(num_batches, N,mini_batch_size)+1j*np.random.randn(num_batches, N,mini_batch_size)\n",
    "# g_rd = (np.random.randn(num_batches, N,mini_batch_size)+1j*np.random.randn(num_batches, N,mini_batch_size))*np.sqrt(b_rd/2)\n",
    "# g_ir = (np.random.randn(num_batches, N,mini_batch_size)+1j*np.random.randn(num_batches, N,mini_batch_size))*np.sqrt(b_ir/2)\n",
    "# G_br = (np.random.randn(num_batches, N,M*mini_batch_size)+1j*np.random.randn(num_batches, N,M*mini_batch_size))*np.sqrt(b_br/2)\n",
    " \n",
    "# print(\"Shape of h_bs  is: \"+str(h_bs.shape))\n",
    "# print(\"Shape of h_is  is: \"+str(h_is.shape))\n",
    "# print(\"Shape of h_id  is: \"+str(h_id.shape))\n",
    "# print(\"Shape of h_sd  is: \"+str(h_sd.shape))\n",
    "# print(\"Shape of g_rs  is: \"+str(g_rs.shape))\n",
    "# print(\"Shape of g_rd  is: \"+str(g_rd.shape))\n",
    "# print(\"Shape of g_ir  is: \"+str(g_ir.shape))\n",
    "# print(\"Shape of G_br  is: \"+str(G_br.shape))\n",
    " \n",
    "# np.random.seed(144)\n",
    "# h_bs_t = (np.random.randn( M,mini_batch_size)+1j*np.random.randn( M,mini_batch_size))*np.sqrt(b_bs/2)\n",
    "# h_is_t = (np.random.randn( 1,mini_batch_size)+1j*np.random.randn( 1,mini_batch_size))*np.sqrt(b_is/2)\n",
    "# h_id_t = (np.random.randn( 1,mini_batch_size)+1j*np.random.randn( 1,mini_batch_size))*np.sqrt(b_id/2)\n",
    "# h_sd_t = (np.random.randn( 1,mini_batch_size)+1j*np.random.randn( 1,mini_batch_size))*np.sqrt(b_sd/2)\n",
    "# g_rs_t = (np.random.randn( N,mini_batch_size)+1j*np.random.randn( N,mini_batch_size))*np.sqrt(b_rs/2)\n",
    "# g_rd_t = (np.random.randn( N,mini_batch_size)+1j*np.random.randn( N,mini_batch_size))*np.sqrt(b_rd/2)\n",
    "# g_ir_t = (np.random.randn( N,mini_batch_size)+1j*np.random.randn( N,mini_batch_size))*np.sqrt(b_ir/2)\n",
    "# G_br_t = (np.random.randn( N,M*mini_batch_size)+1j*np.random.randn( N,M*mini_batch_size))*np.sqrt(b_br/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pOLKgtiZA62D",
    "outputId": "794a8ad2-989a-4503-d4a2-26c9536d6abc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "191\n"
     ]
    }
   ],
   "source": [
    "dims = [2*(N*M+M+N+1),5*(N*M+M+N+1), 6*(N*M+M+N+1), 5*(N*M+M+N+1),4*(N*M+M+N+1),1*(N*M+M+N+1), 2*N+2]\n",
    "seed = np.random.randint(400)\n",
    "#seed = 12\n",
    "print(seed)\n",
    "tf.reset_default_graph()\n",
    "W1=tf.get_variable(\"W1\",[dims[1],dims[0]],dtype=\"float64\", initializer = tf.contrib.layers.xavier_initializer(seed = seed))\n",
    "b1=tf.get_variable(\"b1\",[dims[1],1],dtype=\"float64\", initializer = tf.zeros_initializer())\n",
    "W2=tf.get_variable(\"W2\",[dims[2],dims[1]],dtype=\"float64\", initializer = tf.contrib.layers.xavier_initializer(seed = seed))\n",
    "b2=tf.get_variable(\"b2\",[dims[2],1],dtype=\"float64\", initializer = tf.zeros_initializer())\n",
    "W3=tf.get_variable(\"W3\",[dims[3],dims[2]],dtype=\"float64\", initializer = tf.contrib.layers.xavier_initializer(seed = seed))\n",
    "b3=tf.get_variable(\"b3\",[dims[3],1],dtype=\"float64\", initializer = tf.zeros_initializer())\n",
    "W4=tf.get_variable(\"W4\",[dims[4],dims[3]],dtype=\"float64\", initializer = tf.contrib.layers.xavier_initializer(seed = seed))\n",
    "b4=tf.get_variable(\"b4\",[dims[4],1],dtype=\"float64\", initializer = tf.zeros_initializer())\n",
    "W5=tf.get_variable(\"W5\",[dims[5],dims[4]],dtype=\"float64\", initializer = tf.contrib.layers.xavier_initializer(seed = seed))\n",
    "b5=tf.get_variable(\"b5\",[dims[5],1],dtype=\"float64\", initializer = tf.zeros_initializer())\n",
    "W6=tf.get_variable(\"W6\",[dims[6],dims[5]],dtype=\"float64\", initializer = tf.contrib.layers.xavier_initializer(seed = seed))\n",
    "b6=tf.get_variable(\"b6\",[dims[6],1],dtype=\"float64\", initializer = tf.zeros_initializer())\n",
    "#tau = tf.get_variable(\"tau\",[1,1],dtype=\"float64\", initializer = tf.zeros_initializer())\n",
    " \n",
    "X = tf.placeholder(tf.float64, shape=(2*(N*M+M+N+1),None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CwqDWbowBNCX"
   },
   "outputs": [],
   "source": [
    "Z1 = tf.add(tf.matmul(W1,X),b1)                     # Linear Layer\n",
    "mean, variance = tf.nn.moments(Z1, axes= [1] ,keepdims=True)\n",
    "Z1_BN = tf.nn.batch_normalization(Z1, mean=mean, variance= variance, scale=1.0, offset= None, variance_epsilon=10**-5) # Batch Normalization Layer\n",
    "A1 = tf.nn.dropout(tf.nn.relu(Z1_BN),rate = 0.0, seed= 95)        # Relu Layer\n",
    " \n",
    "Z2 = tf.add(tf.matmul(W2,A1),b2)                     # Linear Layer\n",
    "mean, variance = tf.nn.moments(Z2, axes= [1] ,keepdims=True)\n",
    "Z2_BN = tf.nn.batch_normalization(Z2, mean=mean, variance= variance, scale=1.0, offset= None, variance_epsilon=10**-5) # Batch Normalization Layer\n",
    "A2 = tf.nn.dropout(tf.nn.relu(Z2),rate = 0.0, seed= 195) \n",
    " \n",
    " \n",
    "Z3 = tf.add(tf.matmul(W3,A2),b3)                     # Linear Layer\n",
    "mean, variance = tf.nn.moments(Z3, axes= [1] ,keepdims=True)\n",
    "Z3_BN = tf.nn.batch_normalization(Z3, mean=mean, variance= variance, scale=1, offset= None, variance_epsilon=10**-5) # Batch Normalization Layer\n",
    "A3 = tf.nn.relu(Z3)\n",
    "\n",
    "Z4 = tf.add(tf.matmul(W4,A3),b4)                     # Linear Layer\n",
    "mean, variance = tf.nn.moments(Z4, axes= [1],keepdims=True)\n",
    "Z4_BN = tf.nn.batch_normalization(Z4, mean=mean, variance= variance, scale=1, offset= None, variance_epsilon=10**-5) # Batch Normalization Layer\n",
    "A4 = tf.nn.relu(Z4) \n",
    "\n",
    "\n",
    "Z5 = tf.add(tf.matmul(W5,A4),b5)                     # Linear Layer\n",
    "mean, variance = tf.nn.moments(Z5, axes= [1],keepdims=True)\n",
    "Z5_BN = tf.nn.batch_normalization(Z5, mean=mean, variance= variance, scale=1, offset= None, variance_epsilon=10**-5) # Batch Normalization Layer\n",
    "A5 = tf.nn.leaky_relu(Z5_BN,0.) \n",
    "theta1 = tf.nn.sigmoid(Z5)\n",
    "Z6 = tf.add(tf.matmul(W6,A5),b6)                     # Linear Layer\n",
    "# mean, variance = tf.nn.moments(Z6, axes= [1],keepdims=True)\n",
    "# Z6_BN = tf.nn.batch_normalization(Z6, mean=mean, variance= variance/variance, scale=1, offset= None, variance_epsilon=10**-5) # Batch Normalization Layer\n",
    "theta1 = tf.nn.sigmoid(Z6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gjsTiX8zZo4z"
   },
   "source": [
    "## The cost function is,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CLSwW0PBcvq1"
   },
   "outputs": [],
   "source": [
    "real = tf.zeros([N,tf.shape(X)[1]],dtype=tf.float64)\n",
    "#tau = tf.constant(0.1,dtype= tf.float64)\n",
    "log_base = tf.constant(2,dtype=tf.float64)\n",
    "etta = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RqN-u2hIpdPX"
   },
   "outputs": [],
   "source": [
    "theta_et1, theta_it1,tau,lemda = tf.split(theta1, num_or_size_splits= [N,N,1,1], axis = 0)\n",
    "theta_et = tf.exp(tf.complex(real,theta_et1*2*np.pi))\n",
    "theta_it = tf.exp(tf.complex(real,theta_it1*2*np.pi))\n",
    "# theta_et =loadmat('/content/drive/MyDrive/DL_prejects/Cost_varification_data/theta_et.mat')['theta_et']\n",
    "# print(theta_et.shape)  \n",
    "# theta_it = loadmat('/content/drive/MyDrive/DL_prejects/Cost_varification_data/theta_it.mat')['theta_it']\n",
    "# print(theta_it.shape) \n",
    "\n",
    "g_theta_et1 = tf.reshape(tf.complex(X[0:N*M,:],X[N*M:2*N*M,:]),[N,M*tf.shape(X)[1]])\n",
    "\n",
    "g_theta_et2 = g_theta_et1*tf.repeat(theta_et,M,axis=1)\n",
    "\n",
    "#g_theta_et6 = tf.reduce_sum(tf.transpose(g_theta_et2),axis = 1, keepdims=True)\n",
    "\n",
    "g_theta_et = tf.transpose(tf.reshape((tf.reduce_sum(g_theta_et2,axis = 0, keepdims= True)),[tf.shape(X)[1],M]),perm=[1,0])\n",
    "\n",
    "h_bs_c = tf.complex(X[2*N*M:2*N*M+M,:],X[2*N*M+M:2*N*M+2*M,:])\n",
    "\n",
    "\n",
    "Es = etta*tau*P_b*(tf.real(tf.norm((h_bs_c+g_theta_et), axis=0))**2)\n",
    "\n",
    "P_s = etta*(tau/(1-tau))*P_b*(tf.real(tf.norm((h_bs_c+g_theta_et), axis=0))**2)\n",
    "\n",
    "\n",
    "g_theta_it1 = tf.complex(X[2*N*M+2*M:2*N*M+2*M+N,:],X[2*N*M+2*M+N:2*N*M+2*M+2*N,:])\n",
    "g_theta_it = tf.reduce_sum(g_theta_it1*theta_it,axis = 0)\n",
    "h_sd_c = tf.complex(X[2*N*M+2*M+2*N:2*N*M+2*M+2*N+1,:],X[2*N*M+2*M+2*N+1:2*N*M+2*M+2*N+2,:])\n",
    "rd = P_s*(tf.abs((h_sd_c+g_theta_it)**2))/((sigma_n)**2)\n",
    "r = (1-tau)*tf.log(1+rd)/tf.log(log_base)\n",
    "cost = -tf.reduce_mean(r)*10\n",
    "cost1 = cost+lemda*tf.norm(theta1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aj7ZnWoGEd5R"
   },
   "outputs": [],
   "source": [
    "\n",
    "# g_rs = loadmat('/content/drive/MyDrive/DL_prejects/Cost_varification_data/g_rs.mat')['g_rs']\n",
    "# G_br = loadmat('/content/drive/MyDrive/DL_prejects/Cost_varification_data/G_br.mat')['G_br']\n",
    "# h_bs = loadmat('/content/drive/MyDrive/DL_prejects/Cost_varification_data/h_bs.mat')['h_bs']\n",
    "# g_rd = loadmat('/content/drive/MyDrive/DL_prejects/Cost_varification_data/g_rd.mat')['g_rd']\n",
    "# h_sd = loadmat('/content/drive/MyDrive/DL_prejects/Cost_varification_data/h_sd.mat')['h_sd']\n",
    "\n",
    "# # j =0\n",
    "# mini_batch_size = 3;\n",
    "# g_rs1 =  np.repeat(g_rs,M,axis = 1)\n",
    "# F = np.reshape(G_br*g_rs1, [N*M,mini_batch_size])\n",
    "# F = np.concatenate([np.real(F),np.imag(F)],axis=0)\n",
    "# F1 = np.concatenate([np.real(h_bs),np.imag(h_bs)],axis=0)\n",
    "# F2 = g_rd*g_rs\n",
    "# F2 = np.concatenate([np.real(F2),np.imag(F2)],axis=0)\n",
    "# F3 = np.concatenate([np.real(h_sd),np.imag(h_sd)],axis=0)\n",
    "# F = np.concatenate([F,F1,F2,F3],axis=0)\n",
    "# F.shape\n",
    "# sess = tf.Session()\n",
    "# init = tf.global_variables_initializer()\n",
    "# sess.run(init)\n",
    "# print(sess.run(r,feed_dict={X:F}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2dlzO2CxvWrB"
   },
   "outputs": [],
   "source": [
    "decay = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zYNJOh4GBmoI"
   },
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "step_rate =500*10**2\n",
    "\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "increment_global_step = tf.assign(global_step, global_step + 1)\n",
    "learning_rate = tf.train.exponential_decay(lr, global_step, step_rate, decay, staircase=True)\n",
    "\n",
    "  \n",
    "optimizer = tf.train.AdamOptimizer(learning_rate= learning_rate )\n",
    "minimize_loss = optimizer.minimize(cost, global_step = global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4G7IyIKGBnjb"
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cBhx94XKVh4n"
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "num_epochs = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "pbaykM2CBqGX",
    "outputId": "622951e9-bb14-4d19-9828-7771980dfde9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate 0.00025\n",
      "Epoch: 0 Training Loss= -32.52435524096151 Validation Loss= -33.40449531885883\n",
      "Learning Rate 0.00025\n",
      "Epoch: 1 Training Loss= -32.533137680354294 Validation Loss= -33.401453284090934\n",
      "Learning Rate 0.000125\n",
      "Epoch: 2 Training Loss= -32.52939290614153 Validation Loss= -33.41266933383937\n",
      "Learning Rate 0.000125\n",
      "Epoch: 3 Training Loss= -32.53396088592217 Validation Loss= -33.41626610424865\n",
      "Learning Rate 0.000125\n",
      "Epoch: 4 Training Loss= -32.54160247522962 Validation Loss= -33.4181118672028\n",
      "Learning Rate 0.000125\n",
      "Epoch: 5 Training Loss= -32.536629445287126 Validation Loss= -33.415047450876145\n",
      "Learning Rate 0.000125\n",
      "Epoch: 6 Training Loss= -32.533598151506254 Validation Loss= -33.417705976666106\n",
      "Learning Rate 0.000125\n",
      "Epoch: 7 Training Loss= -32.53579140990698 Validation Loss= -33.415968368259136\n",
      "Learning Rate 0.000125\n",
      "Epoch: 8 Training Loss= -32.53768491630498 Validation Loss= -33.41900026007898\n",
      "Learning Rate 0.000125\n",
      "Epoch: 9 Training Loss= -32.53276574018069 Validation Loss= -33.41289886338511\n",
      "Learning Rate 0.000125\n",
      "Epoch: 10 Training Loss= -32.54093492809961 Validation Loss= -33.410171213157525\n",
      "Learning Rate 0.000125\n",
      "Epoch: 11 Training Loss= -32.53816731624411 Validation Loss= -33.411330172025856\n",
      "Learning Rate 0.000125\n",
      "Epoch: 12 Training Loss= -32.537083674879504 Validation Loss= -33.412822323469406\n",
      "Learning Rate 0.000125\n",
      "Epoch: 13 Training Loss= -32.54047271373362 Validation Loss= -33.407816525067645\n",
      "Learning Rate 0.000125\n",
      "Epoch: 14 Training Loss= -32.540240494019905 Validation Loss= -33.41424323568726\n",
      "Learning Rate 0.000125\n",
      "Epoch: 15 Training Loss= -32.5394881500947 Validation Loss= -33.41129219253339\n",
      "Learning Rate 0.000125\n",
      "Epoch: 16 Training Loss= -32.5451858760247 Validation Loss= -33.412126054386746\n",
      "Learning Rate 0.000125\n",
      "Epoch: 17 Training Loss= -32.54031964262323 Validation Loss= -33.41247398708101\n",
      "Learning Rate 0.000125\n",
      "Epoch: 18 Training Loss= -32.54102966178181 Validation Loss= -33.41577780733536\n",
      "Learning Rate 0.000125\n",
      "Epoch: 19 Training Loss= -32.544590560094505 Validation Loss= -33.41207129701564\n",
      "Learning Rate 0.000125\n",
      "Epoch: 20 Training Loss= -32.539581116421715 Validation Loss= -33.41393133328282\n",
      "Learning Rate 0.000125\n",
      "Epoch: 21 Training Loss= -32.54097653640949 Validation Loss= -33.40959246159704\n",
      "Learning Rate 0.000125\n",
      "Epoch: 22 Training Loss= -32.539744146357734 Validation Loss= -33.4117291295453\n",
      "Learning Rate 0.000125\n",
      "Epoch: 23 Training Loss= -32.54697544417717 Validation Loss= -33.41202906749628\n",
      "Learning Rate 0.000125\n",
      "Epoch: 24 Training Loss= -32.54811938969145 Validation Loss= -33.41071436101825\n",
      "Learning Rate 0.000125\n",
      "Epoch: 25 Training Loss= -32.54470973236118 Validation Loss= -33.412435494781654\n",
      "Learning Rate 0.000125\n",
      "Epoch: 26 Training Loss= -32.54538431405548 Validation Loss= -33.41319000649716\n",
      "Learning Rate 0.000125\n",
      "Epoch: 27 Training Loss= -32.54710462278311 Validation Loss= -33.41172725596622\n",
      "Learning Rate 0.000125\n",
      "Epoch: 28 Training Loss= -32.54680876700767 Validation Loss= -33.41327708628742\n",
      "Learning Rate 0.000125\n",
      "Epoch: 29 Training Loss= -32.54685221015662 Validation Loss= -33.41362026895425\n",
      "Learning Rate 0.000125\n",
      "Epoch: 30 Training Loss= -32.54314774797817 Validation Loss= -33.414386444276495\n",
      "Learning Rate 0.000125\n",
      "Epoch: 31 Training Loss= -32.548012807928856 Validation Loss= -33.41112247090702\n",
      "Learning Rate 0.000125\n",
      "Epoch: 32 Training Loss= -32.5444623921944 Validation Loss= -33.41241868773752\n",
      "Learning Rate 0.000125\n",
      "Epoch: 33 Training Loss= -32.54472515907638 Validation Loss= -33.41495770820296\n",
      "Learning Rate 0.000125\n",
      "Epoch: 34 Training Loss= -32.54691308458253 Validation Loss= -33.411314320533016\n",
      "Learning Rate 0.000125\n",
      "Epoch: 35 Training Loss= -32.55211568986154 Validation Loss= -33.41526162096322\n",
      "Learning Rate 0.000125\n",
      "Epoch: 36 Training Loss= -32.54682335637009 Validation Loss= -33.4117011542384\n",
      "Learning Rate 0.000125\n",
      "Epoch: 37 Training Loss= -32.54887640708542 Validation Loss= -33.411311336632366\n",
      "Learning Rate 0.000125\n",
      "Epoch: 38 Training Loss= -32.54921781950855 Validation Loss= -33.41370907096391\n",
      "Learning Rate 0.000125\n",
      "Epoch: 39 Training Loss= -32.545623640429234 Validation Loss= -33.41301382144738\n",
      "Learning Rate 0.000125\n",
      "Epoch: 40 Training Loss= -32.55053857143717 Validation Loss= -33.41309710252418\n",
      "Learning Rate 0.000125\n",
      "Epoch: 41 Training Loss= -32.548440230554206 Validation Loss= -33.40826284263376\n",
      "Learning Rate 0.000125\n",
      "Epoch: 42 Training Loss= -32.54908669241973 Validation Loss= -33.41551348345989\n",
      "Learning Rate 0.000125\n",
      "Epoch: 43 Training Loss= -32.54828885790823 Validation Loss= -33.414474146064\n",
      "Learning Rate 0.000125\n",
      "Epoch: 44 Training Loss= -32.54938571746491 Validation Loss= -33.4174987770016\n",
      "Learning Rate 0.000125\n",
      "Epoch: 45 Training Loss= -32.55062680668338 Validation Loss= -33.41607846168335\n",
      "Learning Rate 0.000125\n",
      "Epoch: 46 Training Loss= -32.554642188522635 Validation Loss= -33.415042784816464\n",
      "Learning Rate 0.000125\n",
      "Epoch: 47 Training Loss= -32.553293041555854 Validation Loss= -33.41469916395583\n",
      "Learning Rate 0.000125\n",
      "Epoch: 48 Training Loss= -32.54625172132168 Validation Loss= -33.41153942974576\n",
      "Learning Rate 0.000125\n",
      "Epoch: 49 Training Loss= -32.550585313716546 Validation Loss= -33.41642412431466\n",
      "Learning Rate 0.000125\n",
      "Epoch: 50 Training Loss= -32.54901543974473 Validation Loss= -33.41732196472802\n",
      "Learning Rate 0.000125\n",
      "Epoch: 51 Training Loss= -32.550985620854426 Validation Loss= -33.418741368066854\n",
      "Learning Rate 0.000125\n",
      "Epoch: 52 Training Loss= -32.547867715468726 Validation Loss= -33.41839461054809\n",
      "Learning Rate 0.000125\n",
      "Epoch: 53 Training Loss= -32.54874020711026 Validation Loss= -33.41436567273021\n",
      "Learning Rate 0.000125\n",
      "Epoch: 54 Training Loss= -32.551415914800195 Validation Loss= -33.41264064494189\n",
      "Learning Rate 0.000125\n",
      "Epoch: 55 Training Loss= -32.55384783486918 Validation Loss= -33.41426263273431\n",
      "Learning Rate 0.000125\n",
      "Epoch: 56 Training Loss= -32.55098427803461 Validation Loss= -33.41668062288273\n",
      "Learning Rate 0.000125\n",
      "Epoch: 57 Training Loss= -32.55245654260521 Validation Loss= -33.413285412029964\n",
      "Learning Rate 0.000125\n",
      "Epoch: 58 Training Loss= -32.55219832369224 Validation Loss= -33.419272355258684\n",
      "Learning Rate 0.000125\n",
      "Epoch: 59 Training Loss= -32.55289055374106 Validation Loss= -33.4109964743081\n",
      "Learning Rate 0.000125\n",
      "Epoch: 60 Training Loss= -32.55249309484795 Validation Loss= -33.412925207984074\n",
      "Learning Rate 0.000125\n",
      "Epoch: 61 Training Loss= -32.55444132183793 Validation Loss= -33.415134599236936\n",
      "Learning Rate 0.000125\n",
      "Epoch: 62 Training Loss= -32.55432407268256 Validation Loss= -33.4162483611715\n",
      "Learning Rate 0.000125\n",
      "Epoch: 63 Training Loss= -32.549854939311935 Validation Loss= -33.41470269362554\n",
      "Learning Rate 0.000125\n",
      "Epoch: 64 Training Loss= -32.55200307142018 Validation Loss= -33.41372201231587\n",
      "Learning Rate 0.000125\n",
      "Epoch: 65 Training Loss= -32.553637534626475 Validation Loss= -33.41684081323157\n",
      "Learning Rate 0.000125\n",
      "Epoch: 66 Training Loss= -32.5535671508274 Validation Loss= -33.416414206356976\n",
      "Learning Rate 0.000125\n",
      "Epoch: 67 Training Loss= -32.54595510728037 Validation Loss= -33.41264051772946\n",
      "Learning Rate 0.000125\n",
      "Epoch: 68 Training Loss= -32.55453121594399 Validation Loss= -33.413449950340734\n",
      "Learning Rate 0.000125\n",
      "Epoch: 69 Training Loss= -32.55181869464379 Validation Loss= -33.41710506459642\n",
      "Learning Rate 0.000125\n",
      "Epoch: 70 Training Loss= -32.55697716264367 Validation Loss= -33.4152489844491\n",
      "Learning Rate 0.000125\n",
      "Epoch: 71 Training Loss= -32.55065192784179 Validation Loss= -33.41794583278476\n",
      "Learning Rate 0.000125\n",
      "Epoch: 72 Training Loss= -32.55951013675975 Validation Loss= -33.415721144181234\n",
      "Learning Rate 0.000125\n",
      "Epoch: 73 Training Loss= -32.555763550691 Validation Loss= -33.41094109587211\n",
      "Learning Rate 0.000125\n",
      "Epoch: 74 Training Loss= -32.55751704389145 Validation Loss= -33.415835134758254\n",
      "Learning Rate 0.000125\n",
      "Epoch: 75 Training Loss= -32.55441612756526 Validation Loss= -33.41397904069433\n",
      "Learning Rate 0.000125\n",
      "Epoch: 76 Training Loss= -32.558776017497145 Validation Loss= -33.415436811250956\n",
      "Learning Rate 0.000125\n",
      "Epoch: 77 Training Loss= -32.55550908259022 Validation Loss= -33.41431135701187\n",
      "Learning Rate 0.000125\n",
      "Epoch: 78 Training Loss= -32.554481415086855 Validation Loss= -33.41652228971218\n",
      "Learning Rate 0.000125\n",
      "Epoch: 79 Training Loss= -32.55519789260739 Validation Loss= -33.41787303540591\n",
      "Learning Rate 0.000125\n",
      "Epoch: 80 Training Loss= -32.556589602423216 Validation Loss= -33.415455126157234\n",
      "Learning Rate 0.000125\n",
      "Epoch: 81 Training Loss= -32.55545048567616 Validation Loss= -33.41441935853297\n",
      "Learning Rate 0.000125\n",
      "Epoch: 82 Training Loss= -32.557061816240676 Validation Loss= -33.411617447332546\n",
      "Learning Rate 0.000125\n",
      "Epoch: 83 Training Loss= -32.55409233857857 Validation Loss= -33.416770246238315\n",
      "Learning Rate 0.000125\n",
      "Epoch: 84 Training Loss= -32.55735252399096 Validation Loss= -33.41875163480764\n",
      "Learning Rate 0.000125\n",
      "Epoch: 85 Training Loss= -32.55438386670203 Validation Loss= -33.41633194347263\n",
      "Learning Rate 0.000125\n",
      "Epoch: 86 Training Loss= -32.55335096154805 Validation Loss= -33.41555218527726\n",
      "Learning Rate 0.000125\n",
      "Epoch: 87 Training Loss= -32.560707970637004 Validation Loss= -33.418101200849335\n",
      "Learning Rate 0.000125\n",
      "Epoch: 88 Training Loss= -32.555354937766 Validation Loss= -33.41638743244645\n",
      "Learning Rate 0.000125\n",
      "Epoch: 89 Training Loss= -32.55526455501974 Validation Loss= -33.41286929615425\n",
      "Learning Rate 0.000125\n",
      "Epoch: 90 Training Loss= -32.55578837490786 Validation Loss= -33.4144073632194\n",
      "Learning Rate 0.000125\n",
      "Epoch: 91 Training Loss= -32.55754527266275 Validation Loss= -33.41343400350765\n",
      "Learning Rate 0.000125\n",
      "Epoch: 92 Training Loss= -32.55301030579826 Validation Loss= -33.40907924611348\n",
      "Learning Rate 0.000125\n",
      "Epoch: 93 Training Loss= -32.55261026301088 Validation Loss= -33.414396366351\n",
      "Learning Rate 0.000125\n",
      "Epoch: 94 Training Loss= -32.55370335461439 Validation Loss= -33.41655421072881\n",
      "Learning Rate 0.000125\n",
      "Epoch: 95 Training Loss= -32.551016446959075 Validation Loss= -33.41611366633305\n",
      "Learning Rate 0.000125\n",
      "Epoch: 96 Training Loss= -32.55403586345188 Validation Loss= -33.41597488914065\n",
      "Learning Rate 0.000125\n",
      "Epoch: 97 Training Loss= -32.55391939949184 Validation Loss= -33.41700306416286\n",
      "Learning Rate 0.000125\n",
      "Epoch: 98 Training Loss= -32.553960672247705 Validation Loss= -33.414987694256396\n",
      "Learning Rate 0.000125\n",
      "Epoch: 99 Training Loss= -32.553494814410755 Validation Loss= -33.413327158207245\n",
      "Learning Rate 0.000125\n",
      "Epoch: 100 Training Loss= -32.55406797195501 Validation Loss= -33.41605450585818\n",
      "Learning Rate 0.000125\n",
      "Epoch: 101 Training Loss= -32.55485604569877 Validation Loss= -33.410912930273476\n",
      "Learning Rate 0.000125\n",
      "Epoch: 102 Training Loss= -32.55651169192001 Validation Loss= -33.41343839092318\n",
      "Learning Rate 0.000125\n",
      "Epoch: 103 Training Loss= -32.553272446209995 Validation Loss= -33.41663192190281\n",
      "Learning Rate 0.000125\n",
      "Epoch: 104 Training Loss= -32.55468046427876 Validation Loss= -33.41372482948478\n",
      "Learning Rate 0.000125\n",
      "Epoch: 105 Training Loss= -32.552053597598125 Validation Loss= -33.41006831066945\n",
      "Learning Rate 0.000125\n",
      "Epoch: 106 Training Loss= -32.5560830972538 Validation Loss= -33.413189163235366\n",
      "Learning Rate 0.000125\n",
      "Epoch: 107 Training Loss= -32.55569301200216 Validation Loss= -33.41359015179443\n",
      "Learning Rate 0.000125\n",
      "Epoch: 108 Training Loss= -32.55444776656081 Validation Loss= -33.40759291253451\n",
      "Learning Rate 0.000125\n",
      "Epoch: 109 Training Loss= -32.556476771662155 Validation Loss= -33.414593665947706\n",
      "Learning Rate 0.000125\n",
      "Epoch: 110 Training Loss= -32.55758004713448 Validation Loss= -33.41459337976019\n",
      "Learning Rate 0.000125\n",
      "Epoch: 111 Training Loss= -32.55765847734275 Validation Loss= -33.414969390254335\n",
      "Learning Rate 0.000125\n",
      "Epoch: 112 Training Loss= -32.5560355194071 Validation Loss= -33.410437354337624\n",
      "Learning Rate 0.000125\n",
      "Epoch: 113 Training Loss= -32.55792110927774 Validation Loss= -33.41274037445638\n",
      "Learning Rate 0.000125\n",
      "Epoch: 114 Training Loss= -32.55709259021471 Validation Loss= -33.41733440975803\n",
      "Learning Rate 0.000125\n",
      "Epoch: 115 Training Loss= -32.55872858593949 Validation Loss= -33.41182903415093\n",
      "Learning Rate 0.000125\n",
      "Epoch: 116 Training Loss= -32.55819484691491 Validation Loss= -33.41092541557644\n",
      "Learning Rate 0.000125\n",
      "Epoch: 117 Training Loss= -32.55959215294319 Validation Loss= -33.41371959779623\n",
      "Learning Rate 0.000125\n",
      "Epoch: 118 Training Loss= -32.55588859671364 Validation Loss= -33.415622813167666\n",
      "Learning Rate 0.000125\n",
      "Epoch: 119 Training Loss= -32.55497220270412 Validation Loss= -33.41687813290551\n",
      "Learning Rate 0.000125\n",
      "Epoch: 120 Training Loss= -32.557498500077806 Validation Loss= -33.415433124696165\n",
      "Learning Rate 0.000125\n",
      "Epoch: 121 Training Loss= -32.56183376460318 Validation Loss= -33.41150600101325\n",
      "Learning Rate 0.000125\n",
      "Epoch: 122 Training Loss= -32.56037916928936 Validation Loss= -33.411237171690416\n",
      "Learning Rate 0.000125\n",
      "Epoch: 123 Training Loss= -32.55875101320654 Validation Loss= -33.411901927359715\n",
      "Learning Rate 0.000125\n",
      "Epoch: 124 Training Loss= -32.56029036443746 Validation Loss= -33.42055768362245\n",
      "Learning Rate 0.000125\n",
      "Epoch: 125 Training Loss= -32.5602860321455 Validation Loss= -33.41605616957853\n",
      "Learning Rate 0.000125\n",
      "Epoch: 126 Training Loss= -32.55715663087601 Validation Loss= -33.412047762028976\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 127 Training Loss= -32.56312077040732 Validation Loss= -33.42202512169673\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 128 Training Loss= -32.566711584509704 Validation Loss= -33.42101259479038\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 129 Training Loss= -32.566478829595944 Validation Loss= -33.419135911070214\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 130 Training Loss= -32.566131763731406 Validation Loss= -33.419842494165785\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 131 Training Loss= -32.56815117727352 Validation Loss= -33.42056949191318\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 132 Training Loss= -32.56536856655679 Validation Loss= -33.41992865892792\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 133 Training Loss= -32.56664979033361 Validation Loss= -33.419526546388994\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 134 Training Loss= -32.56622652416239 Validation Loss= -33.42132629976239\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 135 Training Loss= -32.56668054667927 Validation Loss= -33.42025892847551\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 136 Training Loss= -32.56807646094056 Validation Loss= -33.416233497082374\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 137 Training Loss= -32.56852448711721 Validation Loss= -33.418342234158644\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 138 Training Loss= -32.5685225555011 Validation Loss= -33.418022161709146\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 139 Training Loss= -32.56733596913425 Validation Loss= -33.418507595594704\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 140 Training Loss= -32.57077127990939 Validation Loss= -33.421113096896526\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 141 Training Loss= -32.568166849085166 Validation Loss= -33.41963183980959\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 142 Training Loss= -32.566164596732506 Validation Loss= -33.421634132567505\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 143 Training Loss= -32.56450348681518 Validation Loss= -33.418306135535765\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 144 Training Loss= -32.568041327057394 Validation Loss= -33.421883605355504\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 145 Training Loss= -32.56664834036514 Validation Loss= -33.41645885083293\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 146 Training Loss= -32.56600625648358 Validation Loss= -33.41939765148941\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 147 Training Loss= -32.56740117021287 Validation Loss= -33.421039675969595\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 148 Training Loss= -32.566823226655345 Validation Loss= -33.42047153934664\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 149 Training Loss= -32.567314679198105 Validation Loss= -33.42003405047686\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 150 Training Loss= -32.56726197843016 Validation Loss= -33.41822780085971\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 151 Training Loss= -32.567288287079485 Validation Loss= -33.41905198138441\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 152 Training Loss= -32.56495092015329 Validation Loss= -33.419400377768554\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 153 Training Loss= -32.567684595926714 Validation Loss= -33.41947728264515\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 154 Training Loss= -32.56308563155631 Validation Loss= -33.418750509554485\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 155 Training Loss= -32.56839538706428 Validation Loss= -33.421331392850455\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 156 Training Loss= -32.567531371632256 Validation Loss= -33.418456344690256\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 157 Training Loss= -32.56811093406904 Validation Loss= -33.42048008285496\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 158 Training Loss= -32.56576633673382 Validation Loss= -33.418918638355336\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 159 Training Loss= -32.56713315119074 Validation Loss= -33.42191192757727\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 160 Training Loss= -32.5672784719703 Validation Loss= -33.41895151307352\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 161 Training Loss= -32.566660821611265 Validation Loss= -33.41601728096317\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 162 Training Loss= -32.567730143255595 Validation Loss= -33.41877837290765\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 163 Training Loss= -32.56819059791982 Validation Loss= -33.41951327365124\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 164 Training Loss= -32.570603463359134 Validation Loss= -33.420507813263605\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 165 Training Loss= -32.56790221756382 Validation Loss= -33.42219837147453\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 166 Training Loss= -32.566676367579916 Validation Loss= -33.41906327682815\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 167 Training Loss= -32.5660332040875 Validation Loss= -33.420368551490384\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 168 Training Loss= -32.566736236447525 Validation Loss= -33.42374826399614\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 169 Training Loss= -32.56874001408936 Validation Loss= -33.42106782206422\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 170 Training Loss= -32.571177336575786 Validation Loss= -33.423131798460346\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 171 Training Loss= -32.56875948291522 Validation Loss= -33.419727713111655\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 172 Training Loss= -32.56970646991603 Validation Loss= -33.421908595607505\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 173 Training Loss= -32.57012974653911 Validation Loss= -33.42271762247578\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 174 Training Loss= -32.5683876277215 Validation Loss= -33.42287367878618\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 175 Training Loss= -32.569518834980165 Validation Loss= -33.42254952021327\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 176 Training Loss= -32.569968652234635 Validation Loss= -33.423473076382386\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 177 Training Loss= -32.57072363415662 Validation Loss= -33.426001802508495\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 178 Training Loss= -32.569794531198355 Validation Loss= -33.425227171750926\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 179 Training Loss= -32.56928511636714 Validation Loss= -33.42157997987454\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 180 Training Loss= -32.56884425510524 Validation Loss= -33.423483683548255\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 181 Training Loss= -32.568533678555404 Validation Loss= -33.42188760855321\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 182 Training Loss= -32.570373631832 Validation Loss= -33.424147874470115\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 183 Training Loss= -32.5702047678588 Validation Loss= -33.420792394769535\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 184 Training Loss= -32.57070980995789 Validation Loss= -33.42234303119998\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 185 Training Loss= -32.57155759349273 Validation Loss= -33.42118505265135\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 186 Training Loss= -32.56945400680505 Validation Loss= -33.423853440776774\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 187 Training Loss= -32.571831403659786 Validation Loss= -33.42142734859675\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 188 Training Loss= -32.56984076311427 Validation Loss= -33.42418809370843\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 189 Training Loss= -32.570408195071664 Validation Loss= -33.423231784694\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 190 Training Loss= -32.569788619012094 Validation Loss= -33.42290283286889\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 191 Training Loss= -32.5702949295955 Validation Loss= -33.42191792218859\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 192 Training Loss= -32.572440866090204 Validation Loss= -33.420223298522956\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 193 Training Loss= -32.5700663983571 Validation Loss= -33.42088915241404\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 194 Training Loss= -32.5704631413839 Validation Loss= -33.4219929008252\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 195 Training Loss= -32.57349068117404 Validation Loss= -33.422180902760864\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 196 Training Loss= -32.570934600191286 Validation Loss= -33.42398813088575\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 197 Training Loss= -32.57133330172528 Validation Loss= -33.42256112869132\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 198 Training Loss= -32.57150829192742 Validation Loss= -33.42123021747604\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 199 Training Loss= -32.57230756681978 Validation Loss= -33.42116668602636\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 200 Training Loss= -32.57175465992902 Validation Loss= -33.42097994370904\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 201 Training Loss= -32.57333627595445 Validation Loss= -33.423002068270165\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 202 Training Loss= -32.57406323865038 Validation Loss= -33.4220314831991\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 203 Training Loss= -32.56995979233797 Validation Loss= -33.421179242973686\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 204 Training Loss= -32.570098671071904 Validation Loss= -33.42137097427027\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 205 Training Loss= -32.57279673117691 Validation Loss= -33.42084833659137\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 206 Training Loss= -32.570612534957185 Validation Loss= -33.4228793559303\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 207 Training Loss= -32.572618474220945 Validation Loss= -33.422879129345596\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 208 Training Loss= -32.57190993569078 Validation Loss= -33.42131292025235\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 209 Training Loss= -32.573130136255784 Validation Loss= -33.42240793017674\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 210 Training Loss= -32.57480465621486 Validation Loss= -33.42279486022016\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 211 Training Loss= -32.57219701807075 Validation Loss= -33.42034920906772\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 212 Training Loss= -32.57137801252331 Validation Loss= -33.423022304090914\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 213 Training Loss= -32.57174834607896 Validation Loss= -33.421826803166205\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 214 Training Loss= -32.57043442937848 Validation Loss= -33.422394258335586\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 215 Training Loss= -32.570102931874565 Validation Loss= -33.42132945334039\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 216 Training Loss= -32.57434073061391 Validation Loss= -33.42303034571192\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 217 Training Loss= -32.572784698143685 Validation Loss= -33.42126932436092\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 218 Training Loss= -32.574099089306124 Validation Loss= -33.42377824369733\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 219 Training Loss= -32.57291946962187 Validation Loss= -33.422751599552164\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 220 Training Loss= -32.57265382484897 Validation Loss= -33.422129480066495\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 221 Training Loss= -32.571880002087184 Validation Loss= -33.422650772505044\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 222 Training Loss= -32.573181785762266 Validation Loss= -33.42256617851105\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 223 Training Loss= -32.57381276157903 Validation Loss= -33.42156983717456\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 224 Training Loss= -32.573455099132325 Validation Loss= -33.419008931709\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 225 Training Loss= -32.57074074382666 Validation Loss= -33.42097889659793\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 226 Training Loss= -32.57274545112583 Validation Loss= -33.42037631742937\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 227 Training Loss= -32.573150009758386 Validation Loss= -33.422246085723685\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 228 Training Loss= -32.57545090018429 Validation Loss= -33.42371643552036\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 229 Training Loss= -32.57500935490002 Validation Loss= -33.42170126322882\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 230 Training Loss= -32.57304499649328 Validation Loss= -33.42065748145566\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 231 Training Loss= -32.57158664902016 Validation Loss= -33.42077642240423\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 232 Training Loss= -32.57385478534043 Validation Loss= -33.422887968802854\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 233 Training Loss= -32.57372090342986 Validation Loss= -33.4237588313825\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 234 Training Loss= -32.57390813718219 Validation Loss= -33.42026240054388\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 235 Training Loss= -32.572177890713 Validation Loss= -33.41967928126617\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 236 Training Loss= -32.57225168458983 Validation Loss= -33.420660827502715\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 237 Training Loss= -32.573653301249536 Validation Loss= -33.42329674678536\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 238 Training Loss= -32.57333674644065 Validation Loss= -33.422518365284645\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 239 Training Loss= -32.571710667315344 Validation Loss= -33.42476080243165\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 240 Training Loss= -32.575122000695266 Validation Loss= -33.422402741042305\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 241 Training Loss= -32.57487703991893 Validation Loss= -33.4222171471339\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 242 Training Loss= -32.575136418853134 Validation Loss= -33.42362196327401\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 243 Training Loss= -32.57562142194468 Validation Loss= -33.422030436780766\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 244 Training Loss= -32.57543862558385 Validation Loss= -33.42223468141107\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 245 Training Loss= -32.57299594508767 Validation Loss= -33.421306165232465\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 246 Training Loss= -32.57250042006386 Validation Loss= -33.42162004602741\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 247 Training Loss= -32.574305237817825 Validation Loss= -33.423117532877995\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 248 Training Loss= -32.57242940879851 Validation Loss= -33.42388940391692\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 249 Training Loss= -32.57163038396151 Validation Loss= -33.42411585141276\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 250 Training Loss= -32.57485018628663 Validation Loss= -33.424899193608\n",
      "Learning Rate 6.25e-05\n",
      "Epoch: 251 Training Loss= -32.57343544882796 Validation Loss= -33.423013917783194\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 252 Training Loss= -32.57362705325764 Validation Loss= -33.42402264132452\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 253 Training Loss= -32.57431078947496 Validation Loss= -33.42532726725718\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 254 Training Loss= -32.574644415558275 Validation Loss= -33.42313286019785\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 255 Training Loss= -32.576005281107605 Validation Loss= -33.42665292618806\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 256 Training Loss= -32.57614923212546 Validation Loss= -33.42508094219432\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 257 Training Loss= -32.57572167211302 Validation Loss= -33.42532978612266\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 258 Training Loss= -32.57478662424451 Validation Loss= -33.42524386353369\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 259 Training Loss= -32.575845397492984 Validation Loss= -33.42578602620505\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 260 Training Loss= -32.57602661483268 Validation Loss= -33.42472098991734\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 261 Training Loss= -32.5752989452528 Validation Loss= -33.42407421617391\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 262 Training Loss= -32.57451590893876 Validation Loss= -33.424362938448596\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 263 Training Loss= -32.57492870797015 Validation Loss= -33.42578118403352\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 264 Training Loss= -32.57455596247111 Validation Loss= -33.423776854657895\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 265 Training Loss= -32.57472669290225 Validation Loss= -33.42550081166026\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 266 Training Loss= -32.57666324543406 Validation Loss= -33.42520225681518\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 267 Training Loss= -32.57478083008094 Validation Loss= -33.42327660749808\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 268 Training Loss= -32.57407799248493 Validation Loss= -33.42461470144966\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 269 Training Loss= -32.57492632038135 Validation Loss= -33.42373882378527\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 270 Training Loss= -32.57660977125851 Validation Loss= -33.42337511737077\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 271 Training Loss= -32.57680240764353 Validation Loss= -33.42445726603374\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 272 Training Loss= -32.57644329600934 Validation Loss= -33.424725553469436\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 273 Training Loss= -32.57734756932964 Validation Loss= -33.42323013277437\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 274 Training Loss= -32.57695616127193 Validation Loss= -33.42354836674741\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 275 Training Loss= -32.57729562577582 Validation Loss= -33.42318916749599\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 276 Training Loss= -32.57706562758104 Validation Loss= -33.423504336382905\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 277 Training Loss= -32.579219482801406 Validation Loss= -33.422517001848945\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 278 Training Loss= -32.575874172412526 Validation Loss= -33.422646768937724\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 279 Training Loss= -32.57557499999954 Validation Loss= -33.42299483570556\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 280 Training Loss= -32.57731018586372 Validation Loss= -33.42265529127717\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 281 Training Loss= -32.57583344201762 Validation Loss= -33.42151764313938\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 282 Training Loss= -32.57625932046015 Validation Loss= -33.42392692993323\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 283 Training Loss= -32.577158131813576 Validation Loss= -33.42227102467347\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 284 Training Loss= -32.57859191650575 Validation Loss= -33.42305640530665\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 285 Training Loss= -32.578850982877256 Validation Loss= -33.42367712941038\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 286 Training Loss= -32.57678779373822 Validation Loss= -33.423200924762064\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 287 Training Loss= -32.57913246825588 Validation Loss= -33.42386263625376\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 288 Training Loss= -32.576025338977544 Validation Loss= -33.42432763195443\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 289 Training Loss= -32.57636488550591 Validation Loss= -33.423118290187695\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 290 Training Loss= -32.57639687974307 Validation Loss= -33.42481259534159\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 291 Training Loss= -32.57486223362456 Validation Loss= -33.422506061891696\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 292 Training Loss= -32.574860999065 Validation Loss= -33.42384088108314\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 293 Training Loss= -32.578568482854735 Validation Loss= -33.424092354312236\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 294 Training Loss= -32.5773785024526 Validation Loss= -33.42386355667588\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 295 Training Loss= -32.57918667900369 Validation Loss= -33.42293242508785\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 296 Training Loss= -32.57759734237473 Validation Loss= -33.42164664047427\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 297 Training Loss= -32.578263328282176 Validation Loss= -33.421983668552066\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 298 Training Loss= -32.57582782658532 Validation Loss= -33.421356700217586\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 299 Training Loss= -32.57694348299883 Validation Loss= -33.42376009061914\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 300 Training Loss= -32.5762609938948 Validation Loss= -33.4234888987307\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 301 Training Loss= -32.57891069765778 Validation Loss= -33.42262197110932\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 302 Training Loss= -32.579273632458296 Validation Loss= -33.42286519409343\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 303 Training Loss= -32.57652298490788 Validation Loss= -33.422252977373816\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 304 Training Loss= -32.577704582431565 Validation Loss= -33.42229759862562\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 305 Training Loss= -32.5793871991797 Validation Loss= -33.423354626994296\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 306 Training Loss= -32.57811411217775 Validation Loss= -33.42397998099764\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 307 Training Loss= -32.578812861532825 Validation Loss= -33.42374460329116\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 308 Training Loss= -32.57809434318363 Validation Loss= -33.42277188550821\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 309 Training Loss= -32.57893557550196 Validation Loss= -33.42487034080453\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 310 Training Loss= -32.57781886045382 Validation Loss= -33.42270523834233\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 311 Training Loss= -32.57806217652439 Validation Loss= -33.42391550640956\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 312 Training Loss= -32.57859335363054 Validation Loss= -33.42392291874401\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 313 Training Loss= -32.581349968012326 Validation Loss= -33.4230308655182\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 314 Training Loss= -32.578393676788096 Validation Loss= -33.42215653120859\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 315 Training Loss= -32.57938748954591 Validation Loss= -33.42442859425465\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 316 Training Loss= -32.57817067160266 Validation Loss= -33.421766363656324\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 317 Training Loss= -32.5808265788246 Validation Loss= -33.421579593061416\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 318 Training Loss= -32.579848266923264 Validation Loss= -33.424167699957366\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 319 Training Loss= -32.57838290183595 Validation Loss= -33.42560240689504\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 320 Training Loss= -32.579097060035416 Validation Loss= -33.421745557170375\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 321 Training Loss= -32.57922847662393 Validation Loss= -33.42381353409415\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 322 Training Loss= -32.579118454568444 Validation Loss= -33.423587901739594\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 323 Training Loss= -32.577611513007824 Validation Loss= -33.42345888534494\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 324 Training Loss= -32.58134566065389 Validation Loss= -33.42255464277287\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 325 Training Loss= -32.57938126231564 Validation Loss= -33.42362785872932\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 326 Training Loss= -32.57984423279861 Validation Loss= -33.4240603846711\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 327 Training Loss= -32.57980678056746 Validation Loss= -33.42290109220929\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 328 Training Loss= -32.57825009686088 Validation Loss= -33.42277923861781\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 329 Training Loss= -32.5789129193756 Validation Loss= -33.42198901455373\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 330 Training Loss= -32.57855798583218 Validation Loss= -33.42578360917892\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 331 Training Loss= -32.578129559245475 Validation Loss= -33.42415797800016\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 332 Training Loss= -32.579817461890855 Validation Loss= -33.42261869446561\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 333 Training Loss= -32.58028829358234 Validation Loss= -33.42184235191178\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 334 Training Loss= -32.579118862833724 Validation Loss= -33.421385854922214\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 335 Training Loss= -32.578469788152084 Validation Loss= -33.422389726545376\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 336 Training Loss= -32.57961692933123 Validation Loss= -33.42307360165959\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 337 Training Loss= -32.58046046814809 Validation Loss= -33.42270913714112\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 338 Training Loss= -32.578258646776284 Validation Loss= -33.42159736536901\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 339 Training Loss= -32.581236213203965 Validation Loss= -33.423798262406585\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 340 Training Loss= -32.58081423122328 Validation Loss= -33.42246297476869\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 341 Training Loss= -32.580030757346144 Validation Loss= -33.42249629799104\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 342 Training Loss= -32.57850960723007 Validation Loss= -33.42283697143324\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 343 Training Loss= -32.57883178655887 Validation Loss= -33.42277882268651\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 344 Training Loss= -32.57722307215269 Validation Loss= -33.42368147400541\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 345 Training Loss= -32.57943125508205 Validation Loss= -33.421563865958134\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 346 Training Loss= -32.57954096119185 Validation Loss= -33.42032179959396\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 347 Training Loss= -32.57975050602367 Validation Loss= -33.420706360960565\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 348 Training Loss= -32.578434935917684 Validation Loss= -33.422295112633776\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 349 Training Loss= -32.58041737507041 Validation Loss= -33.422580377271295\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 350 Training Loss= -32.578160981358934 Validation Loss= -33.42224425498044\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 351 Training Loss= -32.58178557798094 Validation Loss= -33.42220117510888\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 352 Training Loss= -32.58056566654676 Validation Loss= -33.42135424407527\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 353 Training Loss= -32.57912569349222 Validation Loss= -33.423272884246686\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 354 Training Loss= -32.57991552112409 Validation Loss= -33.423334977086895\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 355 Training Loss= -32.58204163878874 Validation Loss= -33.423215206202926\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 356 Training Loss= -32.58019629882867 Validation Loss= -33.42238520281004\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 357 Training Loss= -32.581756156590174 Validation Loss= -33.42079347473879\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 358 Training Loss= -32.58162341218583 Validation Loss= -33.422139944656465\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 359 Training Loss= -32.58021963349003 Validation Loss= -33.422875228909945\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 360 Training Loss= -32.58012505042522 Validation Loss= -33.42363698101223\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 361 Training Loss= -32.58029641808763 Validation Loss= -33.42184190687102\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 362 Training Loss= -32.57863761761859 Validation Loss= -33.42040751797404\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 363 Training Loss= -32.583107990469834 Validation Loss= -33.42332606857488\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 364 Training Loss= -32.581213632458834 Validation Loss= -33.421802882341176\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 365 Training Loss= -32.58288270225425 Validation Loss= -33.42378462793277\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 366 Training Loss= -32.58179747428595 Validation Loss= -33.42293497107025\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 367 Training Loss= -32.58017976420839 Validation Loss= -33.42047186724513\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 368 Training Loss= -32.58186941761395 Validation Loss= -33.42195385704574\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 369 Training Loss= -32.58382954576808 Validation Loss= -33.421021738398366\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 370 Training Loss= -32.58247249775513 Validation Loss= -33.422411901040675\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 371 Training Loss= -32.58101249709499 Validation Loss= -33.422627351953906\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 372 Training Loss= -32.58183650111654 Validation Loss= -33.422517106296944\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 373 Training Loss= -32.580516467162624 Validation Loss= -33.42305211383584\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 374 Training Loss= -32.58167315852702 Validation Loss= -33.42335900450446\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 375 Training Loss= -32.58176488701103 Validation Loss= -33.42186748467677\n",
      "Learning Rate 3.125e-05\n",
      "Epoch: 376 Training Loss= -32.58116704997359 Validation Loss= -33.4216692444511\n",
      "Learning Rate 1.5625e-05\n",
      "Epoch: 377 Training Loss= -32.58014791663819 Validation Loss= -33.42475584476384\n",
      "Learning Rate 1.5625e-05\n",
      "Epoch: 378 Training Loss= -32.581484229455796 Validation Loss= -33.422745399891845\n",
      "Learning Rate 1.5625e-05\n",
      "Epoch: 379 Training Loss= -32.581042330682685 Validation Loss= -33.42365622199297\n",
      "Learning Rate 1.5625e-05\n",
      "Epoch: 380 Training Loss= -32.581354146295034 Validation Loss= -33.42341212500669\n",
      "Learning Rate 1.5625e-05\n",
      "Epoch: 381 Training Loss= -32.5821321544191 Validation Loss= -33.42351864144855\n",
      "Learning Rate 1.5625e-05\n",
      "Epoch: 382 Training Loss= -32.58093692196665 Validation Loss= -33.425370001419225\n",
      "Learning Rate 1.5625e-05\n",
      "Epoch: 383 Training Loss= -32.5812623511319 Validation Loss= -33.422818819820876\n",
      "Learning Rate 1.5625e-05\n",
      "Epoch: 384 Training Loss= -32.58060850871527 Validation Loss= -33.423375201204884\n",
      "Learning Rate 1.5625e-05\n",
      "Epoch: 385 Training Loss= -32.579782693072765 Validation Loss= -33.42345542198734\n",
      "Learning Rate 1.5625e-05\n",
      "Epoch: 386 Training Loss= -32.58123520832376 Validation Loss= -33.423584601171484\n",
      "Learning Rate 1.5625e-05\n",
      "Epoch: 387 Training Loss= -32.58116155842815 Validation Loss= -33.42323507071137\n",
      "Learning Rate 1.5625e-05\n",
      "Epoch: 388 Training Loss= -32.58191801886174 Validation Loss= -33.42275649125435\n",
      "Learning Rate 1.5625e-05\n",
      "Epoch: 389 Training Loss= -32.58137307889291 Validation Loss= -33.42358649183389\n",
      "Learning Rate 1.5625e-05\n",
      "Epoch: 390 Training Loss= -32.5829564844845 Validation Loss= -33.42553350142654\n",
      "Learning Rate 1.5625e-05\n",
      "Epoch: 391 Training Loss= -32.58253805336895 Validation Loss= -33.425380075937724\n",
      "Learning Rate 1.5625e-05\n",
      "Epoch: 392 Training Loss= -32.581840567307694 Validation Loss= -33.42408133253885\n",
      "Learning Rate 1.5625e-05\n",
      "Epoch: 393 Training Loss= -32.58271178812479 Validation Loss= -33.424655443395054\n",
      "Learning Rate 1.5625e-05\n",
      "Epoch: 394 Training Loss= -32.580804979010985 Validation Loss= -33.42322173435015\n",
      "Learning Rate 1.5625e-05\n",
      "Epoch: 395 Training Loss= -32.58174199568421 Validation Loss= -33.423119501291474\n",
      "Learning Rate 1.5625e-05\n",
      "Epoch: 396 Training Loss= -32.58202446788444 Validation Loss= -33.423085584075515\n",
      "Learning Rate 1.5625e-05\n",
      "Epoch: 397 Training Loss= -32.582627532627676 Validation Loss= -33.421856579175234\n",
      "Learning Rate 1.5625e-05\n",
      "Epoch: 398 Training Loss= -32.583055745197846 Validation Loss= -33.42443780416534\n",
      "Learning Rate 1.5625e-05\n",
      "Epoch: 399 Training Loss= -32.58107427366676 Validation Loss= -33.42244260393912\n",
      "Learning Rate 1.5625e-05\n",
      "Epoch: 400 Training Loss= -32.58195999552847 Validation Loss= -33.42363258273731\n",
      "Learning Rate 1.5625e-05\n",
      "Epoch: 401 Training Loss= -32.58209195229112 Validation Loss= -33.42343065458925\n",
      "Learning Rate 1.5625e-05\n",
      "Epoch: 402 Training Loss= -32.581292934209756 Validation Loss= -33.423749060240894\n",
      "Learning Rate 1.5625e-05\n",
      "Epoch: 403 Training Loss= -32.5832133930728 Validation Loss= -33.424664053617875\n",
      "Learning Rate 1.5625e-05\n",
      "Epoch: 404 Training Loss= -32.58273282540629 Validation Loss= -33.42479632696508\n",
      "Learning Rate 1.5625e-05\n",
      "Epoch: 405 Training Loss= -32.581636272642484 Validation Loss= -33.42580390615141\n",
      "Learning Rate 1.5625e-05\n",
      "Epoch: 406 Training Loss= -32.582017009616635 Validation Loss= -33.425350768435834\n",
      "Learning Rate 1.5625e-05\n",
      "Epoch: 407 Training Loss= -32.5825323141595 Validation Loss= -33.425051707945926\n",
      "Learning Rate 1.5625e-05\n",
      "Epoch: 408 Training Loss= -32.58281070631344 Validation Loss= -33.42415345138022\n",
      "Learning Rate 1.5625e-05\n",
      "Epoch: 409 Training Loss= -32.58264151445679 Validation Loss= -33.42390632489731\n",
      "Learning Rate 1.5625e-05\n",
      "Epoch: 410 Training Loss= -32.58260039250986 Validation Loss= -33.42351406963308\n",
      "Learning Rate 1.5625e-05\n",
      "Epoch: 411 Training Loss= -32.58261615410874 Validation Loss= -33.42436540524157\n",
      "Learning Rate 1.5625e-05\n",
      "Epoch: 412 Training Loss= -32.581600108906954 Validation Loss= -33.42353805595188\n",
      "Learning Rate 1.5625e-05\n",
      "Epoch: 413 Training Loss= -32.58209771313481 Validation Loss= -33.424443136634466\n",
      "Learning Rate 1.5625e-05\n",
      "Epoch: 414 Training Loss= -32.5824749933886 Validation Loss= -33.42407181412946\n",
      "Learning Rate 1.5625e-05\n",
      "Epoch: 415 Training Loss= -32.582930632618826 Validation Loss= -33.4246021616371\n",
      "Learning Rate 1.5625e-05\n",
      "Epoch: 416 Training Loss= -32.58234413611761 Validation Loss= -33.425306008628034\n",
      "Learning Rate 1.5625e-05\n",
      "Epoch: 417 Training Loss= -32.58181893643443 Validation Loss= -33.42452569680182\n",
      "Learning Rate 1.5625e-05\n",
      "Epoch: 418 Training Loss= -32.58211287986876 Validation Loss= -33.42413913061803\n",
      "Learning Rate 1.5625e-05\n",
      "Epoch: 419 Training Loss= -32.58325334335516 Validation Loss= -33.423515299302615\n",
      "Learning Rate 1.5625e-05\n",
      "Epoch: 420 Training Loss= -32.58268176074135 Validation Loss= -33.42277026099993\n",
      "Learning Rate 1.5625e-05\n",
      "Epoch: 421 Training Loss= -32.58364691523371 Validation Loss= -33.42342200459814\n",
      "Learning Rate 1.5625e-05\n",
      "Epoch: 422 Training Loss= -32.58321128823684 Validation Loss= -33.42337876485088\n",
      "Learning Rate 1.5625e-05\n",
      "Epoch: 423 Training Loss= -32.5831200160534 Validation Loss= -33.42242948666406\n",
      "Learning Rate 1.5625e-05\n",
      "Epoch: 424 Training Loss= -32.582464628474725 Validation Loss= -33.42460208929999\n",
      "Learning Rate 1.5625e-05\n",
      "Epoch: 425 Training Loss= -32.583822962025685 Validation Loss= -33.42347946339497\n",
      "Learning Rate 1.5625e-05\n",
      "Epoch: 426 Training Loss= -32.583103752433125 Validation Loss= -33.42371606361171\n",
      "Learning Rate 1.5625e-05\n",
      "Epoch: 427 Training Loss= -32.583078013901144 Validation Loss= -33.423959345495334\n",
      "Learning Rate 1.5625e-05\n",
      "Epoch: 428 Training Loss= -32.58246669334419 Validation Loss= -33.42382010011167\n",
      "Learning Rate 1.5625e-05\n",
      "Epoch: 429 Training Loss= -32.58311714447257 Validation Loss= -33.42498480355415\n",
      "Learning Rate 1.5625e-05\n",
      "Epoch: 430 Training Loss= -32.58340623727827 Validation Loss= -33.42300110633731\n",
      "Learning Rate 1.5625e-05\n",
      "Epoch: 431 Training Loss= -32.58252148715116 Validation Loss= -33.424093270309115\n",
      "Learning Rate 1.5625e-05\n",
      "Epoch: 432 Training Loss= -32.58230101683105 Validation Loss= -33.42457453871019\n",
      "Learning Rate 1.5625e-05\n",
      "Epoch: 433 Training Loss= -32.58317944051023 Validation Loss= -33.42284363865091\n",
      "Learning Rate 1.5625e-05\n",
      "Epoch: 434 Training Loss= -32.58385859678636 Validation Loss= -33.42331845937031\n",
      "Learning Rate 1.5625e-05\n",
      "Epoch: 435 Training Loss= -32.582508767439386 Validation Loss= -33.423411319080536\n",
      "Learning Rate 1.5625e-05\n",
      "Epoch: 436 Training Loss= -32.58376808380873 Validation Loss= -33.42374102963008\n",
      "Learning Rate 1.5625e-05\n",
      "Epoch: 437 Training Loss= -32.58340106871969 Validation Loss= -33.42368617851956\n",
      "Learning Rate 1.5625e-05\n",
      "Epoch: 438 Training Loss= -32.58373216730127 Validation Loss= -33.423630337292195\n",
      "Learning Rate 1.5625e-05\n",
      "Epoch: 439 Training Loss= -32.58341957383879 Validation Loss= -33.42257364355437\n",
      "Learning Rate 1.5625e-05\n",
      "Epoch: 440 Training Loss= -32.58377219192323 Validation Loss= -33.42333427721464\n",
      "Learning Rate 1.5625e-05\n",
      "Epoch: 441 Training Loss= -32.58317058281245 Validation Loss= -33.424603526199675\n",
      "Learning Rate 1.5625e-05\n",
      "Epoch: 442 Training Loss= -32.58259876361216 Validation Loss= -33.42397774955609\n",
      "Learning Rate 1.5625e-05\n",
      "Epoch: 443 Training Loss= -32.58378291027948 Validation Loss= -33.42414115605364\n",
      "Learning Rate 1.5625e-05\n",
      "Epoch: 444 Training Loss= -32.582344432340605 Validation Loss= -33.42275889907814\n",
      "Learning Rate 1.5625e-05\n",
      "Epoch: 445 Training Loss= -32.583983503264236 Validation Loss= -33.422556745592374\n",
      "Learning Rate 1.5625e-05\n",
      "Epoch: 446 Training Loss= -32.58470410354579 Validation Loss= -33.42235832126083\n",
      "Learning Rate 1.5625e-05\n",
      "Epoch: 447 Training Loss= -32.583844256795885 Validation Loss= -33.422264033861275\n",
      "Learning Rate 1.5625e-05\n",
      "Epoch: 448 Training Loss= -32.58358203535975 Validation Loss= -33.422479048625405\n",
      "Learning Rate 1.5625e-05\n",
      "Epoch: 449 Training Loss= -32.583814957651306 Validation Loss= -33.423791088798495\n",
      "Learning Rate 1.5625e-05\n",
      "Epoch: 450 Training Loss= -32.58375367794645 Validation Loss= -33.42463208318223\n",
      "Learning Rate 1.5625e-05\n",
      "Epoch: 451 Training Loss= -32.5850645432508 Validation Loss= -33.424973584855216\n",
      "Learning Rate 1.5625e-05\n",
      "Epoch: 452 Training Loss= -32.584565000043085 Validation Loss= -33.4247605103255\n",
      "Learning Rate 1.5625e-05\n",
      "Epoch: 453 Training Loss= -32.58483872118384 Validation Loss= -33.42427562884401\n",
      "Learning Rate 1.5625e-05\n",
      "Epoch: 454 Training Loss= -32.58460620554339 Validation Loss= -33.42429730236488\n",
      "Learning Rate 1.5625e-05\n",
      "Epoch: 455 Training Loss= -32.58466225292162 Validation Loss= -33.424535465375406\n",
      "Learning Rate 1.5625e-05\n",
      "Epoch: 456 Training Loss= -32.584929164597575 Validation Loss= -33.4228318201782\n",
      "Learning Rate 1.5625e-05\n",
      "Epoch: 457 Training Loss= -32.58547560652779 Validation Loss= -33.42395377885452\n",
      "Learning Rate 1.5625e-05\n",
      "Epoch: 458 Training Loss= -32.585993477951874 Validation Loss= -33.423424044323\n",
      "Learning Rate 1.5625e-05\n",
      "Epoch: 459 Training Loss= -32.58436874030526 Validation Loss= -33.422495445011386\n",
      "Learning Rate 1.5625e-05\n",
      "Epoch: 460 Training Loss= -32.583290504182145 Validation Loss= -33.42299488281016\n",
      "Learning Rate 1.5625e-05\n",
      "Epoch: 461 Training Loss= -32.584509659294866 Validation Loss= -33.423308886773846\n",
      "Learning Rate 1.5625e-05\n",
      "Epoch: 462 Training Loss= -32.584123245835556 Validation Loss= -33.42284681082516\n",
      "Learning Rate 1.5625e-05\n",
      "Epoch: 463 Training Loss= -32.58398092503421 Validation Loss= -33.424601400322466\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-3ad8b34df0fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mF3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_sd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_sd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mF1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mF2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mF3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminimize_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;31m#print(sess.run(cost, feed_dict={X:h_train[j]}))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdisplay_step\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1363\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cost_train_plt = []\n",
    "cost_test_plt = []\n",
    "start_time=time.time()\n",
    "for epoch in range(num_epochs):\n",
    "  epoch1 = epoch\n",
    "  #np.random.shuffle(h_train)\n",
    "  for j in range(num_batches):\n",
    "    g_rs1 =  np.repeat(g_rs[j],M,axis = 1)\n",
    "    F = np.reshape(G_br[j]*g_rs1, [N*M,mini_batch_size])\n",
    "    F = np.concatenate([np.real(F),np.imag(F)],axis=0)\n",
    "    F1 = np.concatenate([np.real(h_bs[j]),np.imag(h_bs[j])],axis=0)\n",
    "    F2 = g_rd[j]*g_rs[j]\n",
    "    F2 = np.concatenate([np.real(F2),np.imag(F2)],axis=0)\n",
    "    F3 = np.concatenate([np.real(h_sd[j]),np.imag(h_sd[j])],axis=0)\n",
    "    F = np.concatenate([F,F1,F2,F3],axis=0)\n",
    "    sess.run(minimize_loss,feed_dict={X:F})\n",
    "    #print(sess.run(cost, feed_dict={X:h_train[j]}))\n",
    "  if (epoch) % display_step == 0:\n",
    "      print('Learning Rate',sess.run(optimizer._lr))\n",
    "      #print('Sum_Rate:',sess.run(sum_R, feed_dict={X:h_train[j]}))\n",
    "      c = sess.run(cost, feed_dict={X:F})\n",
    "      cost_train_plt.append(np.reshape(c,[1]))\n",
    "\n",
    "      g_rs1 =  np.repeat(g_rs_t,M,axis = 1)\n",
    "      F = np.reshape(G_br_t*g_rs1, [N*M,mini_batch_size])\n",
    "      F = np.concatenate([np.real(F),np.imag(F)],axis=0)\n",
    "      F1 = np.concatenate([np.real(h_bs_t),np.imag(h_bs_t)],axis=0)\n",
    "      F2 = g_rd_t*g_rs_t\n",
    "      F2 = np.concatenate([np.real(F2),np.imag(F2)],axis=0)\n",
    "      F3 = np.concatenate([np.real(h_sd_t),np.imag(h_sd_t)],axis=0)\n",
    "      F_test = np.concatenate([F,F1,F2,F3],axis=0)\n",
    "      \n",
    "      cost_test=sess.run(cost, feed_dict={X:F_test}) \n",
    "      cost_test_plt.append(np.reshape(cost_test,[1]))\n",
    "      print(\"Epoch: \" +str(epoch) + \" Training Loss= \"+ str(c) + \" Validation Loss= \"+ str(cost_test) )\n",
    "      #print(sess.run(P,feed_dict={X:h_train[j]}))\n",
    "duration = time.time()-start_time        \n",
    "print(\"The duration for training is: \"+str(duration)) \n",
    "print(\"The cost for test data is: \" + str(cost_test))\n",
    "plt.subplot(223)\n",
    "plt.plot(cost_train_plt ,label= 'training Loss')\n",
    "plt.plot(cost_test_plt, label= 'test loss')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "plt.show()\n",
    "print(\"Optimization Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 161
    },
    "id": "ycukftHnr4pZ",
    "outputId": "13dd81c5-9474-46ee-bbe6-90846e58499c"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXEAAACQCAYAAAD+6Fs3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXxV1dXw8d+6QyYykJAQZsMUQgIEJNLaqlVwQCu0laIg6PNaea1aWy1vqVrtALXP0ypVH6q2DlWgVUEFa4uK1YJitbWCzEoEBISQQCDzfIf1/nFOMEKGCxIgZH0/n/u59+xz9jn7XMi6++57ztqiqhhjjOmYPCe7AcYYY46dBXFjjOnALIgbY0wHZkHcGGM6MAvixhjTgVkQN8aYDsx3shtwIqSmpmpGRsbJboY5Ta1Zs+aAqqa1sr67z+d7AhiGdZzM0QkDm4LB4IzRo0fvb26DThHEMzIyWL169cluhjlNiciu1tb7fL4nevToMTQtLa3U4/HYjRkmYuFwWIqLi7OLioqeACY2t431Coxpf8PS0tIqLICbo+XxeDQtLa0c51tcszpFT/y0pwoizuvSXRCfDt6oz8oan49GOAyhBvDHQDgEdeVQWQjBOojpChqGqC7Oa6/feTTe/RsOOnV9MeDxftbGcNDZl8cHoXoIBSAmySmrKIDkM46+nYE6p03+WBAvlO6ApL7OMcQDB7dBdDzEpjjn4PFBl27OMWvLnHr+WOc9qq9yzkkVqveDL9o5v1AAqoshqffRt8/hsQBujpX7f6fFDrcF8a1vwLo/Q84VkHU5hAPQUA1xKc76cAgCtYCCPw4+fs0JklVFUFcBtSXOcnJ/CFTD3rWQngP+Ls42DTVOQMia4AS+8t0Q183Zd48RTnCo2gfRCVB9APJfhvcehYxzncBSWeQEll3vOAFl6OXOcT/9txOQUgfDjrec/XmjnODZVEJPSM2Eir1OUPNGOe2p2gfRiVBf4bRfw07Q9fqhpsQJXIFqSOwDgRrnPFskEBUPDZXOMTTsFnuc9yVY57yHre4DJ/jWlTvn2yUVFKg54AbpLlCxx9kuKt5ZH6hz3uPDefzOv2NLDn+fPD6nzRp2juOL/nxbPT5IHQI3v9t6+09RRUVF3vPPP38IwIEDB/wej0dTUlKCAOvWrfsoJiamxQ+YVatWxT355JPd5s+fv7u1Y4waNSpr7dq1W45Xm7/zne/0XbZsWXJhYeEGr9d7vHZ73OXn50fl5uYOy8jIqAsEAjJixIjqRYsW7YqOjm7xPV22bFlCdHR0+KKLLqo+Hm3o3EH8vcfg1VnO680vHrk+rhvUHDw+x3rjF0e3/dbXwBvt9Fgb1ZVB/nInsDWq3PvZ68EXO4G+YDXUljrBLKU/1FdCcobzLAIJ6VCV7gRt1An0hesgKgG8Pqg+CF37QXx3Z5uYJEgZ4PSWu2eDL8r5UPDHOQE21ADBeqen7vE5dSuLIDYZyj512iTifEAE650PkLLdzna9z3Q+lPZ/6ARvr9/5AAk1OOV9v+QE1ZqD0G0A1JQ6+0o+w/mQqCyClIHut4IkiO0K5Xtg/0fQa6TzARaodT4s47o57S3b7QTspN7O+xQOO71xj895jxuqnA/Y+krn4fFCj+Ff4B//5OrRo0doy5YtHwLMnDmzV3x8fGjOnDn7GtcHAgH8fn+zdc8777ya8847r6atYxzPAB4KhVi+fHnXnj17NrzyyisJEyZMqPwi+wuHw6gq7fVh0Ldv3/otW7Z8GAwGOeecczKffPLJ5JtuuqnFHsuKFSsS4uPjQxbEv6iKws8C+Ni7Yf0i56t3o54joUua88e+/R9OMItLdQLoV28F3KC0aYkTZIZ83fkqH2qAkh3Q78tOQKnYC32/DB/91QleZ82A134CoSDUlzvHEi9knAPdBkLRRqfXd/kDTlAp2e4EqUC1E9wavyGoOr362GSnF2/MUZg0aVJGdHR0eNOmTXFjxoypmjZtWskPf/jDfvX19Z6YmJjw/Pnzd+Tm5tYvW7Ys4be//W36ypUrt82cObPX7t27o3bt2hW9d+/eqBtvvHHf3XffvR8gLi5uVE1Nzdply5YlzJkzp1dKSkogPz8/dvjw4TV/+ctfdng8HhYvXpx0xx139ImLiwufddZZVbt27YpeuXLltsPb9vLLLycMHjy49tvf/nbpM888kzJhwoTKm2++uXffvn0b7rzzzmL4/IfRT3/60/QXX3wxpaGhQb7+9a+XPfDAA3vz8/OjLrnkksxRo0ZVbdy4scsrr7yydfbs2T3Wr1/fpa6uzjNhwoTSBx54YC/QYrsqKio8119/fb8tW7bEBoNBueuuu/ZOnz69rKX31OfzceaZZ1YXFBT4AZ555pmkX//61z0DgYAnOTk5uHjx4k9qamo8CxcuTPN4PPrcc891e/DBBz8dMWJE3XXXXXdGQUFBFMD999//6cUXXxxxgO+8QTyxJ1z+oNPLOvNaOG+WEyS9zfdIgM+PPTc670eRHS/3qiavpzjPoaDz7G3lnyF1sPN8eKAWcXqypkOZ9cL6vh8XVcYdz31m9kioue/bua0OdzSnsLAw6oMPPtji8/koKSnxvP/++1v8fj9/+ctfEn784x/3ee2117YfXmfbtm0x7777bn5ZWZl36NChw2bNmlV8+NDBRx99FLtu3bpPMjIyAqNHj856/fXX488999zqW2+99Yw333xzS1ZWVsOECRP6t9SuZ555JuXKK68smTp1atkvf/nL3vX19TJt2rSS2267rV9jEH/ppZeSX3vttY+XLl2auG3btpgNGzZ8pKpceOGFg1599dX4AQMGNHz66afRf/zjH3eMGzduJ8D9999fkJ6eHgoGg3zlK18Z8t5778UOHz68rqV2/eQnP+l5wQUXVDz//PM7Dxw44M3Lyxs6ceLEisTExHBz7a6pqZE1a9Z0mTdv3m6Aiy66qGrKlClbPB4P999/f+qcOXN6PP7443uuvfba4qbfhiZMmNB/5syZ+y655JKqrVu3Rl1yySWDP/nkk82R/jt23iAOkHfd55dbC+BwbD8Qtqa14G1MO7viiitKfT7n/2BJSYn3qquu6r9z584YEdFAINDsf/aLL764LDY2VmNjY4MpKSmBPXv2+AYOHPi5HyCGDx9e3ViWk5NTs3379qiEhIRQ375967OyshoApkyZUvLEE08ccW19XV2drFixIun3v//97uTk5PDIkSOrly5dmjh16tTygwcP+nbu3OkvLCz0JSUlhQYNGhS4995701etWpWYnZ2dDVBTU+PZsmVLzIABAxp69uzZMG7cuEM92gULFqTMnz8/NRgMSnFxsX/9+vUxoVCIltr15ptvJr722mtd582b1wOgvr5etm3bFnXmmWfWNW3z7t27o7OysrILCgqizj///PIvfelLtQA7duyI+uY3v9mnuLjY39DQ4Onbt289zXjnnXcSt27dGtu4XFVV5S0vL/ckJSU1+2FxOIsixpxAx9Jjbi/x8fGHgsTtt9/e+2tf+1rl66+/vj0/Pz9q7NixQ5qr07TX7fV6CQaDRwT7SLZpydKlSxMrKyu9w4YNywGora31xMTEhKdOnVo+ceLE0j//+c/JRUVF/iuuuKIEQFW57bbbCmfNmnWg6X7y8/Oj4uLiDp3fli1boh566KH0NWvWfJSWlhaaNGlSRl1dXauXWKsqL7zwwrbc3Nxmg2+jxjHxwsJC39lnn5319NNPJ02bNq38lltu6XfrrbcWTZs2rbxxmKml43zwwQcfxcXFHdMVTHaduDGGiooKb58+fRoAHn300dTjvf8RI0bU7d69Ozo/Pz8KYPHixSnNbffss8+mPPjgg7sKCgo2FhQUbNy5c+fGf/7zn4mVlZWe6dOnlyxZsiRl2bJlyddcc00pwKWXXlrxpz/9KbW8vNwDsGPHDn9BQcERndPS0lJvbGxsOCUlJbR7927fm2++mdRWuy644IKK3/72t+nhsPNZ8M4778Qevt+mevbsGZwzZ86e++67rydAZWWlt1+/fgGA+fPnd2vcLiEhIVRZWXnoV9Zzzjmn4n/+53+6Ny6/++67rR7ncBbEjTHcfvvtRb/4xS/6DB06NDsYDB73/cfHx+v999+/a/z48YNzcnKGxsfHhxISEkJNt6msrPSsWrUqafLkyYd+PExMTAzn5eVVLVq0KCkvL6+uurrak56e3nDGGWcEAK644oqKyZMnl5x11llZmZmZ2d/61rcGlpWVHXEZytlnn107bNiwmoEDBw678sorB4wePbqqrXb9+te/3hsMBiUrKyt70KBBOXfffXebNwpMnz69rLa21rN8+fL4u+66a+/UqVMH5uTkDO3WrduhN3XSpEllL7/8ctesrKzs5cuXxz/22GO7P/jggy6ZmZnZAwcOzHnooYdaTOHQHOkM07Pl5eWp3XZv2ouIrFHVvJbWr1+/fmdubu6BltZ3Fo3jvOFwmGuvvbbf4MGD637+8583mw/E2vV569evT83Nzc1obp31xI0xJ8SDDz6YmpWVlT148OCciooK78yZM0+JD7ZTtV2Rsp64MV+Q9cRNe/vCPXERiRWRZn+tNsYYc/K0GcRFZAKwDljuLo8Ukb+2d8OMMca0LZKe+C+AMUAZgKquA1q828oYY8yJE0kQD6hq+WFlbQ6ki0iMiPxHRNaLyGYRme2W/9Et2yAiL4hIfDN1LxKRNSKy0X0e22TdmyKSLyLr3Ef3w+sbY0xnEUkQ3ywiVwNeERksIr8DIsnJWQ+MVdVcYCQwXkS+DPxQVXNVdQTwKXBLM3UPABNUdTjwX8CfDls/TVVHuo9T6lIgY041RUVF3qysrOysrKzs1NTU3O7du49oXK6rq2vzbsply5YlvP76610al++99960hx56qFtrdY5GYWGhz+fznXnvvfce1fXRJ8OkSZMyevfuPTwrKyt7yJAh2S+99FKb2efuuOOOHu3ZpkiC+PeBHJyg/AxQDtzaViV1VLmLfvehqloBICICxNJMr15V16pqY47VzUCsiERH0FZjzGEaU9Fu2bLlw2uvvbb4xhtv3Ne43Fou8UYrVqxIePvttw99Y/7xj39cfMsttxynHM2wcOHC5Nzc3Ornn3++2bs4j1Yg0Eou+ePgnnvu2bNly5YP586du/sHP/hBmzOZzJs3r2d7tieSIP51Vb1LVc9yH3fTwlxvhxMRr4isA/YDr6vqe275U0ARkAX8ro3dTAI+UNWm+QuecodSfup+GBhjjsLbb78dd9ZZZw3JyckZes455wzetWuXH+Cee+7pPnDgwJzMzMzsyy+/fEB+fn7UwoUL0/7whz+kN95hOHPmzF4/+9nP0gHGjBkz5Kabbuo9fPjwoRkZGcOWL18eD87dl5dddtmAgQMH5lx00UUDR4wYkbVq1apmszc+//zzKXPnzt29b98+//bt2/0HDx709urVa3go5NzQWVFR4enRo8eI+vp62bx5c/S55547OCcnZ+jo0aOHrF27NgacHvLVV1/db8SIEVk33XRTn5UrV8aNHDkya+jQodmjRo3KWr9+fXRb7Vq6dGniyJEjs7Kzs4deeumlAxpv5W/JuHHjqvbv338oa96FF144MCcnZ+igQYNy5s6dmwpw8803966vr/dkZWVlT5w4sT/AI488kjJ8+PChWVlZ2VdfffUZX/QO2UgSYN0JPB9B2RFUNQSMFJGuwIsiMkxVN6nqdSLixQngVwFPNVdfRHKA3wAXNymepqoFIpIALAGuARY2U/cG4AaAfv0sZas5Rfzle33Z/+FxTUVL9+wavvlwxIm1VJUf/OAH/V5++eVtvXr1Cj7++OPJP/rRj3o///zzO+fNm9dj165dG2NjY/XAgQPe1NTU0OGpU//+978nNt1fMBiUjRs3frR48eKkOXPm9Bo/fvzH9913X1rXrl1D27dv3/z+++/HnH322TnNtWXbtm3+4uJi/wUXXFAzceLE0oULF6bMnj1739ChQ2saJ4RYvHhx0te+9rXy6OhonTFjxhmPPfbYruHDh9evWLGiy0033dTv3//+98cQWWrdltpVWFjo++///u+eq1at+jgxMTF811139fjlL3+ZPnfu3MKW3sclS5YkXXjhhYdSBDz99NM709PTQ1VVVTJq1Kjs6dOnlz7yyCMF8+fP7944KccHH3wQ88ILL6SsXr16S3R0tE6fPr3fH/7wh25f5JtNi0FcRC4FLgN6i8i8JqsSgaP66FDVMhFZCYwHNrllIRFZBPyYZoK4iPQBXgSuVdXtTfZV4D5XisgzOFfOHBHEVfUx4DFwbvY5mvYaczqrr6/3bN26NXbs2LGZ4Mx8k5aWFgAYMmRI7be+9a3+EydOLJs2bVqLEyA0NXny5FKAr3zlK9WzZs2KAnj33Xfjb7311v0AZ511Vl1mZmazswMtXLgwZeLEiaUA11xzTcn111+fMXv27H2TJ08uffbZZ5MnTJhQ+dxzz6XcfPPNxeXl5Z61a9fGT548eWBj/YaGhkPfxCNJrdtSu958880u27dvjxkzZkwWQCAQkMb8Koe7++67+8yePbv3vn37/CtWrDg0o9FvfvOb9JdffrkrQFFRkX/z5s0xPXr0+NzkDsuXL0/YtGlTXG5u7lCAuro6T/fu3b9QV7y1nvheYDXO0MmaJuWVwA/b2rGIpOFc2VImIrHARcC9IjJIVbe5wyATgSOmdXJ77i8Dd6jqO03KfUBXVT0gIn7gcuCNttpizCnjKHrM7UVVGTRoUO26deuO+NtbuXLl1ldffTXhpZdeSpo7d27P/Pz8NicnaBxX9/l8hEKhoxreXLJkSUpxcbF/6dKlKQD79+/3b9y4MbpxQoh9+/Z5N23aFDdhwoSKiooKT0JCQrCxV3u4Y0mt20hVOeeccyr+9re/7Wirzffcc8+e6667rvRXv/pV9xkzZmRs3rz5o2XLliW89dZbCatXr96SkJAQHjNmzJDa2tojhmNUVSZPnnzw4YcfLmj73YlMi2M+qrpeVRcAg1R1QZPHUlUtjWDfPYGVIrIBeB94HScwLxCRjcBGd5s5ACIyUUTmuHVvAQYBPzvsUsJo4DV3n+uAAuDxYzlxYzqr6OjocElJie+NN97oAs5kB6tXr44JhUJs3749asKECZUPP/xwgTs5gffw1KmROPvss6sWLVqUDLBmzZqYjz/++Ij0qhs2bIiurq727t+/f0Nj6tlbbrmlaMGCBSlJSUnhESNGVH/3u9/tN27cuHKfz0dKSkq4T58+DU8++WQyON8g/vWvfzWbtrWl1Lottev888+vXr16dfymTZui3fqeDRs2tHoxxZ133rk/HA7LkiVLEsvKyrxJSUmhhISE8Nq1a2PWr19/6Goen8+n9fX1AjB+/PiKZcuWJTemy923b5/3448/jjqa9/ZwkfywmeFez/2hiHzS+GirkqpuUNVRqjpCVYep6hxVDavqV1V1uFs2rfFqFVX9q6r+zH19j6p2aXIZ4UhV3a+q1ao62t1njqre6o67G2Mi5PF4WLRo0fY77rijz5AhQ7JzcnKy33rrrfhgMChXX311/8zMzOxhw4Zlz5gxY39qamro8NSpkRxj1qxZxQcPHvQNHDgw58477+w9aNCguuTk5M/9rS5YsCDlsssu+1yHcMqUKaWNvfIrr7yy9KWXXkqZOnXqoUmHn3322U+eeuqp1CFDhmQPHjw4Z8mSJV2bO35LqXVbalevXr2Cjz766M4pU6YMyMzMzM7Ly8vauHFjTFvv4+2337537ty5PSZNmlQeDAZlwIABObNmzeqdm5t7aBhl2rRpxUOHDs2eOHFi/9GjR9fdfffdBePGjcvMzMzMHjt2bObu3bvbmFKsdW0mwBKRfwI/Bx4AJgDXAZ7GgNsRWAIs054sAdaRgsEgDQ0NEhcXp5s3b46++OKLM7dv374pkksaO2O72tJaAqxIrk6JVdV/iIio6i7gFyKyBugwQdwYc2JVVlZ6zj333CGBQEBUlQceeGDXqRAoT9V2fRGRBPF6EfEAW0XkFpxx6Ii+UhljOqfk5OTwpk2bPjrZ7TjcqdquLyKSMfFbgTjgB8BonOuyr23PRhljjIlMmz1xVX3ffVkFNN6kMwV4rz0bZsxpJBwOh8Xj8XTor+3m5AiHwwKEW1rfYk9cRBJF5E4ReUhELhbHLcA24Mp2aKsxp6tNxcXFSe4fozERC4fDUlxcnIR7k2RzWuuJ/wkoBf4FzAB+AgjwLTenuDEmAsFgcEZRUdETRUVFw7B5bc3RCQObgsHgjJY2aC2ID3BTwSIiTwCFQD9VrTu+bTTm9DZ69Oj9RJg0zpij1Vqv4FA+R/eGmj0WwI0x5tTSWk88V0Qq3NeCk9O7wn2tqprYclVjjDEnQotBXFWPKleCMcaYE89+ZDHGmA7MgrgxxnRgFsSNMaYDiyR3CuDc/NN0e1UtaWVzY4wxJ0CbQVxEvgvMBur4bGZ6BQa0Y7uMMcZEIJLhlB8Bw1Q1Q1X7u482A7iIxIjIf0RkvYhsFpHZbvkf3bIN7mQTzWZEdG/53yYi+SJySZPy8W7ZNhG5I9ITNcaY01EkQXw70Owkp22oB8aqai4wEhgvIl8Gfqiquao6AvgUZyq2zxGRbJwkWzk4kys/IiJeN/nWw8ClQDYw1d3WGGM6pUjGxO8E3hWR93ACMwCq+oPWKqkzZVDjbNF+96GN07G5EyXH8tkQTVPfABapaj2wQ0S24cxqD7BNVT9x97HI3bbZiVONMeZ0F0kQfxRYgTOxcYvpEJvj9pzX4Ex6/LCqvueWPwVchhN8/18zVXsD/26yvMctA9h9WPmXjqZNxhhzOokkiPtVdeax7NzNuTJSRLoCL4rIMFXdpKqNecl/B1wFPHUs+2+NiNwA3ADQr1+/4717Y4w5JUQyJv6qiNwgIj1FJKXxcTQHUdUyYCXO+HZjWQhYBExqpkoB0LfJch+3rKXy5o75mKrmqWpeWlra0TTXGGM6jEiC+FTccXGcoZE1QJtTx4tImtsDR0RigYuAfBEZ5JYJTnrOLc1U/yswRUSiRaQ/MBj4D/A+MFhE+otIFM6Pn3+N4ByMMea0FMn0bP2Pcd89gQXusIkHeA54GXjbvXFIgPXATQAiMhHIU9WfqepmEXkOZ8w8CHzP7bnjzi70GuAFnlTVzcfYPmOM6fDEuYiklQ1Emp0UWVUXtkuL2kFeXp6uXt3mlwdjjomIrFHVvJPdDtM5RfLD5llNXscA44APgA4TxI0x5nQVyXDK95suu+Pci9qtRcYYYyJ2LFkMq4FjHSc3xhhzHEWSAOtvfHZXpRcYivMjpTHGmJMskjHxuU1eB4FdqrqnndpjjDHmKEQyJv4WgIh0A87DSUlrQdwYY04BLY6Ji8gyERnmvu4JbAK+A/xJRG47Qe0zxhjTitZ+2Oyvqpvc19cBr6vqBJyEU99p95YZY4xpU2tBPNDk9TjgFQBVreQosxkaY4xpH62Nie8Wke/jjH+fCSyHQ3lQ/CegbcYYY9rQWk/8epyZdf4PcJWbiRDgy7RD6lhjjDFHr8WeuKruB25spnwlTlpZY4wxJ9mx3LFpjDHmFGFB3BhjOjAL4sYY04FFkjtlXjPF5cBqVX3p+DfJGGNMpCLpiccAI4Gt7mMEztyW14vIgy1VEpEYEfmPiKwXkc0iMtstf1pE8kVkk4g8KSJHXK4oIheIyLomjzoR+aa7br6I7GiybuQxnLcxxpwWIkmANQL4apPp0X4PvA2cA2xspV49MFZVq9xA/U8ReRV4GpjubvMMMAP4fdOK7hUwI93jpQDbgL832WSWqr4QQduNMea0FklPPBmIb7LcBUhxg3p9S5XUUeUu+t2Hquor7jrFmfy4TxvH/zbwqqrWRNBWY4zpVCIJ4vcC60TkKRGZD6wF7hORLsAbrVUUEa+IrAP24+Reea/JOj9wDe6doK2YAjx7WNmvRGSDiDwgItERnIMxxpyW2pwoGQ5lMRzjLr6vqnuP6iDOlG4vAt9vTKolIo8D1araYkZE97gbgF6qGmhSVgREAY8B21V1TjN1bwBuAOjXr9/oXbt2HU2TjYmYTZRsTqY2e+LuzD7nA2+o6ktHG8AB3Fv2VwLj3X3+HEgDZrZR9UrgxcYA7u6r0B2Nqce5/X9McxVV9TFVzVPVvLS0tKNtsjHGdAiRDKfMBc4FPhSRF0Tk2yIS01YlEUlze+CNSbMuAraIyAzgEmCqqraVDXEqhw2luD1xRESAb+LkOTfGmE4p0pl93hIRLzAW+L/Ak0BiG1V7Agvceh7gOVVdJiJBYBfwLycOs1RV54hIHnCjqs4AEJEMoC/w1mH7fVpE0gAB1tFMfhdjjOksIrnEsLEnPQG4Cict7YK26qjqBmBUM+XNHlNVV+Ncbti4vBPo3cx2YyNpszHGdAaR3LH5HM6483LgIeCtCIZBjDHGnACR9MT/iDN+3XizzzkiMlVVv9e+TTPGGNOWSMbEXxORUSIyFedqkR3A0nZvmTHGmDa1GMRFJBPn6pCpwAFgMc515RecoLYZY4xpQ2s98S04OVIuV9VtACLywxPSKmOMMRFp7TrxK4BCYKWIPC4i43Au6zPGGHOKaDGIq+pfVHUKkIVzt+VtQHcR+b2IXHyiGmiMMaZlbd6xqarVqvqMqk7AyTi4Fri93VtmjDGmTUc1PZuqlro5Sca1V4OMMcZEzubYNMaYDiyi2+6NOV5UlfpgGK9H8Hs91DaEiI3yHrE+xu9FVSmpbiCsUFkXIDUhmvd3lNA1zk9YIT0hBr9PqAuESY2PorQ6QEFZLQkxPqrrg4gIwbCzr/rAZzcZV9YFCKsS7feCQlltA16Phyivh4q6AOGw0hAKc7Cqger6IOmJMfRNiWP8sB4n4y0zplUWxDuAUFjxelq/MKguECLG76UhGCbK5wSj6vogMT4v24urCIaVyrogAD6PsK+ijiE9EthUUE7PpFgq6gKU1wYorQlwRkocIVW27a8i1u8lOc5PVX0Qv9fDpyU1xEf7UKA+EEJE+ORANQWlNfRP7cL+ynp2HaxhRJ8kPi2pIcrroaYhRHltgKRYP2FVCsvrAIiL8lLTECIh2kdSnJ8DVfXUucHW5xGC4bZz3bc3r0cIhZXM9N29aegAAAlfSURBVHgL4uaUZEH8BAiHFU8bQRigoi6AzyMs21DI39bvpbo+SGVdkK37q4iP9jG8dxINoTBrdpXSu2ssvZNj2VJYQd+UODbvrQBABGJ8XmoDoXY5l65xfmoaQqAQVsXnFQamxbO9uJrdpbVkdIujvDbAe5+UEAyH6ZMcB0Cf5Fh6d42lpiFElM9D3+Q4usVHsWFPOSP6JFEfCBMf46OiNkBCjJ+iilq6xkWxu6SGzPQEMtPj2VtWR1lNA7l9u+LzCGGFQMjpaZfXBkiM9dMrKYZAKEwoDA2hEAnRfrweIcb/WW8/yuch1OQDwiMQH+Ojuj5EKKx0T4imS7SP5Dg/wbByoKoeN+OmMaccC+IRCIeVijqnJ1kfDPPX9Xu5cGg6tYEQ/9p+kPhoLyXVAUprGrjvtfxD9Sbk9mLZhr2oQv/ULlwwpDs9k2IIhpXXPywiIcaPAntKa6ioDXCgquFzx02I8ZHVIwGvR6iqD/LJgSqifU4wKiirJcrnIatnIvsq6sjumUhWjwTSk2IoqWoguUsUReW19EiK5UBVPYO7xzM4PZ73Pikhxu8lu1ci+yvq8Hk99EuJo7IuQGKMH49HEKA+GCY1PpruidFs2FOG1+NhRO8kkrtEEQorqkrQ/Ybg9x7500oorHiEDh/8fF4OfRAZcyqKaHq2ji4vL09Xr17d7Lq6QAi/13MoKD37n0/xeT28s/UAdcEQb+YXf257j8Dx/Jaf0iWK7gnR7CmtJazK8N5JdIuP4lffHE5yl6hD2zUdUimraSAp1t/hA+TpwqZnMydTp+6JH6yq59L/fZv9lfUR1zl3cBq1gRDrdpcxJiOFL/VPYVS/ZDbtLefp93aR0a0LP7s8m/6pXfCIEAiHqagNktIlitc/3EeXaC9DeyZysKqBGL/TC44kGDcdE+8aF9XKlsaYzqTdeuLuFG6rgGicD4sXVPXnIvI0kAcEgP8A3206h2aT+iFgo7v4qapOdMv7A4uAbsAa4BpVbTi8flMt9cTH/OqNzwXwWL+Xswd2IyHGx+gzkrnmy2dQURskLtrb7JCBMWA9cXNytWdPvB4Yq6pVIuIH/ikirwJPA9PdbZ7Bmc3n983Ur1XVkc2U/wZ4QFUXicgfgOtbqN+q0uoG9lfW07trLK/cei4VtQH6phw59pkU5z/aXRtjzAnTbkFcnS5+lbvodx+qqq80biMi/8G5lT8i7uTIY4Gr3aIFwC84hiCe3CWKTbMvIaxKYoyfpFgL1saYjqddxwhExCsi64D9wOuq+l6TdX7gGpxp35oTIyKrReTfIvJNt6wbUKaqQXd5D83Mwxmp+GgfiTEWvI0xHVe7/rDpTuk2UkS6Ai+KyDBV3eSufgRYpapvt1D9DFUtEJEBwAoR2QiUR3psEbkBuAGgX79+x34SxhhzCjshv9apahlOOtvxACLycyANmNlKnQL3+RPgTWAUcBDoKiKNHz59gIIW6j+mqnmqmpeWlnaczsQYY04t7Xl1ShoQUNUyEYkF/o7zo2QP4DvAOFWtbaFuMlCjqvUikgr8C/iGqn4oIs8DS5r8sLlBVR9poy3FwK4WVqfiTD/XWXSm8z1R53qGqlpPwZwU7RnER+D88OjF6fE/p6pzRCSIE1Ar3U2XuuV5wI2qOkNEvgI8CoTdug+q6h/d/Q7AucQwBSe3+XRVjfxC7yPbubozXR7Wmc63M52r6bw6xR2brelsf+id6Xw707mazsvuYDHGmA7Mgjg8drIbcIJ1pvPtTOdqOqlOP5xijDEdmfXEjTGmA+u0QVxExotIvohsE5E7TnZ7jgcR6SsiK0XkQxHZLCK3uuUpIvK6iGx1n5PdchGRee57sEFEzjy5Z3D03LuC14rIMne5v4i8557TYhGJcsuj3eVt7vqMk9luY46XThnERcQLPAxcCmQDU0Uk++S26rgIAv9PVbOBLwPfc8/rDuAfqjoY+Ie7DM75D3YfN3AMOWhOAbcCHzVZbkyQNggoxUmQhvtc6pY/4G5nTIfXKYM4MAbYpqqfuGlsFwHfOMlt+sJUtVBVP3BfV+IEt94457bA3WwB0JiL5hvAQnX8G+du2J4nuNnHTET6AF8HnnCXGxOkveBucvi5Nr4HLwDjxGbVMKeBzhrEewO7myx/oURapyJ3uGAU8B6QrqqF7qoiIN193dHfhweBH+PcFAatJ0g7dK7u+nJ3e2M6tM4axE9rIhIPLAFuU9WKpuvcFMEd/pIkEbkc2K+qa052W4w5mTrr9GwFQN8myy0m0upo3BS/S4CnVXWpW7xPRHqqaqE7XLLfLe/I78NXgYkichkQAyQC/4ubIM3tbTc9n8Zz3eMmUEvCSahmTIfWWXvi7wOD3SsZooApwF9Pcpu+MHeM94/AR6p6f5NVfwX+y339X8BLTcqvda9S+TJQ3mTY5ZSmqneqah9VzcD591uhqtNwsmV+293s8HNtfA++7W7f4b+RGNNpb/Zxe3AP4iToelJVf3WSm/SFicg5wNs4c5M2jhP/BGdc/DmgH07ysStVtcQN+g/hpAiuAa5T1SMnIz3Ficj5wI9U9fKWEqS5c77+Ced3ghJgipvm2JgOrdMGcWOMOR101uEUY4w5LVgQN8aYDsyCuDHGdGAWxI0xpgOzIG6MMR1YZ73Zp1MQkRDO5YaNFqnqr09We4wxx59dYngaE5EqVY0/2e0wxrQfG07phERkp4jcKyIbReQ/IjLILc8QkRVubvF/iEg/tzxdRF4UkfXu4ysi0kVEXnaXN4nIVSf3rIzpnCyIn95iRWRdk0fTQFuuqsNx7th80C37HbBAVUcATwPz3PJ5wFuqmgucCWzGuctzr6rmquowYPmJOCFjzOfZcMpprKXhFBHZCYxV1U/chFlFqtpNRA4APVU14JYXqmqqiBQDfVS1vsk+MoG/A4uBZar69gk5KWPM51hPvPPSFl5HVln1Y5xe+UbgHhH52fFqmDEmchbEO6+rmjz/y339Lk5GQIBpOMm0wJnS7SY4NKdlkoj0AmpU9c/AfTgB3RhzgtlwymmsmUsMl6vqHe5wymKcOTbrgamquk1EzgCeAlKBYpyshp+KSDrwGDAACOEE9ESc4B0GAsBNHTEDojEdnQXxTsgN4nmqeuBkt8UY88XYcIoxxnRg1hM3xpgOzHrixhjTgVkQN8aYDsyCuDHGdGAWxI0xpgOzIG6MMR2YBXFjjOnA/j/ks/X1UvkrvwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = -np.array(cost_train_plt)\n",
    "b = -np.array(cost_test_plt)\n",
    "plt.subplot(223)\n",
    "plt.plot(a ,label= 'Training Average Rate')\n",
    "plt.plot(b, label= 'Testing Average Rate')\n",
    "plt.xlabel('Epocs')\n",
    "plt.ylabel('Avg Sum Rate')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad = 0.)\n",
    "plt.show()\n",
    "# graph_folder = '/content/drive/MyDrive/DL_prejects/Graph_data'\n",
    "# savemat(graph_folder+'/M8_tau_learned_train_rate.mat',{'Train_rate': a})\n",
    "# savemat(graph_folder+'/M8_tau_learned_Valid_rate.mat',{'Valid_rate': b})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UDqiGdrCoWEB"
   },
   "source": [
    "## Save Trained Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yFpf9xkszdYT"
   },
   "outputs": [],
   "source": [
    "trained_param_folder = '/content/drive/MyDrive/Trainedwieghts'\n",
    "Save_train_param = True \n",
    "if Save_train_param== True:\n",
    "  savemat(trained_param_folder+'/W1.mat',{'W1': sess.run(W1)})\n",
    "  savemat(trained_param_folder+'/W2.mat',{'W2': sess.run(W2)})\n",
    "  savemat(trained_param_folder+'/W3.mat',{'W3': sess.run(W3)})\n",
    "  savemat(trained_param_folder+'/W4.mat',{'W4': sess.run(W4)})\n",
    "  savemat(trained_param_folder+'/W5.mat',{'W5': sess.run(W5)})\n",
    "  savemat(trained_param_folder+'/W6.mat',{'W6': sess.run(W6)})\n",
    "\n",
    "  savemat(trained_param_folder+'/b1.mat',{'b1': sess.run(b1)})\n",
    "  savemat(trained_param_folder+'/b2.mat',{'b2': sess.run(b2)})\n",
    "  savemat(trained_param_folder+'/b3.mat',{'b3': sess.run(b3)})\n",
    "  savemat(trained_param_folder+'/b4.mat',{'b4': sess.run(b4)})\n",
    "  savemat(trained_param_folder+'/b5.mat',{'b5': sess.run(b5)})\n",
    "  savemat(trained_param_folder+'/b6.mat',{'b6': sess.run(b6)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "j997S9ZwwY1F",
    "outputId": "5bdffc45-81e0-48d8-a63f-5d38c6535290"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'4'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hHpnsoRSz_ME"
   },
   "source": [
    "## Load Trained Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "usUG4lgsnNeS",
    "outputId": "741e815f-b353-485b-c510-16cc63ee8ce4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.005674839019775391\n",
      "-33.00000949863198\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.31654362196226293"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "mini_batch_size = 10**3\n",
    "np.random.seed(112)\n",
    "\n",
    "h_bs_test = (np.random.randn( M,mini_batch_size)+1j*np.random.randn( M,mini_batch_size))*np.sqrt(b_bs/2)\n",
    "h_is_test = (np.random.randn( 1,mini_batch_size)+1j*np.random.randn( 1,mini_batch_size))*np.sqrt(b_is/2)\n",
    "h_id_test = (np.random.randn( 1,mini_batch_size)+1j*np.random.randn( 1,mini_batch_size))*np.sqrt(b_id/2)\n",
    "h_sd_test = (np.random.randn( 1,mini_batch_size)+1j*np.random.randn( 1,mini_batch_size))*np.sqrt(b_sd/2)\n",
    "g_rs_test = (np.random.randn( N,mini_batch_size)+1j*np.random.randn( N,mini_batch_size))*np.sqrt(b_rs/2)\n",
    "g_sr_test = (np.random.randn( N,mini_batch_size)+1j*np.random.randn( N,mini_batch_size))*np.sqrt(b_rd/2)\n",
    "g_rd_test = (np.random.randn( N,mini_batch_size)+1j*np.random.randn( N,mini_batch_size))*np.sqrt(b_rd/2)\n",
    "g_ir_test = (np.random.randn( N,mini_batch_size)+1j*np.random.randn( N,mini_batch_size))*np.sqrt(b_ir/2)\n",
    "G_br_test = (np.random.randn( N,M*mini_batch_size)+1j*np.random.randn( N,M*mini_batch_size))*np.sqrt(b_br/2)\n",
    "\n",
    "tic = time.time()\n",
    "g_rs1 =  np.repeat(g_rs_test,M,axis = 1)\n",
    "F = np.reshape(G_br_test*g_rs1, [N*M,mini_batch_size])\n",
    "F = np.concatenate([np.real(F),np.imag(F)],axis=0)\n",
    "F1 = np.concatenate([np.real(h_bs_test),np.imag(h_bs_test)],axis=0)\n",
    "F2 = g_rd_test*g_rs_test\n",
    "F2 = np.concatenate([np.real(F2),np.imag(F2)],axis=0)\n",
    "F3 = np.concatenate([np.real(h_sd_test),np.imag(h_sd_test)],axis=0)\n",
    "F = np.concatenate([F,F1,F2,F3],axis=0)\n",
    "\n",
    "Cost = sess.run(cost, feed_dict={X:F})\n",
    "toc = time.time() \n",
    "print(toc-tic)\n",
    "print(Cost)\n",
    "Cost = sess.run(tau, feed_dict={X:F})\n",
    "np.mean(Cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lKZtg7Yz5M3z",
    "outputId": "19cf78c2-a4d6-40e8-d0db-dd952fe30182"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0018897230625152589\n",
      "-23.704075023649054\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.40101115901751966"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mini_batch_size = 10**0\n",
    "\n",
    "t = np.zeros((1000,1))\n",
    "for i in range(1000):\n",
    "    mini_batch_size = 10**0\n",
    "    h_bs_test = (np.random.randn( M,mini_batch_size)+1j*np.random.randn( M,mini_batch_size))*np.sqrt(b_bs/2)\n",
    "    h_is_test = (np.random.randn( 1,mini_batch_size)+1j*np.random.randn( 1,mini_batch_size))*np.sqrt(b_is/2)\n",
    "    h_id_test = (np.random.randn( 1,mini_batch_size)+1j*np.random.randn( 1,mini_batch_size))*np.sqrt(b_id/2)\n",
    "    h_sd_test = (np.random.randn( 1,mini_batch_size)+1j*np.random.randn( 1,mini_batch_size))*np.sqrt(b_sd/2)\n",
    "    g_rs_test = (np.random.randn( N,mini_batch_size)+1j*np.random.randn( N,mini_batch_size))*np.sqrt(b_rs/2)\n",
    "    g_sr_test = (np.random.randn( N,mini_batch_size)+1j*np.random.randn( N,mini_batch_size))*np.sqrt(b_rd/2)\n",
    "    g_rd_test = (np.random.randn( N,mini_batch_size)+1j*np.random.randn( N,mini_batch_size))*np.sqrt(b_rd/2)\n",
    "    g_ir_test = (np.random.randn( N,mini_batch_size)+1j*np.random.randn( N,mini_batch_size))*np.sqrt(b_ir/2)\n",
    "    G_br_test = (np.random.randn( N,M*mini_batch_size)+1j*np.random.randn( N,M*mini_batch_size))*np.sqrt(b_br/2)\n",
    "    \n",
    "    tic = time.time()\n",
    "    g_rs1 =  np.repeat(g_rs_test,M,axis = 1)\n",
    "    F = np.reshape(G_br_test*g_rs1, [N*M,mini_batch_size])\n",
    "    F = np.concatenate([np.real(F),np.imag(F)],axis=0)\n",
    "    F1 = np.concatenate([np.real(h_bs_test),np.imag(h_bs_test)],axis=0)\n",
    "    F2 = g_rd_test*g_rs_test\n",
    "    F2 = np.concatenate([np.real(F2),np.imag(F2)],axis=0)\n",
    "    F3 = np.concatenate([np.real(h_sd_test),np.imag(h_sd_test)],axis=0)\n",
    "    F = np.concatenate([F,F1,F2,F3],axis=0)\n",
    "\n",
    "    Cost = sess.run(cost, feed_dict={X:F})\n",
    "    toc = time.time() \n",
    "    t[i] = toc-tic\n",
    "print(np.mean(t))\n",
    "print(Cost)\n",
    "Cost = sess.run(tau, feed_dict={X:F})\n",
    "np.mean(Cost)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "RR_0_996_Leky_relu_BN_IRSNet_tau_learn_M2_N16_BN1_varified_cost.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
