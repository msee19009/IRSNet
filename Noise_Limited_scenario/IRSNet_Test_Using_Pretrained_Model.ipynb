{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test IRS-Net on Test Data Using Pretrained Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import savemat\n",
    "from scipy.io import loadmat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Parameters for Noise Limited Scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 1\n",
    "P_b = 10        #Watt or 40 dBm\n",
    "P_i = 0         #Watt or 40 dBm\n",
    "sigma_n = np.sqrt(3.98*10**-14)\n",
    "M = 2           #Power Beam\n",
    "N = 8           #of IRS elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Path Loss Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 2.57\n",
    "alpha_irs = 2.2;\n",
    "Lc = 10**-3;\n",
    "\n",
    "d_br = 20;\n",
    "d_rs = 15;\n",
    "d_sb = 25;\n",
    "d_is = 15;\n",
    "d_id = 30\n",
    "d_rd = 15;\n",
    "d_sd = 25;\n",
    "d_ir = 15;\n",
    "\n",
    "b_bs = Lc*d_sb**(-alpha);\n",
    "b_is = Lc*d_is**(-alpha);\n",
    "b_id = Lc*d_id**(-alpha);\n",
    "b_sd = Lc*d_sd**(-alpha_irs);\n",
    "b_rs = Lc*d_rs**(-alpha_irs);\n",
    "b_rd = Lc*d_rd**(-alpha_irs);\n",
    "b_ir = Lc*d_ir**(-alpha_irs);\n",
    "b_br = Lc*d_br**(-alpha_irs);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pretained Weights\n",
    "To load pretrained parameters,\n",
    "i) Download pretrained parameters from this repository.\n",
    "ii) Un-zip folder.\n",
    "iii) Give directory path, where you unzipped pretrained weights, in folowing cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Ahsann Mehmood\\\\Desktop\\\\IRS revision\\\\Final revision\\\\Noise_limited_scenario\\\\M2N8Pi0'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if M==2 and N==8:\n",
    "    trained_param_folder = r'C:\\Users\\Ahsann Mehmood\\Desktop\\IRS revision\\Final revision\\Noise_limited_scenario\\M2N8Pi0'\n",
    "elif M==4 and N==8:\n",
    "    trained_param_folder = r'C:\\Users\\Ahsann Mehmood\\Desktop\\IRS revision\\Final revision\\Noise_limited_scenario\\M4N8Pi0'\n",
    "elif M==8 and N==8:\n",
    "    print(N,M)\n",
    "    trained_param_folder = r'C:\\Users\\Ahsann Mehmood\\Desktop\\IRS revision\\Final revision\\Noise_limited_scenario\\M8N8Pi0'\n",
    "elif M==2 and N==16:\n",
    "    trained_param_folder = r'C:\\Users\\Ahsann Mehmood\\Desktop\\IRS revision\\Final revision\\Noise_limited_scenario\\M2N16Pi0'\n",
    "elif M==4 and N==16:\n",
    "    trained_param_folder = r'C:\\Users\\Ahsann Mehmood\\Desktop\\IRS revision\\Final revision\\Noise_limited_scenario\\M4N16Pi0'\n",
    "elif M==8 and N==16:\n",
    "    trained_param_folder = r'C:\\Users\\Ahsann Mehmood\\Desktop\\IRS revision\\Final revision\\Noise_limited_scenario\\M8N16Pi0'\n",
    "elif M==2 and N==32:\n",
    "    trained_param_folder = r'C:\\Users\\Ahsann Mehmood\\Desktop\\IRS revision\\Final revision\\Noise_limited_scenario\\M2N32Pi0'\n",
    "elif M==4 and N==32:\n",
    "    trained_param_folder = r'C:\\Users\\Ahsann Mehmood\\Desktop\\IRS revision\\Final revision\\Noise_limited_scenario\\M4N32Pi0'\n",
    "elif M==8 and N==32:\n",
    "    trained_param_folder = r'C:\\Users\\Ahsann Mehmood\\Desktop\\IRS revision\\Final revision\\Noise_limited_scenario\\M8N32Pi0'\n",
    "\n",
    "W1 = loadmat(trained_param_folder+'/W1.mat')\n",
    "W2 = loadmat(trained_param_folder+'/W2.mat')\n",
    "W3 = loadmat(trained_param_folder+'/W3.mat')\n",
    "W4 = loadmat(trained_param_folder+'/W4.mat')\n",
    "W5 = loadmat(trained_param_folder+'/W5.mat')\n",
    "W6 = loadmat(trained_param_folder+'/W6.mat')\n",
    "    \n",
    "b1 = loadmat(trained_param_folder+'/b1.mat')\n",
    "b2 = loadmat(trained_param_folder+'/b2.mat')\n",
    "b3 = loadmat(trained_param_folder+'/b3.mat')\n",
    "b4 = loadmat(trained_param_folder+'/b4.mat')\n",
    "b5 = loadmat(trained_param_folder+'/b5.mat')\n",
    "b6 = loadmat(trained_param_folder+'/b6.mat')\n",
    "trained_param_folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(243, 243)\n",
      "(18, 54)\n"
     ]
    }
   ],
   "source": [
    "print(W3['W3'].shape)\n",
    "print(W6['W6'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a Placeholder to feed feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "X = tf.placeholder(tf.float64, shape=(2*(N*M+M+N+1),None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z1 = tf.add(tf.matmul(W1['W1'],X),b1['b1'])                     # Linear Layer\n",
    "mean, variance = tf.nn.moments(Z1, axes= [1] ,keepdims=True)\n",
    "Z1_BN = tf.nn.batch_normalization(Z1, mean=mean, variance= variance, scale=1.0, offset= None, variance_epsilon=10**-5) # Batch Normalization Layer\n",
    "A1 = tf.nn.relu(Z1_BN)      # Relu Layer\n",
    " \n",
    "Z2 = tf.add(tf.matmul(W2['W2'],A1),b2['b2'])                     # Linear Layer\n",
    "mean, variance = tf.nn.moments(Z2, axes= [1] ,keepdims=True)\n",
    "Z2_BN = tf.nn.batch_normalization(Z2, mean=mean, variance= variance, scale=1.0, offset= None, variance_epsilon=10**-5) # Batch Normalization Layer\n",
    "A2 = tf.nn.relu(Z2)\n",
    " \n",
    "Z3 = tf.add(tf.matmul(W3['W3'],A2),b3['b3'])                     # Linear Layer\n",
    "mean, variance = tf.nn.moments(Z3, axes= [1] ,keepdims=True)\n",
    "Z3_BN = tf.nn.batch_normalization(Z3, mean=mean, variance= variance, scale=1, offset= None, variance_epsilon=10**-5) # Batch Normalization Layer\n",
    "A3 = tf.nn.relu(Z3)\n",
    "\n",
    "Z4 = tf.add(tf.matmul(W4['W4'],A3),b4['b4'])                     # Linear Layer\n",
    "mean, variance = tf.nn.moments(Z4, axes= [1],keepdims=True)\n",
    "Z4_BN = tf.nn.batch_normalization(Z4, mean=mean, variance= variance, scale=1, offset= None, variance_epsilon=10**-5) # Batch Normalization Layer\n",
    "A4 = tf.nn.relu(Z4) \n",
    "\n",
    "Z5 = tf.add(tf.matmul(W5['W5'],A4),b5['b5'])                     # Linear Layer\n",
    "mean, variance = tf.nn.moments(Z5, axes= [1],keepdims=True)\n",
    "Z5_BN = tf.nn.batch_normalization(Z5, mean=mean, variance= variance, scale=1, offset= None, variance_epsilon=10**-5) # Batch Normalization Layer\n",
    "A5 = tf.nn.leaky_relu(Z5_BN,0.) \n",
    "\n",
    "Z6 = tf.add(tf.matmul(W6['W6'],A5),b6['b6'])                     # Linear Layer\n",
    "theta1 = tf.nn.sigmoid(Z6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "real = tf.zeros([N,tf.shape(X)[1]],dtype=tf.float64)\n",
    "log_base = tf.constant(2,dtype=tf.float64)\n",
    "etta = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_et1, theta_it1,tau,lemda = tf.split(theta1, num_or_size_splits= [N,N,1,1], axis = 0)\n",
    "theta_et = tf.exp(tf.complex(real,theta_et1*2*np.pi))\n",
    "theta_it = tf.exp(tf.complex(real,theta_it1*2*np.pi))\n",
    "\n",
    "g_theta_et1 = tf.reshape(tf.complex(X[0:N*M,:],X[N*M:2*N*M,:]),[N,M*tf.shape(X)[1]])\n",
    "g_theta_et2 = g_theta_et1*tf.repeat(theta_et,M,axis=1)\n",
    "\n",
    "g_theta_et = tf.transpose(tf.reshape((tf.reduce_sum(g_theta_et2,axis = 0, keepdims= True)),[tf.shape(X)[1],M]),perm=[1,0])\n",
    "h_bs_c = tf.complex(X[2*N*M:2*N*M+M,:],X[2*N*M+M:2*N*M+2*M,:])\n",
    "\n",
    "Es = etta*tau*P_b*(tf.real(tf.norm((h_bs_c+g_theta_et), axis=0))**2)\n",
    "P_s = etta*(tau/(1-tau))*P_b*(tf.real(tf.norm((h_bs_c+g_theta_et), axis=0))**2)\n",
    "\n",
    "g_theta_it1 = tf.complex(X[2*N*M+2*M:2*N*M+2*M+N,:],X[2*N*M+2*M+N:2*N*M+2*M+2*N,:])\n",
    "g_theta_it = tf.reduce_sum(g_theta_it1*theta_it,axis = 0)\n",
    "h_sd_c = tf.complex(X[2*N*M+2*M+2*N:2*N*M+2*M+2*N+1,:],X[2*N*M+2*M+2*N+1:2*N*M+2*M+2*N+2,:])\n",
    "\n",
    "rd = P_s*(tf.abs((h_sd_c+g_theta_it)**2))/((sigma_n)**2)\n",
    "r = (1-tau)*tf.log(1+rd)/tf.log(log_base)\n",
    "cost = -tf.reduce_mean(r)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average Rate and Time for a batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for a Batch of 1000 test data: 0.14621210098266602\n",
      "Avg Rate: -3.2593278875161706\n",
      "Avg tau 0.31937924747394253\n"
     ]
    }
   ],
   "source": [
    "M_C =1000\n",
    "\n",
    "mini_batch_size = 10**3\n",
    "np.random.seed(112)    # Must be same seed as used in GA\n",
    "\n",
    "h_bs_test = (np.random.randn( M,mini_batch_size)+1j*np.random.randn( M,mini_batch_size))*np.sqrt(b_bs/2)\n",
    "h_is_test = (np.random.randn( 1,mini_batch_size)+1j*np.random.randn( 1,mini_batch_size))*np.sqrt(b_is/2)\n",
    "h_id_test = (np.random.randn( 1,mini_batch_size)+1j*np.random.randn( 1,mini_batch_size))*np.sqrt(b_id/2)\n",
    "h_sd_test = (np.random.randn( 1,mini_batch_size)+1j*np.random.randn( 1,mini_batch_size))*np.sqrt(b_sd/2)\n",
    "g_rs_test = (np.random.randn( N,mini_batch_size)+1j*np.random.randn( N,mini_batch_size))*np.sqrt(b_rs/2)\n",
    "g_sr_test = (np.random.randn( N,mini_batch_size)+1j*np.random.randn( N,mini_batch_size))*np.sqrt(b_rd/2)\n",
    "g_rd_test = (np.random.randn( N,mini_batch_size)+1j*np.random.randn( N,mini_batch_size))*np.sqrt(b_rd/2)\n",
    "g_ir_test = (np.random.randn( N,mini_batch_size)+1j*np.random.randn( N,mini_batch_size))*np.sqrt(b_ir/2)\n",
    "G_br_test = (np.random.randn( N,M*mini_batch_size)+1j*np.random.randn( N,M*mini_batch_size))*np.sqrt(b_br/2)\n",
    "\n",
    "tic = time.time()\n",
    "g_rs1 =  np.repeat(g_rs_test,M,axis = 1)\n",
    "F = np.reshape(G_br_test*g_rs1, [N*M,mini_batch_size])\n",
    "F = np.concatenate([np.real(F),np.imag(F)],axis=0)\n",
    "F1 = np.concatenate([np.real(h_bs_test),np.imag(h_bs_test)],axis=0)\n",
    "F2 = g_rd_test*g_rs_test\n",
    "F2 = np.concatenate([np.real(F2),np.imag(F2)],axis=0)\n",
    "F3 = np.concatenate([np.real(h_sd_test),np.imag(h_sd_test)],axis=0)\n",
    "F = np.concatenate([F,F1,F2,F3],axis=0)\n",
    "\n",
    "rate = sess.run(cost, feed_dict={X:F})\n",
    "toc = time.time() \n",
    "print('Time for a Batch of 1000 test data:', toc-tic)\n",
    "print('Avg Rate:',rate)\n",
    "tau1 = sess.run(tau, feed_dict={X:F})\n",
    "print('Avg tau',np.mean(tau1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average Time taken by IRS-Net to compute Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average time per 1 channel Coefficient 0.0006310133934020996\n"
     ]
    }
   ],
   "source": [
    "mini_batch_size = 1\n",
    "t = np.zeros((1000,1))\n",
    "for i in range(1000):\n",
    "    np.random.seed(i*2)\n",
    "    h_bs_test = (np.random.randn( M,mini_batch_size)+1j*np.random.randn( M,mini_batch_size))*np.sqrt(b_bs/2)\n",
    "    h_is_test = (np.random.randn( 1,mini_batch_size)+1j*np.random.randn( 1,mini_batch_size))*np.sqrt(b_is/2)\n",
    "    h_id_test = (np.random.randn( 1,mini_batch_size)+1j*np.random.randn( 1,mini_batch_size))*np.sqrt(b_id/2)\n",
    "    h_sd_test = (np.random.randn( 1,mini_batch_size)+1j*np.random.randn( 1,mini_batch_size))*np.sqrt(b_sd/2)\n",
    "    g_rs_test = (np.random.randn( N,mini_batch_size)+1j*np.random.randn( N,mini_batch_size))*np.sqrt(b_rs/2)\n",
    "    g_sr_test = (np.random.randn( N,mini_batch_size)+1j*np.random.randn( N,mini_batch_size))*np.sqrt(b_rd/2)\n",
    "    g_rd_test = (np.random.randn( N,mini_batch_size)+1j*np.random.randn( N,mini_batch_size))*np.sqrt(b_rd/2)\n",
    "    g_ir_test = (np.random.randn( N,mini_batch_size)+1j*np.random.randn( N,mini_batch_size))*np.sqrt(b_ir/2)\n",
    "    G_br_test = (np.random.randn( N,M*mini_batch_size)+1j*np.random.randn( N,M*mini_batch_size))*np.sqrt(b_br/2)\n",
    "    \n",
    "    tic = time.time()\n",
    "    g_rs1 =  np.repeat(g_rs_test,M,axis = 1)\n",
    "    F = np.reshape(G_br_test*g_rs1, [N*M,mini_batch_size])\n",
    "    F = np.concatenate([np.real(F),np.imag(F)],axis=0)\n",
    "    F1 = np.concatenate([np.real(h_bs_test),np.imag(h_bs_test)],axis=0)\n",
    "    F2 = g_rd_test*g_rs_test\n",
    "    F2 = np.concatenate([np.real(F2),np.imag(F2)],axis=0)\n",
    "    F3 = np.concatenate([np.real(h_sd_test),np.imag(h_sd_test)],axis=0)\n",
    "    F = np.concatenate([F,F1,F2,F3],axis=0)\n",
    "\n",
    "    rate = sess.run(cost, feed_dict={X:F})\n",
    "    toc = time.time() \n",
    "    t[i] = toc-tic\n",
    "print('Average time per 1 channel Coefficient',np.mean(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
